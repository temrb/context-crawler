<directory_structure>
configurations/jobs/ai-sdk.ts
configurations/jobs/next-js-16.ts
configurations/jobs/polar-sh.ts
configurations/jobs/prisma.ts
configurations/jobs/react-19.ts
configurations/jobs/trpc.ts
configurations/jobs/zod.ts
src/cli.ts
src/config.ts
src/core.ts
src/job-store.ts
src/llm-service.ts
src/logger.ts
src/queue.ts
src/schema.ts
src/scripts/check-llm-artifacts.ts
src/scripts/generate-llm-artifacts.ts
src/server.ts
src/task-runner.ts
src/worker.ts
src/xenova-embeddings.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/scripts/check-llm-artifacts.ts">
import { glob } from "glob";
import { parse } from "path";
import logger from "../logger.js";
import { llmService } from "../llm-service.js";
const JOBS_OUTPUT_GLOB = "output/jobs/*.json";
async function checkAllArtifacts(): Promise<void> {
  const jobFiles = await glob(JOBS_OUTPUT_GLOB);
  const staleJobs: string[] = [];
  for (const jobFile of jobFiles) {
    const jobName = parse(jobFile).name;
    const isStale = await llmService.isArtifactStale(jobName, jobFile);
    if (isStale) {
      staleJobs.push(jobName);
    }
  }
  if (staleJobs.length > 0) {
    const messages = [
      "Warning: LLM artifacts are out of sync!",
      `Stale jobs: ${staleJobs.join(", ")}`,
      "Run 'npm run generate:llm-artifacts' to update them.",
    ];
    const contentWidth = messages.reduce(
      (max, line) => Math.max(max, line.length),
      0,
    );
    const border = `+${"-".repeat(contentWidth + 4)}+`;
    console.warn("");
    console.warn(border);
    messages.forEach((line) => {
      const paddedLine = line.padEnd(contentWidth, " ");
      console.warn(`|  ${paddedLine}  |`);
    });
    console.warn(border);
    console.warn("");
    logger.warn({ staleJobs }, "Stale LLM artifacts detected.");
  } else {
    logger.info("LLM artifacts are up-to-date.");
  }
}
checkAllArtifacts().catch((error) => {
  logger.error({ error }, "Failed to check LLM artifact status.");
  process.exit(1);
});
</file>

<file path="src/scripts/generate-llm-artifacts.ts">
import { glob } from "glob";
import { parse } from "path";
import logger from "../logger.js";
import { llmService } from "../llm-service.js";
const JOBS_OUTPUT_GLOB = "output/jobs/*.json";
async function generateAllArtifacts(): Promise<void> {
  logger.info("Starting LLM artifact generation process...");
  const jobFiles = await glob(JOBS_OUTPUT_GLOB);
  if (jobFiles.length === 0) {
    logger.warn("No job output files found. Nothing to generate.");
    return;
  }
  let generatedCount = 0;
  let skippedCount = 0;
  for (const jobFile of jobFiles) {
    const jobName = parse(jobFile).name;
    try {
      const shouldUpdate = await llmService.isArtifactStale(jobName, jobFile);
      if (shouldUpdate) {
        await llmService.generateArtifacts(jobName, jobFile);
        generatedCount += 1;
      } else {
        logger.debug({ job: jobName }, "Artifacts are up-to-date. Skipping.");
        skippedCount += 1;
      }
    } catch (error) {
      logger.error(
        { job: jobName, error: error instanceof Error ? error.message : error },
        `Failed to generate artifacts for job '${jobName}'`,
      );
    }
    await new Promise((resolve) => setImmediate(resolve));
  }
  logger.info("LLM artifact generation complete.");
  logger.info(
    `Summary: ${generatedCount} generated/updated, ${skippedCount} skipped.`,
  );
}
generateAllArtifacts().catch((error) => {
  logger.fatal(
    { error },
    "An unexpected error occurred during artifact generation.",
  );
  process.exit(1);
});
</file>

<file path="src/llm-service.ts">
import { Document } from '@langchain/core/documents';
import { HNSWLib } from '@langchain/community/vectorstores/hnswlib';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { createReadStream, existsSync } from 'fs';
import { mkdir, readFile, stat, writeFile } from 'fs/promises';
import { join } from 'path';
import logger from './logger.js';
import { CrawledData } from './schema.js';
import { XenovaTransformersEmbeddings } from './xenova-embeddings.js';
const LLMS_DIR = join(process.cwd(), 'data', 'llms');
const INDEXES_DIR = join(process.cwd(), 'data', 'indexes');
const JOBS_OUTPUT_DIR = join(process.cwd(), 'output', 'jobs');
const DEFAULT_CHUNK_SIZE = 10000;
const DEFAULT_CHUNK_OVERLAP = 100;
const CHUNK_SEPARATORS = ['\n---\n', '\n\n', '\n', ' '];
const ARTIFACT_METADATA_VERSION = 2;
type ArtifactMetadata = {
	version: number;
	chunkSize: number;
	chunkOverlap: number;
	separators: string[];
};
async function ensureDirectories(): Promise<void> {
	await mkdir(LLMS_DIR, { recursive: true });
	await mkdir(INDEXES_DIR, { recursive: true });
}
function parseEnvInt(
	value: string | undefined,
	defaultValue: number,
	{ allowZero = false }: { allowZero?: boolean } = {}
): number {
	if (value === undefined || value.trim() === '') {
		return defaultValue;
	}
	const parsed = Number.parseInt(value, 10);
	if (Number.isNaN(parsed)) {
		return defaultValue;
	}
	if (allowZero) {
		return parsed < 0 ? defaultValue : parsed;
	}
	return parsed <= 0 ? defaultValue : parsed;
}
function getChunkingOptions(): { chunkSize: number; chunkOverlap: number } {
	const chunkSize = parseEnvInt(process.env.LLM_CHUNK_SIZE, DEFAULT_CHUNK_SIZE);
	let chunkOverlap = parseEnvInt(
		process.env.LLM_CHUNK_OVERLAP,
		DEFAULT_CHUNK_OVERLAP,
		{ allowZero: true }
	);
	if (chunkOverlap >= chunkSize) {
		chunkOverlap = Math.max(0, chunkSize - 1);
	}
	return { chunkSize, chunkOverlap };
}
class LLMService {
	private vectorStores: Map<string, HNSWLib> = new Map();
	private embeddings: XenovaTransformersEmbeddings;
	private initializationPromise: Promise<void>;
	constructor() {
		this.embeddings = new XenovaTransformersEmbeddings({
			model: 'Xenova/all-MiniLM-L6-v2',
		});
		this.initializationPromise = ensureDirectories().catch((error) => {
			logger.error(
				{ error: error instanceof Error ? error.message : error },
				'Failed to create LLM data directories'
			);
			throw error;
		});
		logger.info('LLMService initialized with embedding model.');
	}
	public async processJobOutput(jobName: string): Promise<void> {
		await this.initializationPromise;
		const jsonPath = join(JOBS_OUTPUT_DIR, `${jobName}.json`);
		if (!existsSync(jsonPath)) {
			throw new Error(
				`Job output file for '${jobName}' does not exist at ${jsonPath}.`
			);
		}
		await this.generateArtifacts(jobName, jsonPath);
	}
	public async generateArtifacts(
		jobName: string,
		jsonPath: string
	): Promise<void> {
		await this.initializationPromise;
		logger.info(
			{ job: jobName },
			`Generating/Updating LLM artifacts for ${jobName}...`
		);
		const llmTextPath = join(LLMS_DIR, `${jobName}.txt`);
		const rawJson = await readFile(jsonPath, 'utf-8');
		const data = JSON.parse(rawJson) as CrawledData[];
		const formattedText = data
			.map(
				(item) => `---
Title: ${item.title}
URL: ${item.url}
---
${item.html}`
			)
			.join('\n\n');
		await writeFile(llmTextPath, formattedText);
		logger.debug(
			{ job: jobName, path: llmTextPath },
			`Generated ${jobName}.txt`
		);
		const { chunkSize, chunkOverlap } = getChunkingOptions();
		const textSplitter = new RecursiveCharacterTextSplitter({
			chunkSize,
			chunkOverlap,
			separators: CHUNK_SEPARATORS,
		});
		const documents = data.map(
			(item) =>
				new Document({
					pageContent: `Title: ${item.title}\nURL: ${item.url}\n\n${item.html}`,
					metadata: {
						title: item.title,
						url: item.url,
					},
				})
		);
		const docs = await textSplitter.splitDocuments(documents);
		const vectorStore = await HNSWLib.fromDocuments(docs, this.embeddings);
		const indexPath = join(INDEXES_DIR, `${jobName}.index`);
		const metadataPath = join(INDEXES_DIR, `${jobName}.meta.json`);
		await vectorStore.save(indexPath);
		const metadata: ArtifactMetadata = {
			version: ARTIFACT_METADATA_VERSION,
			chunkSize,
			chunkOverlap,
			separators: CHUNK_SEPARATORS,
		};
		await writeFile(metadataPath, JSON.stringify(metadata, null, 2));
		logger.info({ job: jobName, path: indexPath }, 'Saved vector index');
		this.vectorStores.delete(jobName);
		logger.debug({ job: jobName }, 'Invalidated in-memory vector store cache.');
	}
	public async isArtifactStale(
		jobName: string,
		jsonPath: string
	): Promise<boolean> {
		const indexPath = join(INDEXES_DIR, `${jobName}.index`);
		const metadataPath = join(INDEXES_DIR, `${jobName}.meta.json`);
		if (!existsSync(indexPath) || !existsSync(metadataPath)) {
			return true;
		}
		let metadata: ArtifactMetadata | undefined;
		try {
			const rawMetadata = await readFile(metadataPath, 'utf-8');
			metadata = JSON.parse(rawMetadata) as ArtifactMetadata;
		} catch (error) {
			logger.warn(
				{ job: jobName, error: error instanceof Error ? error.message : error },
				'Failed to read artifact metadata. Forcing regeneration.'
			);
			return true;
		}
		const { chunkSize, chunkOverlap } = getChunkingOptions();
		if (
			metadata.version !== ARTIFACT_METADATA_VERSION ||
			metadata.chunkSize !== chunkSize ||
			metadata.chunkOverlap !== chunkOverlap ||
			!Array.isArray(metadata.separators) ||
			metadata.separators.join('||') !== CHUNK_SEPARATORS.join('||')
		) {
			return true;
		}
		const jsonStats = await stat(jsonPath);
		const indexStats = await stat(indexPath);
		const metadataStats = await stat(metadataPath);
		return (
			jsonStats.mtime > indexStats.mtime ||
			jsonStats.mtime > metadataStats.mtime
		);
	}
	public jobExists(jobName: string): boolean {
		return existsSync(join(LLMS_DIR, `${jobName}.txt`));
	}
	public getFullTextStream(jobName: string) {
		const llmTextPath = join(LLMS_DIR, `${jobName}.txt`);
		return createReadStream(llmTextPath, { encoding: 'utf-8' });
	}
	public async search(jobName: string, query: string, k = 5): Promise<string> {
		await this.initializationPromise;
		if (!this.vectorStores.has(jobName)) {
			const indexPath = join(INDEXES_DIR, `${jobName}.index`);
			if (!existsSync(indexPath)) {
				logger.warn(
					{ job: jobName },
					'Index not found for search. Attempting just-in-time generation.'
				);
				await this.processJobOutput(jobName);
				if (!existsSync(indexPath)) {
					throw new Error(
						`Index for job '${jobName}' could not be found or generated.`
					);
				}
			}
			const loadedVectorStore = await HNSWLib.load(indexPath, this.embeddings);
			this.vectorStores.set(jobName, loadedVectorStore);
			logger.debug({ job: jobName }, 'Lazy-loaded vector store into memory.');
		}
		const vectorStore = this.vectorStores.get(jobName)!;
		const results = await vectorStore.similaritySearch(query, k);
		if (results.length === 0) {
			return `No relevant information found for the subject: "${query}"`;
		}
		return results
			.map((doc, index) => {
				const metadata = (doc.metadata ?? {}) as Record<string, unknown>;
				const title =
					typeof metadata.title === 'string' ? metadata.title : 'Untitled';
				const url = typeof metadata.url === 'string' ? metadata.url : 'Unknown';
				return [
					`--- Result ${index + 1} ---`,
					`Title: ${title}`,
					`URL: ${url}`,
					'',
					doc.pageContent,
				].join('\n');
			})
			.join('\n\n');
	}
}
export const llmService = new LLMService();
</file>

<file path="src/xenova-embeddings.ts">
import { Embeddings, type EmbeddingsParams } from "@langchain/core/embeddings";
import { chunkArray } from "@langchain/core/utils/chunk_array";
import { pipeline, type Tensor } from "@xenova/transformers";
type FeatureExtractionPipeline = (
  inputs: string | string[],
  options?: Record<string, unknown>,
) => Promise<Tensor>;
export interface XenovaEmbeddingsParams extends EmbeddingsParams {
  model: string;
  batchSize?: number;
  stripNewLines?: boolean;
  pipelineOptions?: Record<string, unknown>;
  pretrainedOptions?: Record<string, unknown>;
}
export class XenovaTransformersEmbeddings extends Embeddings {
  private model: string;
  private batchSize: number;
  private stripNewLines: boolean;
  private pipelineOptions: Record<string, unknown>;
  private pretrainedOptions: Record<string, unknown>;
  private pipelinePromise?: Promise<FeatureExtractionPipeline>;
  constructor(params?: Partial<XenovaEmbeddingsParams>) {
    super(params ?? {});
    this.model = params?.model ?? "Xenova/all-MiniLM-L6-v2";
    this.batchSize = params?.batchSize ?? 512;
    this.stripNewLines = params?.stripNewLines ?? true;
    this.pipelineOptions = {
      pooling: "mean",
      normalize: true,
      ...params?.pipelineOptions,
    };
    this.pretrainedOptions = params?.pretrainedOptions ?? {};
  }
  async embedDocuments(texts: string[]): Promise<number[][]> {
    const normalizedTexts = this.normalizeInputs(texts);
    const batches = chunkArray(normalizedTexts, this.batchSize);
    const embeddings: number[][] = [];
    for (const batch of batches) {
      const batchEmbeddings = await this.runEmbedding(batch);
      embeddings.push(...batchEmbeddings);
    }
    return embeddings;
  }
  async embedQuery(text: string): Promise<number[]> {
    const [embedding] = await this.runEmbedding(this.normalizeInputs([text]));
    if (!embedding) {
      throw new Error("Failed to compute embedding for query text.");
    }
    return embedding;
  }
  private async runEmbedding(texts: string[]): Promise<number[][]> {
    if (texts.length === 0) {
      return [];
    }
    const pipe = await this.getPipeline();
    const tensor = (await this.caller.call(() =>
      pipe(texts as string | string[], this.pipelineOptions),
    )) as Tensor;
    if (typeof tensor.tolist !== "function") {
      throw new Error("Unexpected embedding pipeline output format.");
    }
    const result = tensor.tolist() as number[][] | number[];
    if (Array.isArray(result) && result.length > 0 && Array.isArray(result[0])) {
      return result as number[][];
    }
    return [result as number[]];
  }
  private async getPipeline(): Promise<FeatureExtractionPipeline> {
    if (!this.pipelinePromise) {
      this.pipelinePromise = pipeline(
        "feature-extraction",
        this.model,
        this.pretrainedOptions,
      ) as Promise<FeatureExtractionPipeline>;
    }
    return this.pipelinePromise;
  }
  private normalizeInputs(texts: string[]): string[] {
    if (!this.stripNewLines) {
      return texts;
    }
    return texts.map((text) => text.replace(/\n/g, " "));
  }
}
</file>

<file path="src/job-store.ts">
import Database from "better-sqlite3";
import { PathLike } from "fs";
import { mkdirSync } from "fs";
import { dirname } from "path";
import { Config } from "./schema.js";
export type JobStatus = "pending" | "running" | "completed" | "failed";
export interface Job {
  id: string;
  status: JobStatus;
  config: Config;
  outputFile?: PathLike | null;
  error?: string;
  createdAt: Date;
  completedAt?: Date;
}
export interface JobRecord {
  id: string;
  status: JobStatus;
  config: string;
  outputFile: string | null;
  error: string | null;
  createdAt: string;
  completedAt: string | null;
}
class JobStore {
  private db: Database.Database;
  constructor(dbPath: string = "./data/jobs.db") {
    const dir = dirname(dbPath);
    mkdirSync(dir, { recursive: true });
    this.db = new Database(dbPath);
    this.db.pragma("journal_mode = WAL");
    this.initialize();
  }
  private initialize(): void {
    this.db.exec(`
      CREATE TABLE IF NOT EXISTS jobs (
        id TEXT PRIMARY KEY,
        status TEXT NOT NULL,
        config TEXT NOT NULL,
        outputFile TEXT,
        error TEXT,
        createdAt TEXT NOT NULL,
        completedAt TEXT
      )
    `);
  }
  createJob(jobId: string, config: Config): void {
    const stmt = this.db.prepare(`
      INSERT INTO jobs (id, status, config, createdAt)
      VALUES (?, ?, ?, ?)
    `);
    stmt.run(
      jobId,
      "pending",
      JSON.stringify(config),
      new Date().toISOString(),
    );
  }
  getJobById(jobId: string): Job | undefined {
    const stmt = this.db.prepare("SELECT * FROM jobs WHERE id = ?");
    const record = stmt.get(jobId) as JobRecord | undefined;
    if (!record) {
      return undefined;
    }
    return this.recordToJob(record);
  }
  updateJobStatus(
    jobId: string,
    status: JobStatus,
    data?: {
      outputFile?: PathLike;
      error?: string;
      completedAt?: Date;
    },
  ): void {
    const job = this.getJobById(jobId);
    if (!job) {
      throw new Error(`Job with id ${jobId} not found`);
    }
    const updates: string[] = ["status = ?"];
    const params: (string | null)[] = [status];
    if (data?.outputFile !== undefined) {
      updates.push("outputFile = ?");
      params.push(String(data.outputFile));
    }
    if (data?.error !== undefined) {
      updates.push("error = ?");
      params.push(data.error);
    }
    if (data?.completedAt !== undefined) {
      updates.push("completedAt = ?");
      params.push(data.completedAt.toISOString());
    }
    params.push(jobId);
    const stmt = this.db.prepare(`
      UPDATE jobs
      SET ${updates.join(", ")}
      WHERE id = ?
    `);
    stmt.run(...params);
  }
  getAllJobs(): Job[] {
    const stmt = this.db.prepare("SELECT * FROM jobs ORDER BY createdAt DESC");
    const records = stmt.all() as JobRecord[];
    return records.map((record) => this.recordToJob(record));
  }
  deleteJob(jobId: string): void {
    const stmt = this.db.prepare("DELETE FROM jobs WHERE id = ?");
    stmt.run(jobId);
  }
  close(): void {
    this.db.close();
  }
  private recordToJob(record: JobRecord): Job {
    return {
      id: record.id,
      status: record.status,
      config: JSON.parse(record.config) as Config,
      outputFile: record.outputFile,
      error: record.error ?? undefined,
      createdAt: new Date(record.createdAt),
      completedAt: record.completedAt
        ? new Date(record.completedAt)
        : undefined,
    };
  }
}
export const jobStore = new JobStore();
</file>

<file path="src/logger.ts">
import pino from "pino";
const isDevelopment = process.env.NODE_ENV === "development";
export const logger = pino({
  level: process.env.LOG_LEVEL || "info",
  transport: isDevelopment
    ? {
        target: "pino-pretty",
        options: {
          colorize: true,
          translateTime: "HH:MM:ss Z",
          ignore: "pid,hostname",
        },
      }
    : undefined,
});
export default logger;
</file>

<file path="src/task-runner.ts">
import { PathLike } from 'fs';
import ContextCrawlerCore from './core.js';
import logger from './logger.js';
import { Config } from './schema.js';
export interface TaskResult {
	success: boolean;
	outputFile: PathLike | null;
	error?: string;
}
export async function runTask(config: Config, jobName: string): Promise<TaskResult> {
	let crawler: ContextCrawlerCore | null = null;
	try {
		logger.info({ job: jobName }, `Starting job: ${jobName}`);
		crawler = new ContextCrawlerCore(config, jobName);
		await crawler.crawl();
		const outputFile = await crawler.write();
		await crawler.cleanup();
		logger.info(
			{ job: jobName, outputFile },
			`Job completed: ${jobName}`
		);
		return {
			success: true,
			outputFile,
		};
	} catch (error) {
		const errorMessage =
			error instanceof Error ? error.message : 'Unknown error occurred';
		logger.error(
			{ job: jobName, error: errorMessage },
			`Job failed: ${jobName}`
		);
		if (crawler) {
			try {
				await crawler.cleanup();
			} catch (cleanupError) {
				logger.warn(
					{
						job: jobName,
						error:
							cleanupError instanceof Error
								? cleanupError.message
								: cleanupError,
					},
					'Failed to cleanup storage after error'
				);
			}
		}
		return {
			success: false,
			outputFile: null,
			error: errorMessage,
		};
	}
}
</file>

<file path="configurations/jobs/ai-sdk.ts">
import { defineJob } from '../types.js';
export default defineJob({
	entry: 'https://ai-sdk.dev/docs/foundations/overview',
	match: [
		'https://ai-sdk.dev/docs/foundations/**',
		'https://ai-sdk.dev/docs/getting-started/**',
		'https://ai-sdk.dev/docs/agents/**',
		'https://ai-sdk.dev/docs/ai-sdk-core/**',
		'https://ai-sdk.dev/docs/ai-sdk-ui/**',
		'https://ai-sdk.dev/docs/advanced/**',
	],
	exclude: ['**/support', '**/changelog', '**/llms-full.txt*'],
	selector: 'article',
});
</file>

<file path="configurations/jobs/next-js-16.ts">
import { defineJob } from '../types.js';
export default defineJob([
	{
		entry: 'https://nextjs.org/docs/app/getting-started/proxy',
		match: [
			'https://nextjs.org/docs/app/api-reference/**',
			'https://nextjs.org/docs/architecture/accessibility',
			'https://nextjs.org/docs/app/getting-started/metadata-and-og-images',
		],
		selector: 'article',
	},
]);
</file>

<file path="configurations/jobs/polar-sh.ts">
import { defineJob } from '../types.js';
export default defineJob({
	entry: 'https://polar.sh/docs',
	match: [
		'https://polar.sh/docs/features/**',
		'https://polar.sh/docs/integrate/authentication/**',
		'https://polar.sh/docs/api-reference/**',
		'https://polar.sh/docs/guides/**',
	],
	exclude: ['**/support', '**/changelog', '**/llms-full.txt*'],
	selector: '#content-area',
});
</file>

<file path="configurations/jobs/prisma.ts">
import { defineJob } from '../types.js';
export default defineJob({
	entry: 'https://www.prisma.io/docs/orm/reference',
	match: 'https://www.prisma.io/docs/orm/reference/**',
	selector: 'article',
});
</file>

<file path="configurations/jobs/react-19.ts">
import { defineJob } from '../types.js';
export default defineJob({
	entry: 'https://react.dev/reference/react',
	match: [
		'https://react.dev/reference/react/**',
		'https://react.dev/reference/react-dom/**',
	],
	selector: 'article',
});
</file>

<file path="configurations/jobs/trpc.ts">
import { defineJob } from '../types.js';
export default defineJob({
	entry: 'https://trpc.io/docs/server/introduction',
	match: [
		'https://trpc.io/docs/server/**',
		'https://trpc.io/docs/client/**',
		'https://trpc.io/docs/typedoc/**',
	],
	selector: 'article',
});
</file>

<file path="configurations/jobs/zod.ts">
import { defineJob } from '../types.js';
export default defineJob({
	entry: 'https://zod.dev',
	match: [
		'https://zod.dev/basics',
		'https://zod.dev/api',
		'https://zod.dev/error-customization',
		'https://zod.dev/error-formatting',
		'https://zod.dev/metadata',
		'https://zod.dev/json-schema',
		'https://zod.dev/codecs',
		'https://zod.dev/packages/zod',
		'https://zod.dev/packages/mini',
		'https://zod.dev/packages/core',
	],
	selector: 'article',
});
</file>

<file path="src/queue.ts">
import Database from "better-sqlite3";
import { mkdir } from "fs/promises";
import { dirname } from "path";
import { Config } from "./schema.js";
export interface CrawlJobData {
  config: Config;
  jobName: string;
}
export interface QueueJobOptions {
  jobId: string;
  priority?: number;
  maxAttempts?: number;
  backoffDelay?: number;
}
interface QueueJobRecord {
  id: string;
  jobId: string;
  status: "pending" | "claimed" | "completed" | "failed";
  data: string;
  priority: number;
  attempts: number;
  maxAttempts: number;
  nextRetryAt: string | null;
  claimedAt: string | null;
  completedAt: string | null;
  error: string | null;
  createdAt: string;
}
export interface QueueJob {
  id: string;
  jobId: string;
  data: CrawlJobData;
  attempts: number;
  maxAttempts: number;
}
class SQLiteQueue {
  private db!: Database.Database;
  private dbPath: string;
  private initialized = false;
  constructor(dbPath: string = "./data/queue.db") {
    this.dbPath = dbPath;
  }
  async initialize(): Promise<void> {
    if (this.initialized) {
      return;
    }
    const dir = dirname(this.dbPath);
    await mkdir(dir, { recursive: true });
    this.db = new Database(this.dbPath);
    this.db.pragma("journal_mode = WAL");
    this.db.exec(`
      CREATE TABLE IF NOT EXISTS queue (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        jobId TEXT NOT NULL UNIQUE,
        status TEXT NOT NULL DEFAULT 'pending',
        data TEXT NOT NULL,
        priority INTEGER NOT NULL DEFAULT 0,
        attempts INTEGER NOT NULL DEFAULT 0,
        maxAttempts INTEGER NOT NULL DEFAULT 3,
        nextRetryAt TEXT,
        claimedAt TEXT,
        completedAt TEXT,
        error TEXT,
        createdAt TEXT NOT NULL
      );
      CREATE INDEX IF NOT EXISTS idx_queue_status ON queue(status);
      CREATE INDEX IF NOT EXISTS idx_queue_priority ON queue(priority DESC);
      CREATE INDEX IF NOT EXISTS idx_queue_nextRetryAt ON queue(nextRetryAt);
    `);
    this.initialized = true;
  }
  async add(
    _queueName: string,
    data: CrawlJobData,
    options: QueueJobOptions,
  ): Promise<void> {
    const {
      jobId,
      priority = 0,
      maxAttempts = 3,
      backoffDelay = 1000,
    } = options;
    const stmt = this.db.prepare(`
      INSERT INTO queue (jobId, status, data, priority, maxAttempts, createdAt)
      VALUES (?, 'pending', ?, ?, ?, ?)
    `);
    stmt.run(
      jobId,
      JSON.stringify(data),
      priority,
      maxAttempts,
      new Date().toISOString(),
    );
  }
  claimNextJob(): QueueJob | null {
    const now = new Date().toISOString();
    const claimJob = this.db.transaction(() => {
      const findStmt = this.db.prepare(`
        SELECT * FROM queue
        WHERE status = 'pending'
          AND (nextRetryAt IS NULL OR nextRetryAt <= ?)
        ORDER BY priority DESC, createdAt ASC
        LIMIT 1
      `);
      const record = findStmt.get(now) as QueueJobRecord | undefined;
      if (!record) {
        return null;
      }
      const updateStmt = this.db.prepare(`
        UPDATE queue
        SET status = 'claimed',
            claimedAt = ?,
            attempts = attempts + 1
        WHERE id = ?
      `);
      updateStmt.run(now, record.id);
      return {
        id: record.id.toString(),
        jobId: record.jobId,
        data: JSON.parse(record.data) as CrawlJobData,
        attempts: record.attempts + 1,
        maxAttempts: record.maxAttempts,
      };
    });
    return claimJob();
  }
  markCompleted(queueJobId: string): void {
    const stmt = this.db.prepare(`
      UPDATE queue
      SET status = 'completed',
          completedAt = ?
      WHERE id = ?
    `);
    stmt.run(new Date().toISOString(), queueJobId);
  }
  markFailed(
    queueJobId: string,
    error: string,
    shouldRetry: boolean,
    backoffDelay: number = 1000,
  ): void {
    const record = this.db
      .prepare("SELECT * FROM queue WHERE id = ?")
      .get(queueJobId) as QueueJobRecord | undefined;
    if (!record) {
      return;
    }
    if (shouldRetry && record.attempts < record.maxAttempts) {
      const delay = backoffDelay * Math.pow(2, record.attempts - 1);
      const nextRetryAt = new Date(Date.now() + delay).toISOString();
      const stmt = this.db.prepare(`
        UPDATE queue
        SET status = 'pending',
            nextRetryAt = ?,
            error = ?
        WHERE id = ?
      `);
      stmt.run(nextRetryAt, error, queueJobId);
    } else {
      const stmt = this.db.prepare(`
        UPDATE queue
        SET status = 'failed',
            completedAt = ?,
            error = ?
        WHERE id = ?
      `);
      stmt.run(new Date().toISOString(), error, queueJobId);
    }
  }
  resetStuckJobs(timeoutMs: number = 30 * 60 * 1000): number {
    const cutoffTime = new Date(Date.now() - timeoutMs).toISOString();
    const stmt = this.db.prepare(`
      UPDATE queue
      SET status = 'pending',
          claimedAt = NULL
      WHERE status = 'claimed'
        AND claimedAt < ?
    `);
    const result = stmt.run(cutoffTime);
    return result.changes;
  }
  cleanupOldJobs(ageMs: number = 7 * 24 * 60 * 60 * 1000): number {
    const cutoffTime = new Date(Date.now() - ageMs).toISOString();
    const stmt = this.db.prepare(`
      DELETE FROM queue
      WHERE status IN ('completed', 'failed')
        AND completedAt < ?
    `);
    const result = stmt.run(cutoffTime);
    return result.changes;
  }
  clearCompletedJobs(): number {
    const stmt = this.db.prepare(`
      DELETE FROM queue
      WHERE status IN ('completed', 'failed')
    `);
    const result = stmt.run();
    return result.changes;
  }
  getStats(): {
    pending: number;
    claimed: number;
    completed: number;
    failed: number;
    total: number;
  } {
    const stmt = this.db.prepare(`
      SELECT
        status,
        COUNT(*) as count
      FROM queue
      GROUP BY status
    `);
    const results = stmt.all() as Array<{ status: string; count: number }>;
    const stats = {
      pending: 0,
      claimed: 0,
      completed: 0,
      failed: 0,
      total: 0,
    };
    results.forEach((row) => {
      const status = row.status as keyof typeof stats;
      if (status in stats) {
        stats[status] = row.count;
      }
      stats.total += row.count;
    });
    return stats;
  }
  close(): void {
    this.db.close();
  }
}
export const crawlQueue = new SQLiteQueue();
</file>

<file path="src/worker.ts">
#!/usr/bin/env node
import { configDotenv } from "dotenv";
import { jobStore } from "./job-store.js";
import logger from "./logger.js";
import { crawlQueue, QueueJob } from "./queue.js";
import { runTask } from "./task-runner.js";
import { llmService } from "./llm-service.js";
configDotenv();
const WORKER_CONCURRENCY = Number(process.env.WORKER_CONCURRENCY) || 2;
const POLL_INTERVAL_MS = Number(process.env.POLL_INTERVAL_MS) || 1000;
const MAX_POLL_INTERVAL_MS = Number(process.env.MAX_POLL_INTERVAL_MS) || 10000;
const JOB_TIMEOUT_MS = Number(process.env.JOB_TIMEOUT_MS) || 30 * 60 * 1000;
const BACKOFF_DELAY_MS = Number(process.env.BACKOFF_DELAY_MS) || 5000;
const activeJobs = new Set<Promise<void>>();
let isShuttingDown = false;
let pollIntervalId: NodeJS.Timeout | null = null;
let currentPollInterval = POLL_INTERVAL_MS;
async function processCrawlJob(job: QueueJob): Promise<void> {
  const { config, jobName } = job.data;
  const { jobId } = job;
  logger.info(
    {
      jobId,
      jobName,
      queueJobId: job.id,
      attempt: job.attempts,
      maxAttempts: job.maxAttempts,
    },
    "Processing crawl job",
  );
  try {
    jobStore.updateJobStatus(jobId, "running");
    const result = await runTask(config, jobName || 'unknown');
    if (result.success) {
      crawlQueue.markCompleted(job.id);
      jobStore.updateJobStatus(jobId, "completed", {
        outputFile: result.outputFile || undefined,
        completedAt: new Date(),
      });
      if (result.outputFile && jobName) {
        logger.info({ jobId, jobName }, "Triggering LLM artifact generation...");
        void llmService.processJobOutput(jobName).catch((error) => {
          logger.error(
            {
              jobId,
              jobName,
              error: error instanceof Error ? error.message : error,
            },
            "LLM artifact generation failed post-job.",
          );
        });
      }
      const clearedCount = crawlQueue.clearCompletedJobs();
      if (clearedCount > 0) {
        logger.debug(
          { clearedCount },
          "Auto-cleared completed/failed jobs from queue",
        );
      }
      logger.info(
        { jobId, queueJobId: job.id, outputFile: result.outputFile },
        "Crawl job completed successfully",
      );
    } else {
      throw new Error(result.error || "Task execution failed");
    }
  } catch (error) {
    const errorMessage =
      error instanceof Error ? error.message : "Unknown error occurred";
    logger.error(
      { jobId, queueJobId: job.id, error: errorMessage, attempt: job.attempts },
      "Crawl job failed",
    );
    const shouldRetry = job.attempts < job.maxAttempts;
    const backoffWithJitter =
      BACKOFF_DELAY_MS *
      Math.pow(2, job.attempts - 1) *
      (0.5 + Math.random() * 0.5);
    crawlQueue.markFailed(job.id, errorMessage, shouldRetry, backoffWithJitter);
    if (!shouldRetry) {
      jobStore.updateJobStatus(jobId, "failed", {
        error: errorMessage,
        completedAt: new Date(),
      });
      const clearedCount = crawlQueue.clearCompletedJobs();
      if (clearedCount > 0) {
        logger.debug(
          { clearedCount },
          "Auto-cleared completed/failed jobs from queue",
        );
      }
    }
  }
}
async function pollQueue(): Promise<void> {
  if (isShuttingDown) {
    return;
  }
  if (activeJobs.size >= WORKER_CONCURRENCY) {
    return;
  }
  let jobClaimed = false;
  const jobsToStart: QueueJob[] = [];
  while (activeJobs.size + jobsToStart.length < WORKER_CONCURRENCY) {
    const job = crawlQueue.claimNextJob();
    if (!job) {
      break;
    }
    jobClaimed = true;
    jobsToStart.push(job);
  }
  for (const job of jobsToStart) {
    const jobPromise = processCrawlJob(job)
      .catch((error) => {
        logger.error(
          { error: error instanceof Error ? error.message : error },
          "Unhandled error in job processing",
        );
      })
      .finally(() => {
        activeJobs.delete(jobPromise);
      });
    activeJobs.add(jobPromise);
  }
  if (jobClaimed) {
    currentPollInterval = POLL_INTERVAL_MS;
  } else {
    currentPollInterval = Math.min(
      currentPollInterval * 1.5,
      MAX_POLL_INTERVAL_MS,
    );
  }
}
async function startWorker(): Promise<void> {
  await crawlQueue.initialize();
  logger.info(
    {
      concurrency: WORKER_CONCURRENCY,
      pollInterval: POLL_INTERVAL_MS,
      jobTimeout: JOB_TIMEOUT_MS,
    },
    "Worker starting...",
  );
  const resetCount = crawlQueue.resetStuckJobs(JOB_TIMEOUT_MS);
  if (resetCount > 0) {
    logger.info({ count: resetCount }, "Reset stuck jobs from previous run");
  }
  const cleanedCount = crawlQueue.cleanupOldJobs(7 * 24 * 60 * 60 * 1000);
  if (cleanedCount > 0) {
    logger.info({ count: cleanedCount }, "Cleaned up old jobs");
  }
  const stats = crawlQueue.getStats();
  logger.info({ stats }, "Queue statistics");
  const poll = () => {
    pollQueue()
      .catch((error) => {
        logger.error(
          { error: error instanceof Error ? error.message : error },
          "Error during queue polling",
        );
      })
      .finally(() => {
        if (!isShuttingDown) {
          pollIntervalId = setTimeout(poll, currentPollInterval);
        }
      });
  };
  poll();
  logger.info("Worker is ready and waiting for jobs");
}
async function shutdown(): Promise<void> {
  if (isShuttingDown) {
    return;
  }
  isShuttingDown = true;
  logger.info("Worker shutting down gracefully...");
  if (pollIntervalId) {
    clearTimeout(pollIntervalId);
    pollIntervalId = null;
  }
  if (activeJobs.size > 0) {
    logger.info(
      { activeJobCount: activeJobs.size },
      "Waiting for active jobs to complete...",
    );
    await Promise.all(Array.from(activeJobs));
  }
  crawlQueue.close();
  jobStore.close();
  logger.info("Worker shut down complete");
  process.exit(0);
}
process.on("SIGTERM", () => {
  logger.info("SIGTERM received");
  shutdown();
});
process.on("SIGINT", () => {
  logger.info("SIGINT received");
  shutdown();
});
process.on("uncaughtException", (error) => {
  logger.error(
    { error: error.message, stack: error.stack },
    "Uncaught exception",
  );
  shutdown();
});
process.on("unhandledRejection", (reason) => {
  logger.error(
    { reason: reason instanceof Error ? reason.message : reason },
    "Unhandled rejection",
  );
  shutdown();
});
await startWorker();
</file>

<file path="src/cli.ts">
#!/usr/bin/env node
import { program } from 'commander';
import { randomUUID } from 'crypto';
import { createWriteStream } from 'fs';
import { mkdir, mkdtemp, readFile, rm } from 'fs/promises';
import inquirer from 'inquirer';
import { tmpdir } from 'os';
import { dirname, join } from 'path';
import {
	getAllJobNames,
	getJobConfigs,
	type JobName,
} from './config.js';
import { jobStore } from './job-store.js';
import logger from './logger.js';
import { crawlQueue } from './queue.js';
import { Config, configSchema, generateOutputFileName, NamedConfig } from './schema.js';
import { runTask } from './task-runner.js';
const packageJsonUrl = new URL('../../package.json', import.meta.url);
const packageInfo = JSON.parse(
	await readFile(packageJsonUrl, 'utf-8')
) as { version: string; description: string };
const { version, description } = packageInfo;
const messages = {
	urls: 'Enter starting URLs (comma-separated for multiple):',
	match: 'What is the URL pattern you want to match?',
	selector: 'What is the CSS selector you want to match?',
	outputFileName: 'What is the name of the output file?',
	job: 'Name of the job to run',
};
program.version(version).description(description);
program
	.command('single [jobName]')
	.description('Run a single job (all configs in the job)')
	.option('-j, --job <string>', messages.job)
	.action(async (jobNameArg?: string, options?: { job?: string }) => {
		let selectedJobName: string;
		if (jobNameArg) {
			selectedJobName = jobNameArg;
		} else if (options?.job) {
			selectedJobName = options.job;
		} else {
			const availableJobs = getAllJobNames();
			if (availableJobs.length === 0) {
				logger.error('No jobs found in configurations');
				process.exit(1);
			}
			const jobChoices = availableJobs.map((name) => {
				const count = getJobConfigs(name as JobName).length;
				return {
					name: `${name} (${count} ${count === 1 ? 'config' : 'configs'})`,
					value: name,
				};
			});
			const jobAnswer = await inquirer.prompt({
				type: 'list',
				name: 'jobName',
				message: 'Select a job to run:',
				choices: jobChoices,
			});
			selectedJobName = jobAnswer.jobName;
		}
		const jobConfigs = getJobConfigs(selectedJobName as JobName);
		if (!jobConfigs || jobConfigs.length === 0) {
			logger.error({ job: selectedJobName }, `Job '${selectedJobName}' not found or is empty`);
			const availableJobs = getAllJobNames();
			logger.info(`Available jobs: ${availableJobs.join(', ')}`);
			process.exit(1);
		}
		logger.info(
			{ job: selectedJobName, configCount: jobConfigs.length },
			`Running job '${selectedJobName}' with ${jobConfigs.length} ${
				jobConfigs.length === 1 ? 'config' : 'configs'
			}`
		);
		const tempDir = await mkdtemp(join(tmpdir(), 'context-crawler-'));
		try {
			const results: {
				successful: Array<{ config: NamedConfig; outputFile: string }>;
				failed: Array<{ config: NamedConfig; error: string }>;
			} = {
				successful: [],
				failed: [],
			};
			for (let i = 0; i < jobConfigs.length; i++) {
				const config = jobConfigs[i]!;
				logger.info(
					{
						progress: `${i + 1}/${jobConfigs.length}`,
						job: selectedJobName,
					},
					`Processing config ${i + 1}/${jobConfigs.length}`
				);
				const tempOutputPath = join(tempDir, `config-${i}.json`);
				const tempConfig = { ...config, outputFileName: tempOutputPath };
				const result = await runTask(tempConfig, selectedJobName);
				if (result.success && result.outputFile) {
					results.successful.push({
						config,
						outputFile: result.outputFile.toString(),
					});
					logger.info(
						{
							progress: `${i + 1}/${jobConfigs.length}`,
							job: selectedJobName,
						},
						`Completed config ${i + 1}/${jobConfigs.length}`
					);
				} else {
					results.failed.push({
						config,
						error: result.error || 'Unknown error',
					});
					logger.error(
						{
							progress: `${i + 1}/${jobConfigs.length}`,
							job: selectedJobName,
							error: result.error,
						},
						`Failed config ${i + 1}/${jobConfigs.length}`
					);
				}
			}
			const successCount = results.successful.length;
			const failCount = results.failed.length;
			const totalCount = successCount + failCount;
			logger.info(
				{ job: selectedJobName, successCount, failCount, totalCount },
				`Job '${selectedJobName}' completed: ${successCount}/${totalCount} successful, ${failCount} failed`
			);
			if (successCount > 0) {
				try {
					const aggregatedOutputPath = generateOutputFileName(selectedJobName);
					await mkdir(dirname(aggregatedOutputPath), { recursive: true });
					const writeStream = createWriteStream(aggregatedOutputPath, {
						encoding: 'utf-8',
					});
					writeStream.write('[\n');
					let itemCount = 0;
					let isFirstFile = true;
					for (const { outputFile } of results.successful) {
						const content = await readFile(outputFile, 'utf-8');
						const data = JSON.parse(content);
						const items = Array.isArray(data) ? data : [data];
						for (const item of items) {
							if (!isFirstFile) {
								writeStream.write(',\n');
							}
							writeStream.write('  ');
							writeStream.write(JSON.stringify(item, null, 2).replace(/\n/g, '\n  '));
							isFirstFile = false;
							itemCount++;
						}
					}
					writeStream.write('\n]\n');
					writeStream.end();
					await new Promise<void>((resolve, reject) => {
						writeStream.on('finish', () => resolve());
						writeStream.on('error', reject);
					});
					logger.info(
						{
							job: selectedJobName,
							itemCount,
							outputFile: aggregatedOutputPath,
						},
						`Streamed ${itemCount} items to ${aggregatedOutputPath}`
					);
				} catch (error) {
					logger.error(
						{
							job: selectedJobName,
							error: error instanceof Error ? error.message : error,
						},
						`Failed to aggregate outputs for job '${selectedJobName}'`
					);
				}
			} else {
				logger.info(
					{ job: selectedJobName },
					`Skipping aggregation for '${selectedJobName}' - no successful configs`
				);
			}
		} finally {
			try {
				await rm(tempDir, { recursive: true, force: true });
				logger.debug({ tempDir }, 'Cleaned up temp directory');
			} catch (error) {
				logger.warn(
					{
						tempDir,
						error: error instanceof Error ? error.message : error,
					},
					'Failed to cleanup temp directory'
				);
			}
		}
	});
program
	.command('batch [names...]')
	.description('Run one or more jobs')
	.option('-q, --queue', 'Queue jobs for worker instead of running directly')
	.action(async (names: string[], options: { queue?: boolean }) => {
		let selectedBatches: string[];
		if (!names || names.length === 0) {
			const availableJobs = getAllJobNames();
			if (availableJobs.length === 0) {
				logger.error('No jobs found in configurations');
				process.exit(1);
			}
			const jobChoices = availableJobs.map((name) => {
				const count = getJobConfigs(name as JobName).length;
				return {
					name: `${name} (${count} ${count === 1 ? 'config' : 'configs'})`,
					value: name,
				};
			});
			const jobAnswer = await inquirer.prompt({
				type: 'checkbox',
				name: 'jobs',
				message: 'Select jobs to crawl:',
				choices: jobChoices,
				validate: (answer) => {
					if ((answer as unknown as string[]).length === 0) {
						return 'You must select at least one job';
					}
					return true;
				},
			});
			selectedBatches = jobAnswer.jobs;
		} else {
			selectedBatches = names;
		}
		for (const name of selectedBatches) {
			const jobConfigs = getJobConfigs(name as JobName);
			if (!jobConfigs || jobConfigs.length === 0) {
				logger.error({ job: name }, `Job '${name}' not found or is empty`);
				const availableJobs = getAllJobNames();
				logger.info(`Available jobs: ${availableJobs.join(', ')}`);
				process.exit(1);
			}
		}
		let useQueue = options.queue ?? false;
		if (options.queue === undefined) {
			const modeAnswer = await inquirer.prompt({
				type: 'list',
				name: 'mode',
				message: 'How do you want to run the crawl?',
				choices: [
					{
						name: 'Run directly (wait for completion)',
						value: 'direct',
					},
					{
						name: 'Queue for worker (async)',
						value: 'queue',
					},
				],
			});
			useQueue = modeAnswer.mode === 'queue';
		}
		const allConfigs: Array<{ jobName: string; config: NamedConfig }> = [];
		for (const jobName of selectedBatches) {
			const jobConfigs = getJobConfigs(jobName as JobName);
			for (const config of jobConfigs) {
				allConfigs.push({ jobName, config });
			}
		}
		logger.info(
			{
				jobs: selectedBatches.join(', '),
				totalConfigs: allConfigs.length,
				mode: useQueue ? 'queue' : 'direct',
			},
			`Starting crawl for ${selectedBatches.length} ${
				selectedBatches.length === 1 ? 'job' : 'jobs'
			} (${allConfigs.length} total ${
				allConfigs.length === 1 ? 'config' : 'configs'
			})`
		);
		if (useQueue) {
			const jobIds: string[] = [];
			for (const { jobName, config } of allConfigs) {
				const jobId = randomUUID();
				const outputFileName = generateOutputFileName(jobName);
				const configWithFileName = {
					...config,
					outputFileName,
				};
				jobStore.createJob(jobId, configWithFileName);
				await crawlQueue.add('crawl', { config: configWithFileName, jobName }, { jobId });
				jobIds.push(jobId);
				logger.info(
					{ jobId, job: jobName, outputFile: outputFileName },
					`Queued: ${jobName} (job ID: ${jobId})`
				);
			}
			logger.info(
				{ totalJobs: jobIds.length },
				`Successfully queued ${jobIds.length} ${
					jobIds.length === 1 ? 'job' : 'jobs'
				}`
			);
			logger.info('Worker will process these jobs asynchronously');
			logger.info('Check job status via API: GET /crawl/status/{jobId}');
		} else {
			const tempDir = await mkdtemp(join(tmpdir(), 'context-crawler-'));
			try {
				const jobResults: Record<
					string,
					{
						successful: Array<{ config: NamedConfig; outputFile: string }>;
						failed: Array<{ config: NamedConfig; error: string }>;
					}
				> = {};
				for (const jobName of selectedBatches) {
					jobResults[jobName] = {
						successful: [],
						failed: [],
					};
				}
				for (let i = 0; i < allConfigs.length; i++) {
					const { jobName, config } = allConfigs[i]!;
					logger.info(
						{
							progress: `${i + 1}/${allConfigs.length}`,
							job: jobName,
						},
						`Crawling config ${i + 1}/${allConfigs.length} (from ${jobName})`
					);
					const tempOutputPath = join(tempDir, `${jobName}-${i}.json`);
					const tempConfig = { ...config, outputFileName: tempOutputPath };
					const result = await runTask(tempConfig, jobName);
					if (result.success && result.outputFile) {
						jobResults[jobName]!.successful.push({
							config,
							outputFile: result.outputFile.toString(),
						});
						logger.info(
							{
								progress: `${i + 1}/${allConfigs.length}`,
								job: jobName,
							},
							`Completed config ${i + 1}/${allConfigs.length}`
						);
					} else {
						jobResults[jobName]!.failed.push({
							config,
							error: result.error || 'Unknown error',
						});
						logger.error(
							{
								progress: `${i + 1}/${allConfigs.length}`,
								job: jobName,
								error: result.error,
							},
							`Failed config ${i + 1}/${allConfigs.length}`
						);
					}
				}
				for (const jobName of selectedBatches) {
					const results = jobResults[jobName]!;
					const successCount = results.successful.length;
					const failCount = results.failed.length;
					const totalCount = successCount + failCount;
					logger.info(
						{ job: jobName, successCount, failCount, totalCount },
						`Job '${jobName}' completed: ${successCount}/${totalCount} successful, ${failCount} failed`
					);
					if (successCount > 0) {
						try {
							const aggregatedOutputPath = generateOutputFileName(jobName);
							await mkdir(dirname(aggregatedOutputPath), { recursive: true });
							const writeStream = createWriteStream(aggregatedOutputPath, {
								encoding: 'utf-8',
							});
							writeStream.write('[\n');
							let itemCount = 0;
							let isFirstFile = true;
							for (const { outputFile } of results.successful) {
								const content = await readFile(outputFile, 'utf-8');
								const data = JSON.parse(content);
								const items = Array.isArray(data) ? data : [data];
								for (const item of items) {
									if (!isFirstFile) {
										writeStream.write(',\n');
									}
									writeStream.write('  ');
									writeStream.write(JSON.stringify(item, null, 2).replace(/\n/g, '\n  '));
									isFirstFile = false;
									itemCount++;
								}
							}
							writeStream.write('\n]\n');
							writeStream.end();
							await new Promise<void>((resolve, reject) => {
								writeStream.on('finish', () => resolve());
								writeStream.on('error', reject);
							});
							logger.info(
								{
									job: jobName,
									itemCount,
									outputFile: aggregatedOutputPath,
								},
								`Streamed ${itemCount} items to ${aggregatedOutputPath}`
							);
						} catch (error) {
							logger.error(
								{
									job: jobName,
									error: error instanceof Error ? error.message : error,
								},
								`Failed to aggregate outputs for job '${jobName}'`
							);
						}
					} else {
						logger.info(
							{ job: jobName },
							`Skipping aggregation for '${jobName}' - no successful configs`
						);
					}
				}
				const totalSuccessful = Object.values(jobResults).reduce(
					(sum, r) => sum + r.successful.length,
					0
				);
				const totalFailed = Object.values(jobResults).reduce(
					(sum, r) => sum + r.failed.length,
					0
				);
				logger.info(
					{
						jobs: selectedBatches.join(', '),
						totalSuccessful,
						totalFailed,
						total: allConfigs.length,
					},
					`All jobs completed: ${totalSuccessful}/${allConfigs.length} successful, ${totalFailed} failed`
				);
			} finally {
				try {
					await rm(tempDir, { recursive: true, force: true });
					logger.debug({ tempDir }, 'Cleaned up temp directory');
				} catch (error) {
					logger.warn(
						{
							tempDir,
							error: error instanceof Error ? error.message : error,
						},
						'Failed to cleanup temp directory'
					);
				}
			}
		}
	});
program
	.command('list')
	.description('List all available jobs and their config counts')
	.action(() => {
		const jobNames = getAllJobNames();
		console.log('\nAvailable Jobs:');
		if (jobNames.length === 0) {
			console.log('  (none found)');
		} else {
			jobNames.forEach((jobName: string) => {
				const configs = getJobConfigs(jobName as JobName);
				const configCount = configs.length;
				console.log(
					`  - ${jobName} (${configCount} ${configCount === 1 ? 'config' : 'configs'})`
				);
			});
		}
		console.log();
	});
program.parse();
</file>

<file path="src/schema.ts">
import { configDotenv } from 'dotenv';
import type { Page } from 'playwright';
import { z } from 'zod';
configDotenv();
export interface CrawledData {
	title: string;
	url: string;
	html: string;
	[key: string]: unknown;
}
export type OnVisitPageHook = (context: {
	page: Page;
	pushData: (data: Record<string, unknown>) => Promise<void>;
}) => Promise<void>;
const Page: z.ZodType<Page> = z.custom<Page>((val) => {
	return (
		typeof val === 'object' &&
		val !== null &&
		'goto' in val &&
		'evaluate' in val &&
		'title' in val
	);
});
const onVisitPageHook: z.ZodType<OnVisitPageHook> = z.custom<OnVisitPageHook>(
	(val) => {
		return typeof val === 'function';
	}
);
export const globalConfigSchema = z.object({
	maxPagesToCrawl: z.union([z.number(), z.literal('unlimited')]),
	maxTokens: z.union([z.number(), z.literal('unlimited')]),
});
export type GlobalConfig = z.infer<typeof globalConfigSchema>;
export const configSchema = z.object({
	entry: z.string().url(),
	match: z.union([z.string(), z.array(z.string())]),
	exclude: z.union([z.string(), z.array(z.string())]).optional(),
	selector: z.string(),
	autoDiscoverNav: z.boolean().optional().default(true),
	discoverySelector: z
		.string()
		.optional()
		.default("nav, aside, [role='navigation']"),
	outputFileName: z.string().optional(),
	cookie: z
		.union([
			z.object({
				name: z.string(),
				value: z.string(),
			}),
			z.array(
				z.object({
					name: z.string(),
					value: z.string(),
				})
			),
		])
		.optional(),
	onVisitPage: onVisitPageHook.optional(),
	waitForSelectorTimeout: z.number().optional(),
	resourceExclusions: z.array(z.string()).optional(),
	maxFileSize: z.number().optional(),
	storageDir: z.string().optional(),
	datasetName: z.string().optional(),
});
export type Config = z.infer<typeof configSchema>;
export type ConfigInput = z.input<typeof configSchema>;
export type NamedConfig = Config;
export function generateNameFromUrl(url: string): string {
	try {
		const urlObj = new URL(url);
		const domain = urlObj.hostname.replace(/^www\./, '').split('.')[0];
		// Extract only the first path segment (ignore query params and fragments)
		const pathSegments = urlObj.pathname
			.split('/')
			.filter((segment) => segment.length > 0);
		// Combine domain and first path segment only
		const parts =
			pathSegments.length > 0 ? [domain, pathSegments[0]] : [domain];
		return parts.join('-').toLowerCase();
	} catch (error) {
		// Fallback to a simple sanitized version of the URL
		return url
			.replace(/[^a-zA-Z0-9]/g, '-')
			.replace(/-+/g, '-')
			.replace(/^-|-$/g, '')
			.toLowerCase();
	}
}
export function generateOutputFileName(jobName: string): string {
	return `output/jobs/${jobName}.json`;
}
</file>

<file path="src/config.ts">
import type { JobRegistry } from "../configurations/index.js";
import { jobs as configuredJobs } from "../configurations/index.js";
import { globalConfig } from "../configurations/global.config.js";
import { NamedConfig } from "./schema.js";
export { globalConfig };
const jobs = Object.freeze(configuredJobs) as Readonly<JobRegistry>;
export type JobName = keyof JobRegistry;
const jobNames = Object.freeze(Object.keys(jobs)) as readonly JobName[];
const jobNameSet = new Set<string>(jobNames);
export function isJobName(value: string): value is JobName {
  return jobNameSet.has(value);
}
export function getJobConfigs(
  jobName: JobName,
): readonly NamedConfig[];
export function getJobConfigs(
  jobName: string,
): readonly NamedConfig[];
export function getJobConfigs(
  jobName: string,
): readonly NamedConfig[] {
  if (!isJobName(jobName)) {
    return [];
  }
  return jobs[jobName];
}
export function getAllJobNames(): string[] {
  return Array.from(jobNames);
}
export function getAllConfigs(): NamedConfig[] {
  const entries = Object.entries(jobs) as Array<
    [keyof JobRegistry, readonly NamedConfig[]]
  >;
  const aggregated: NamedConfig[] = [];
  for (const [_jobName, jobConfigs] of entries) {
    for (const config of jobConfigs) {
      aggregated.push(config);
    }
  }
  return aggregated;
}
</file>

<file path="src/server.ts">
import cors from 'cors';
import { randomUUID } from 'crypto';
import { configDotenv } from 'dotenv';
import express, { Express, NextFunction, Request, Response } from 'express';
import { createReadStream, existsSync } from 'fs';
import { readFile, stat } from 'fs/promises';
import { glob } from 'glob';
import net from 'net';
import { join, parse } from 'path';
import swaggerUi from 'swagger-ui-express';
import {
	getAllJobNames,
	getJobConfigs,
} from './config.js';
import { jobStore } from './job-store.js';
import logger from './logger.js';
import { llmService } from './llm-service.js';
import { crawlQueue } from './queue.js';
import { Config, configSchema, generateOutputFileName } from './schema.js';
configDotenv();
const DEFAULT_PORT = 5000;
const JOBS_OUTPUT_DIR = join(process.cwd(), 'output', 'jobs');
const INDEXES_DIR = join(process.cwd(), 'data', 'indexes');
function parsePort(value: string | undefined): number | undefined {
	if (!value) {
		return undefined;
	}
	const portNumber = Number(value);
	if (
		Number.isNaN(portNumber) ||
		!Number.isInteger(portNumber) ||
		portNumber <= 0 ||
		portNumber > 65535
	) {
		return undefined;
	}
	return portNumber;
}
async function isPortAvailable(host: string, port: number): Promise<boolean> {
	return await new Promise<boolean>((resolve, reject) => {
		const tester = net
			.createServer()
			.once('error', (error: NodeJS.ErrnoException) => {
				if (error.code === 'EADDRINUSE' || error.code === 'EACCES') {
					resolve(false);
				} else {
					reject(error);
				}
			})
			.once('listening', () => {
				tester.close(() => resolve(true));
			})
			.listen(port, host);
	});
}
async function findAvailablePort(
	host: string,
	startPort: number,
	strict: boolean
): Promise<number> {
	const maxAttempts = strict ? 1 : 20;
	for (let attempt = 0; attempt < maxAttempts; attempt++) {
		const candidatePort = startPort + attempt;
		const available = await isPortAvailable(host, candidatePort);
		if (available) {
			return candidatePort;
		}
	}
	throw new Error(
		`Unable to find an available port starting at ${startPort} for host '${host}'`
	);
}
const app: Express = express();
const hostname = process.env.API_HOST || 'localhost';
const apiPortRaw = process.env.API_PORT;
const portEnvRaw = process.env.PORT;
const parsedApiPort = parsePort(apiPortRaw);
const parsedPortEnv = parsePort(portEnvRaw);
const preferredPort = parsedApiPort ?? parsedPortEnv ?? DEFAULT_PORT;
const hasExplicitPort =
	Boolean(parsedPortEnv) ||
	Boolean(
		parsedApiPort !== undefined &&
			parsedApiPort !== DEFAULT_PORT
	);
if (!parsedApiPort && apiPortRaw) {
	logger.warn(
		{ value: apiPortRaw },
		'Invalid API_PORT value detected. Falling back to default port handling.'
	);
}
if (!parsedPortEnv && portEnvRaw) {
	logger.warn(
		{ value: portEnvRaw },
		'Invalid PORT value detected. Falling back to default port handling.'
	);
}
if ((!apiPortRaw && !portEnvRaw) || !process.env.API_HOST) {
	logger.warn(
		{ preferredPort, hostname },
		'Using default server configuration. Create a .env file from .env.example to customize.'
	);
}
function authenticateApiKey(req: Request, res: Response, next: NextFunction): void {
	const apiKey = process.env.API_KEY;
	if (!apiKey) {
		next();
		return;
	}
	const requestApiKey = req.headers['x-api-key'];
	if (!requestApiKey || requestApiKey !== apiKey) {
		logger.warn({ path: req.path }, 'Unauthorized API access attempt');
		res.status(401).json({ message: 'Unauthorized' });
		return;
	}
	next();
}
function registerRoutes(): void {
	app.post('/crawl', authenticateApiKey, async (req: Request, res: Response) => {
		const { name, config: customConfig } = req.body;
		let config: Config | undefined;
		let jobName: string | undefined;
		if (name && typeof name === 'string') {
			const configs = getJobConfigs(name);
			if (!configs || configs.length === 0) {
				logger.warn({ name }, 'Job not found');
				res.status(404).json({ message: `Job with name '${name}' not found.` });
				return;
			}
			jobName = name;
			config = configs[0];
		} else if (customConfig && typeof customConfig === 'object') {
			const validationResult = configSchema.safeParse(customConfig);
			if (!validationResult.success) {
				logger.warn(
					{ errors: validationResult.error.issues },
					'Invalid custom config'
				);
				res.status(400).json({
					message: 'Invalid configuration',
					errors: validationResult.error.issues,
				});
				return;
			}
			config = validationResult.data;
			jobName = 'custom';
		} else {
			res.status(400).json({
				message: "Invalid request body. Either 'name' (job name) or 'config' is required.",
			});
			return;
		}
		try {
			const jobId = randomUUID();
			if (!config || !jobName) {
				throw new Error('Config or job name is missing');
			}
			const configWithFileName: Config = {
				...config,
				outputFileName: generateOutputFileName(jobName),
			};
			jobStore.createJob(jobId, configWithFileName);
			await crawlQueue.add('crawl', { config: configWithFileName, jobName }, { jobId });
			logger.info({ jobId, jobName }, 'Crawl job queued');
			res.status(202).json({
				jobId,
				jobName,
				message: 'Crawl job started',
				statusUrl: `/crawl/status/${jobId}`,
				resultsUrl: `/crawl/results/${jobId}`,
			});
		} catch (error) {
			logger.error({ error }, 'Error starting job');
			res.status(500).json({ message: 'Failed to start crawl job.' });
		}
	});
	app.post('/crawl/batch', authenticateApiKey, async (req: Request, res: Response) => {
		const { name } = req.body;
		if (!name || typeof name !== 'string') {
			res.status(400).json({
				message: "Invalid request body. 'name' (job name) is required.",
			});
			return;
		}
		const configs = getJobConfigs(name);
		if (!configs || configs.length === 0) {
			logger.warn({ jobName: name }, 'Job not found or has no configs');
			res.status(404).json({
				message: `Job with name '${name}' not found or has no configs.`,
				availableJobs: getAllJobNames(),
			});
			return;
		}
		try {
			const queuedConfigs: Array<{
				configIndex: number;
				jobId: string;
				statusUrl: string;
				resultsUrl: string;
			}> = [];
			for (let i = 0; i < configs.length; i++) {
				const config = configs[i]!;
				const jobId = randomUUID();
				const configWithFileName = {
					...config,
					outputFileName: generateOutputFileName(name),
				};
				jobStore.createJob(jobId, configWithFileName);
				await crawlQueue.add('crawl', { config: configWithFileName, jobName: name }, { jobId });
				queuedConfigs.push({
					configIndex: i,
					jobId,
					statusUrl: `/crawl/status/${jobId}`,
					resultsUrl: `/crawl/results/${jobId}`,
				});
				logger.info(
					{ jobId, jobName: name, configIndex: i },
					'Config queued for batch job'
				);
			}
			logger.info(
				{ jobName: name, configCount: queuedConfigs.length },
				`Batch job '${name}' queued with ${queuedConfigs.length} configs`
			);
			res.status(202).json({
				message: `Batch job '${name}' queued with ${queuedConfigs.length} ${
					queuedConfigs.length === 1 ? 'config' : 'configs'
				}.`,
				jobName: name,
				configCount: queuedConfigs.length,
				configs: queuedConfigs,
			});
		} catch (error) {
			logger.error({ error, jobName: name }, 'Error queuing batch job');
			res.status(500).json({ message: 'Failed to queue batch job.' });
		}
	});
	app.get('/crawl/status/:jobId', authenticateApiKey, (req: Request, res: Response) => {
		const jobId = req.params.jobId;
		if (!jobId) {
			res.status(400).json({ message: 'Job ID is required' });
			return;
		}
		const job = jobStore.getJobById(jobId);
		if (!job) {
			logger.warn({ jobId }, 'Job not found');
			res.status(404).json({ message: 'Job not found' });
			return;
		}
		res.json({
			jobId: job.id,
			status: job.status,
			createdAt: job.createdAt,
			completedAt: job.completedAt,
			...(job.status === 'failed' && { error: job.error }),
		});
	});
	app.get('/crawl/results/:jobId', authenticateApiKey, async (req: Request, res: Response) => {
		const jobId = req.params.jobId;
		if (!jobId) {
			res.status(400).json({ message: 'Job ID is required' });
			return;
		}
		const job = jobStore.getJobById(jobId);
		if (!job) {
			logger.warn({ jobId }, 'Job not found');
			res.status(404).json({ message: 'Job not found' });
			return;
		}
		if (job.status === 'pending' || job.status === 'running') {
			res.status(202).json({
				message: 'Job is still processing',
				status: job.status,
				statusUrl: `/crawl/status/${jobId}`,
			});
			return;
		}
		if (job.status === 'failed') {
			logger.warn({ jobId, error: job.error }, 'Failed job results requested');
			res.status(500).json({
				message: 'Job failed',
				error: job.error,
			});
			return;
		}
		if (!job.outputFile) {
			logger.warn({ jobId }, 'No output file generated');
			res.status(404).json({
				message: 'No output file generated',
			});
			return;
		}
		try {
			await stat(job.outputFile);
			res.contentType('application/json');
			const fileStream = createReadStream(job.outputFile, 'utf-8');
			fileStream.pipe(res);
			return;
		} catch (error) {
			logger.error({ jobId, error }, 'Error reading output file');
			res.status(500).json({ message: 'Error reading output file' });
		}
	});
	app.get('/configurations', authenticateApiKey, async (_req: Request, res: Response) => {
		try {
			const jobNames = getAllJobNames();
			const jobs = jobNames.map((jobName) => {
				const configs = getJobConfigs(jobName);
				return {
					name: jobName,
					configCount: configs.length,
					outputFileName: generateOutputFileName(jobName),
				};
			});
			res.json({
				jobs,
			});
		} catch (error) {
			logger.error({ error }, 'Error fetching configurations');
			res.status(500).json({ message: 'Error fetching configurations' });
		}
	});
	app.get('/get/:jobName/llms.txt', async (req: Request, res: Response) => {
		const { jobName } = req.params;
		const subject = req.query.subject as string | undefined;
		const kParam = req.query.k as string | undefined;
		const k = kParam ? parseInt(kParam, 10) : 5;
		if (Number.isNaN(k) || k <= 0 || k > 20) {
			res.status(400).json({
				message: 'Query parameter "k" must be a number between 1 and 20.',
			});
			return;
		}
		if (!jobName || !llmService.jobExists(jobName)) {
			res.status(404).json({
				message: `Knowledge file for job '${jobName}' not found.`,
				availableJobs: getAllJobNames(),
			});
			return;
		}
		try {
			res.setHeader('Content-Type', 'text/plain; charset=utf-8');
			if (subject && subject.trim() !== '' && subject.trim() !== '*') {
				logger.info({ job: jobName, subject, k }, 'Performing semantic search');
				const searchResults = await llmService.search(jobName, subject, k);
				res.send(searchResults);
				return;
			}
			logger.info({ job: jobName }, 'Streaming full llms.txt file');
			const stream = llmService.getFullTextStream(jobName);
			stream.pipe(res);
		} catch (error) {
			const errorMessage = error instanceof Error ? error.message : 'Unknown error';
			logger.error(
				{ job: jobName, error: errorMessage },
				'Error serving LLM content',
			);
			res
				.status(500)
				.send(`Error processing request for '${jobName}': ${errorMessage}`);
		}
	});
	app.get('/llms/status', authenticateApiKey, async (_req: Request, res: Response) => {
		try {
			const jobFiles = await glob(`${JOBS_OUTPUT_DIR}/*.json`);
			const statuses = await Promise.all(
				jobFiles.map(async (jobFile) => {
					const jobName = parse(jobFile).name;
					const indexPath = join(INDEXES_DIR, `${jobName}.index`);
					const isIndexed = existsSync(indexPath);
					const isStale = await llmService.isArtifactStale(jobName, jobFile);
					let lastModified: Date | null = null;
					if (isIndexed) {
						lastModified = (await stat(indexPath)).mtime;
					}
					return {
						jobName,
						isIndexed,
						isStale,
						isUpToDate: isIndexed && !isStale,
						lastModified,
					};
				}),
			);
			res.json(statuses);
		} catch (error) {
			logger.error({ error }, 'Failed to get LLM artifact statuses');
			res.status(500).json({ message: 'Could not retrieve artifact statuses.' });
		}
	});
}
(async () => {
	const swaggerDocument = JSON.parse(
		await readFile(new URL('../swagger-output.json', import.meta.url), 'utf-8')
	) as Record<string, unknown>;
	app.use(cors());
	app.use(express.json());
	app.use(
		'/api-docs',
		...(swaggerUi.serve as unknown as express.RequestHandler[]),
		swaggerUi.setup(swaggerDocument) as unknown as express.RequestHandler
	);
	registerRoutes();
	await crawlQueue.initialize();
	let port = preferredPort;
	try {
		port = await findAvailablePort(hostname, preferredPort, hasExplicitPort);
		if (!hasExplicitPort && port !== preferredPort) {
			logger.warn(
				{ requestedPort: preferredPort, fallbackPort: port },
				`Preferred port ${preferredPort} is unavailable. Using ${port} instead.`
			);
		}
	} catch (error) {
		logger.error(
			{
				error: error instanceof Error ? error.message : error,
				host: hostname,
				requestedPort: preferredPort,
			},
			'Failed to resolve a listening port'
		);
		process.exit(1);
	}
	process.env.API_PORT = port.toString();
	const server = app.listen(port, hostname, () => {
		logger.info(`API server listening at http://${hostname}:${port}`);
		logger.info(
			{
				address: server.address(),
				listening: server.listening,
				constructor: server.constructor.name,
			},
			'HTTP server address info'
		);
	});
	server.on('close', () => {
		logger.info('HTTP server close event emitted');
	});
	server.on('listening', () => {
		logger.info('HTTP server listening event emitted');
	});
	server.on('error', (error) => {
		logger.error({ error }, 'HTTP server error event emitted');
	});
	setImmediate(() => {
		logger.info(
			{ address: server.address(), listening: server.listening },
			'HTTP server immediate state'
		);
	});
	setTimeout(() => {
		logger.info({ address: server.address() }, 'HTTP server address after delay');
	}, 1000).unref();
	let isShuttingDown = false;
	let resolveShutdown: ((code: number) => void) | undefined;
	const shutdownComplete = new Promise<number>((resolve) => {
		resolveShutdown = resolve;
	});
	async function shutdown(signal: string, code = 0): Promise<void> {
		if (isShuttingDown) {
			return;
		}
		isShuttingDown = true;
		logger.info({ signal }, 'Shutdown signal received');
		if (server.listening) {
			await new Promise<void>((resolve) => {
				server.close(() => {
					logger.info('HTTP server closed');
					resolve();
				});
			});
		} else {
			logger.info('HTTP server already closed');
		}
		try {
			crawlQueue.close();
			logger.info('Queue connection closed');
		} catch (error) {
			logger.error(
				{ error: error instanceof Error ? error.message : error },
				'Error closing queue connection'
			);
		}
		try {
			jobStore.close();
			logger.info('Job store connection closed');
		} catch (error) {
			logger.error(
				{ error: error instanceof Error ? error.message : error },
				'Error closing job store connection'
			);
		}
		logger.info('Server shutdown complete');
		if (resolveShutdown) {
			resolveShutdown(code);
			resolveShutdown = undefined;
		}
	}
	process.on('SIGTERM', () => shutdown('SIGTERM'));
	process.on('SIGINT', () => shutdown('SIGINT'));
	process.on('uncaughtException', (error) => {
		logger.error(
			{ error: error.message, stack: error.stack },
			'Uncaught exception'
		);
		shutdown('uncaughtException', 1);
	});
	process.on('unhandledRejection', (reason) => {
		logger.error(
			{ reason: reason instanceof Error ? reason.message : reason },
			'Unhandled rejection'
		);
		shutdown('unhandledRejection', 1);
	});
	const exitCode = await shutdownComplete;
	process.exit(exitCode);
})().catch((error) => {
	logger.error({ error }, 'Failed to start server');
	process.exit(1);
});
export default app;
</file>

<file path="src/core.ts">
import {
	Configuration,
	Dataset,
	downloadListOfUrls,
	PlaywrightCrawler,
} from 'crawlee';
import { randomBytes } from 'crypto';
import { PathLike } from 'fs';
import { mkdir, rm, writeFile } from 'fs/promises';
import { isWithinTokenLimit } from 'gpt-tokenizer';
import { minimatch } from 'minimatch';
import { basename, dirname, isAbsolute, join } from 'path';
import { Page } from 'playwright';
import { globalConfig } from './config.js';
import logger from './logger.js';
import {
	Config,
	configSchema,
	CrawledData,
	generateOutputFileName,
} from './schema.js';
let pageCounter = 0;
type ConfigWithDataset = Config & { datasetName: string };
export function getPageHtml(page: Page, selector = 'body') {
	return page.evaluate((selector) => {
		if (selector.startsWith('/')) {
			const elements = document.evaluate(
				selector,
				document,
				null,
				XPathResult.ANY_TYPE,
				null
			);
			let result = elements.iterateNext();
			return result ? result.textContent || '' : '';
		} else {
			// Handle as a CSS selector
			const el = document.querySelector(selector) as HTMLElement | null;
			return el?.innerText || '';
		}
	}, selector);
}
/**
 * Expand exclude patterns to match both the path and all subpaths
 * Example: /support becomes ['/support', '/support/**']
 * Patterns with wildcards are kept as-is to avoid breaking glob semantics
 */
function expandExcludePatterns(patterns: string[]): string[] {
	const expanded = new Set<string>();
	for (const pattern of patterns) {
		expanded.add(pattern);
		if (!pattern.includes('*') && !pattern.endsWith('/')) {
			expanded.add(`${pattern}/**`);
		}
	}
	return Array.from(expanded);
}
function normalizeAndExpandExcludes(exclude?: string | string[]): string[] {
	const patterns = typeof exclude === 'string' ? [exclude] : (exclude ?? []);
	return expandExcludePatterns(patterns);
}
async function discoverNavigationUrls(
	page: Page,
	discoverySelector: string,
	matchPatterns: string[],
	excludePatterns: string[]
): Promise<string[]> {
	const navUrls = await page.evaluate((selector) => {
		const navElements = document.querySelectorAll(selector);
		const urls: string[] = [];
		navElements.forEach((nav) => {
			const links = nav.querySelectorAll('a[href]');
			links.forEach((link) => {
				const href = (link as HTMLAnchorElement).href;
				if (href && !href.startsWith('javascript:') && !href.startsWith('#')) {
					urls.push(href);
				}
			});
		});
		return Array.from(new Set(urls));
	}, discoverySelector);
	const expandedExcludes = expandExcludePatterns(excludePatterns);
	const filteredUrls = navUrls.filter((url) => {
		const matchesInclude = matchPatterns.some((pattern) =>
			minimatch(url, pattern)
		);
		if (!matchesInclude) {
			return false;
		}
		const matchesExclude = expandedExcludes.some((pattern) =>
			minimatch(url, pattern)
		);
		return !matchesExclude;
	});
	return filteredUrls;
}
export async function waitForXPath(page: Page, xpath: string, timeout: number) {
	await page.waitForFunction(
		(xpath) => {
			const elements = document.evaluate(
				xpath,
				document,
				null,
				XPathResult.ANY_TYPE,
				null
			);
			return elements.iterateNext() !== null;
		},
		xpath,
		{ timeout }
	);
}
export async function crawl(config: ConfigWithDataset) {
	configSchema.parse(config);
	if (process.env.NO_CRAWL !== 'true') {
		const storageDir =
			config.storageDir ||
			join(process.cwd(), 'storage', 'jobs', config.datasetName);
		await mkdir(storageDir, { recursive: true });
		const crawler = new PlaywrightCrawler(
			{
				async requestHandler({ request, page, enqueueLinks, log, pushData }) {
					const title = await page.title();
					pageCounter++;
					const maxPages = globalConfig.maxPagesToCrawl === 'unlimited'
						? 'unlimited'
						: globalConfig.maxPagesToCrawl;
					log.info(
						`Crawling: Page ${pageCounter} / ${maxPages} - URL: ${request.loadedUrl}...`
					);
					if (config.selector) {
						if (config.selector.startsWith('/')) {
							await waitForXPath(
								page,
								config.selector,
								config.waitForSelectorTimeout ?? 5000
							);
						} else {
							await page.waitForSelector(config.selector, {
								timeout: config.waitForSelectorTimeout ?? 5000,
							});
						}
					}
					const html = await getPageHtml(page, config.selector);
					await pushData({ title, url: request.loadedUrl, html });
					if (config.onVisitPage) {
						await config.onVisitPage({ page, pushData });
					}
					await enqueueLinks({
						globs:
							typeof config.match === 'string' ? [config.match] : config.match,
						exclude: normalizeAndExpandExcludes(config.exclude),
					});
				},
				...(globalConfig.maxPagesToCrawl !== 'unlimited' && {
					maxRequestsPerCrawl: globalConfig.maxPagesToCrawl,
				}),
				maxConcurrency: 2,
				maxRequestRetries: 2,
				preNavigationHooks: [
					async (crawlingContext, _gotoOptions) => {
						const { request, page, log } = crawlingContext;
						if (config.cookie) {
							const cookies = (
								Array.isArray(config.cookie) ? config.cookie : [config.cookie]
							).map((cookie) => {
								return {
									name: cookie.name,
									value: cookie.value,
									url: request.url,
								};
							});
							await page.context().addCookies(cookies);
						}
						const RESOURCE_EXCLUSTIONS = config.resourceExclusions ?? [];
						if (RESOURCE_EXCLUSTIONS.length === 0) {
							return;
						}
						await page.route(
							`**\/*.{${RESOURCE_EXCLUSTIONS.join()}}`,
							(route) => route.abort('aborted')
						);
						log.info(
							`Aborting requests for as this is a resource excluded route`
						);
					},
				],
			},
			new Configuration({
				purgeOnStart: false,
				defaultDatasetId: config.datasetName,
				persistStorage: true,
				storageClientOptions: {
					localDataDirectory: storageDir,
				},
			})
		);
		let seedUrls = [config.entry];
		const autoDiscoverNav = config.autoDiscoverNav ?? true;
		if (autoDiscoverNav) {
			try {
				logger.info('Starting navigation discovery phase...');
				const { chromium } = await import('playwright');
				const browser = await chromium.launch({ headless: true });
				const context = await browser.newContext();
				const page = await context.newPage();
				if (config.cookie) {
					const cookies = (
						Array.isArray(config.cookie) ? config.cookie : [config.cookie]
					).map((cookie) => ({
						name: cookie.name,
						value: cookie.value,
						url: config.entry,
					}));
					await context.addCookies(cookies);
				}
				await page.goto(config.entry, { waitUntil: 'domcontentloaded' });
				const matchPatterns = Array.isArray(config.match)
					? config.match
					: [config.match];
				const excludePatterns =
					typeof config.exclude === 'string'
						? [config.exclude]
						: (config.exclude ?? []);
				const discoverySelector =
					config.discoverySelector ?? "nav, aside, [role='navigation']";
				const discoveredUrls = await discoverNavigationUrls(
					page,
					discoverySelector,
					matchPatterns,
					excludePatterns
				);
				await browser.close();
				logger.info(
					{ count: discoveredUrls.length },
					`Discovered ${discoveredUrls.length} URLs from navigation`
				);
				seedUrls = Array.from(new Set([...seedUrls, ...discoveredUrls]));
				const expandedExcludes = expandExcludePatterns(excludePatterns);
				seedUrls = seedUrls.filter((url) => {
					return !expandedExcludes.some((pattern) => minimatch(url, pattern));
				});
				logger.info(
					{ total: seedUrls.length },
					`Total seed URLs: ${seedUrls.length}`
				);
			} catch (error) {
				logger.warn(
					{ error: error instanceof Error ? error.message : error },
					'Navigation discovery failed, continuing with provided URLs'
				);
			}
		}
		const sitemapUrls = seedUrls.filter((url) => /sitemap.*\.xml$/.test(url));
		if (sitemapUrls.length > 0) {
			for (const sitemapUrl of sitemapUrls) {
				const listOfUrls = await downloadListOfUrls({ url: sitemapUrl });
				await crawler.addRequests(listOfUrls);
			}
			const regularUrls = seedUrls.filter(
				(url) => !/sitemap.*\.xml$/.test(url)
			);
			if (regularUrls.length > 0) {
				await crawler.addRequests(regularUrls);
			}
			await crawler.run();
		} else {
			await crawler.run(seedUrls);
		}
	}
}
export async function write(
	config: ConfigWithDataset
): Promise<PathLike | null> {
	let nextFileNameString: PathLike | null = null;
	const storageDir =
		config.storageDir ||
		join(process.cwd(), 'storage', 'jobs', config.datasetName);
	const dataset = await Dataset.open(config.datasetName, {
		storageClient: new Configuration({
			storageClientOptions: {
				localDataDirectory: storageDir,
			},
		}).getStorageClient(),
	});
	const itemCount = (await dataset.getInfo())?.itemCount || 0;
	logger.info(
		{ itemCount },
		`Found ${itemCount} items in dataset to process...`
	);
	let currentResults: CrawledData[] = [];
	let currentSize: number = 0;
	let fileCounter: number = 1;
	const maxBytes: number = config.maxFileSize
		? config.maxFileSize * 1024 * 1024
		: Infinity;
	const getStringByteSize = (str: string): number =>
		Buffer.byteLength(str, 'utf-8');
	const nextFileName = (): string =>
		`${config.outputFileName!.replace(/\.json$/, '')}-${fileCounter}.json`;
	const writeBatchToFile = async (): Promise<void> => {
		nextFileNameString = nextFileName();
		const dir = dirname(nextFileNameString as string);
		await mkdir(dir, { recursive: true });
		await writeFile(
			nextFileNameString,
			JSON.stringify(currentResults, null, 2)
		);
		logger.info(
			{ count: currentResults.length, file: nextFileNameString },
			`Wrote ${currentResults.length} items to ${nextFileNameString}`
		);
		currentResults = [];
		currentSize = 0;
		fileCounter++;
	};
	let estimatedTokens: number = 0;
	const addContentOrSplit = async (data: CrawledData): Promise<void> => {
		const contentString: string = JSON.stringify(data);
		if (globalConfig.maxTokens !== 'unlimited') {
			const tokenCount: number | false = isWithinTokenLimit(
				contentString,
				globalConfig.maxTokens
			);
			if (typeof tokenCount === 'number') {
				if (estimatedTokens + tokenCount > globalConfig.maxTokens) {
					if (currentResults.length > 0) {
						await writeBatchToFile();
					}
					estimatedTokens = Math.floor(tokenCount / 2);
					currentResults.push(data);
				} else {
					currentResults.push(data);
					estimatedTokens += tokenCount;
				}
			}
		} else {
			currentResults.push(data);
		}
		currentSize += getStringByteSize(contentString);
		if (currentSize > maxBytes) {
			await writeBatchToFile();
		}
	};
	await dataset.forEach(async (item) => {
		const data: CrawledData = item as CrawledData;
		await addContentOrSplit(data);
	});
	if (currentResults.length > 0) {
		if (fileCounter === 1) {
			const finalFileName = config.outputFileName!;
			const dir = dirname(finalFileName);
			await mkdir(dir, { recursive: true });
			await writeFile(finalFileName, JSON.stringify(currentResults, null, 2));
			logger.info(
				{ count: currentResults.length, file: finalFileName },
				`Wrote ${currentResults.length} items to ${finalFileName}`
			);
			nextFileNameString = finalFileName;
		} else {
			await writeBatchToFile();
		}
	}
	return nextFileNameString;
}
export async function cleanupJobStorage(
	datasetName: string,
	storageDir?: string
): Promise<void> {
	try {
		const targetDir =
			storageDir || join(process.cwd(), 'storage', 'jobs', datasetName);
		try {
			await rm(targetDir, { recursive: true, force: true });
			logger.debug(
				{ storageDir: targetDir },
				'Cleaned up job storage directory'
			);
		} catch (error) {
			if ((error as NodeJS.ErrnoException).code !== 'ENOENT') {
				throw error;
			}
		}
	} catch (error) {
		logger.warn(
			{ datasetName, error: error instanceof Error ? error.message : error },
			'Failed to clean up job storage'
		);
	}
}
class ContextCrawlerCore {
	config: Config;
	datasetName: string;
	storageDir: string;
	jobName: string;
	constructor(config: Config, jobName: string) {
		this.jobName = jobName;
		const sanitizedFileName = config.outputFileName
			? isAbsolute(config.outputFileName)
				? config.outputFileName
				: join('output/jobs', basename(config.outputFileName))
			: generateOutputFileName(jobName);
		this.config = {
			...config,
			outputFileName: sanitizedFileName,
		};
		this.datasetName = `ds-${randomBytes(4).toString('hex')}`;
		this.storageDir = join(process.cwd(), 'storage', 'jobs', this.datasetName);
	}
	async crawl() {
		const configWithDataset: ConfigWithDataset = {
			...this.config,
			datasetName: this.datasetName,
			storageDir: this.storageDir,
		};
		await crawl(configWithDataset);
	}
	async write(): Promise<PathLike | null> {
		const configWithDataset: ConfigWithDataset = {
			...this.config,
			datasetName: this.datasetName,
			storageDir: this.storageDir,
		};
		return write(configWithDataset);
	}
	async cleanup(): Promise<void> {
		await cleanupJobStorage(this.datasetName, this.storageDir);
	}
}
export default ContextCrawlerCore;
</file>

</files>

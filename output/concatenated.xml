<directory_structure>
configurations/
  jobs/
    ai-sdk.ts
    better-auth.ts
    next-js-16.ts
    polar-sh.ts
    prisma.ts
    react-19.ts
    trpc.ts
    zod.ts
  global.config.ts
  index.ts
  types.ts
src/
  cli.ts
  config.ts
  core.ts
  job-store.ts
  logger.ts
  queue.ts
  schema.ts
  server.ts
  worker.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="configurations/jobs/ai-sdk.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'ai-sdk',
	entry: 'https://ai-sdk.dev/docs/foundations/overview',
	match: [
		'https://ai-sdk.dev/docs/foundations/**',
		'https://ai-sdk.dev/docs/getting-started/**',
		'https://ai-sdk.dev/docs/agents/**',
		'https://ai-sdk.dev/docs/ai-sdk-core/**',
		'https://ai-sdk.dev/docs/ai-sdk-ui/**',
		'https://ai-sdk.dev/docs/advanced/**',
	],
	exclude: ['**/support', '**/changelog', '**/llms-full.txt*'],
	selector: '#content-area',
});
</file>

<file path="configurations/global.config.ts">
import type { GlobalConfig } from '../src/schema.js';
export const globalConfig: GlobalConfig = {
	maxPagesToCrawl: 'unlimited',
	maxTokens: 'unlimited',
};
</file>

<file path="configurations/index.ts">
import type { JobTasks } from './types.js';
import aiSdk from './jobs/ai-sdk.js';
import betterAuth from './jobs/better-auth.js';
import nextJs16 from './jobs/next-js-16.js';
import polarSh from './jobs/polar-sh.js';
import prisma from './jobs/prisma.js';
import react19 from './jobs/react-19.js';
import trpc from './jobs/trpc.js';
import zod from './jobs/zod.js';
export const jobs = {
	'ai-sdk': aiSdk,
	'better-auth': betterAuth,
	'next-js-16': nextJs16,
	'polar-sh': polarSh,
	prisma,
	'react-19': react19,
	trpc,
	zod,
} satisfies Record<string, JobTasks>;
export type JobRegistry = typeof jobs;
</file>

<file path="configurations/types.ts">
import type { ConfigInput, NamedConfig } from '../src/schema.js';
import { configSchema } from '../src/schema.js';
export type JobTasks = readonly NamedConfig[];
export interface JobConfig extends ConfigInput {
	name: ConfigInput['name'];
	entry: ConfigInput['entry'];
	match: ConfigInput['match'];
	exclude?: ConfigInput['exclude'];
	selector: ConfigInput['selector'];
	autoDiscoverNav?: ConfigInput['autoDiscoverNav'];
	discoverySelector?: ConfigInput['discoverySelector'];
	outputFileName?: ConfigInput['outputFileName'];
	cookie?: ConfigInput['cookie'];
	onVisitPage?: ConfigInput['onVisitPage'];
	waitForSelectorTimeout?: ConfigInput['waitForSelectorTimeout'];
	resourceExclusions?: ConfigInput['resourceExclusions'];
	maxFileSize?: ConfigInput['maxFileSize'];
	storageDir?: ConfigInput['storageDir'];
	datasetName?: ConfigInput['datasetName'];
}
export function defineJob(task: JobConfig): readonly [NamedConfig];
export function defineJob(tasks: readonly JobConfig[]): readonly NamedConfig[];
export function defineJob(
	job: ConfigInput | readonly ConfigInput[]
): readonly NamedConfig[] {
	const normalized = Array.isArray(job) ? job : [job];
	const parsed = normalized.map((task) => configSchema.parse(task));
	return Object.freeze(parsed) as readonly NamedConfig[];
}
</file>

<file path="configurations/jobs/better-auth.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'better-auth-docs',
	entry: 'https://www.better-auth.com/docs/introduction',
	match: 'https://www.better-auth.com/docs/**',
	selector: 'article',
});
</file>

<file path="configurations/jobs/next-js-16.ts">
import { defineJob } from '../types.js';
export default defineJob([
	{
		name: 'api-reference',
		entry: 'https://nextjs.org/docs/app/api-reference',
		match: ['https://nextjs.org/docs/app/api-reference/**'],
		selector: 'article',
	},
	{
		name: 'metadata',
		entry: 'https://nextjs.org/docs/app/getting-started/metadata-and-og-images',
		match: 'https://nextjs.org/docs/app/getting-started/metadata-and-og-images',
		selector: 'article',
	},
	{
		name: 'architecture-accessibility',
		entry: 'https://nextjs.org/docs/architecture/accessibility',
		match: 'https://nextjs.org/docs/architecture/accessibility',
		selector: 'article',
	},
	{
		name: 'proxy',
		entry: 'https://nextjs.org/docs/app/getting-started/proxy',
		match: 'https://nextjs.org/docs/app/getting-started/proxy',
		selector: 'article',
	},
]);
</file>

<file path="configurations/jobs/polar-sh.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'polar-sh',
	entry: 'https://polar.sh/docs',
	match: [
		'https://polar.sh/docs/features/**',
		'https://polar.sh/docs/integrate/authentication/**',
		'https://polar.sh/docs/api-reference/**',
		'https://polar.sh/docs/guides/**',
	],
	exclude: ['**/support', '**/changelog', '**/llms-full.txt*'],
	selector: '#content-area',
});
</file>

<file path="configurations/jobs/prisma.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'prisma-reference',
	entry: 'https://www.prisma.io/docs/orm/reference',
	match: 'https://www.prisma.io/docs/orm/reference/**',
	selector: 'article',
});
</file>

<file path="configurations/jobs/react-19.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'reference',
	entry: 'https://react.dev/reference/react',
	match: [
		'https://react.dev/reference/react/**',
		'https://react.dev/reference/react-dom/**',
	],
	selector: 'article',
});
</file>

<file path="configurations/jobs/trpc.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'trpc-docs',
	entry: 'https://trpc.io/docs/server/introduction',
	match: [
		'https://trpc.io/docs/server/**',
		'https://trpc.io/docs/client/**',
		'https://trpc.io/docs/typedoc/**',
	],
	selector: 'article',
});
</file>

<file path="configurations/jobs/zod.ts">
import { defineJob } from '../types.js';
export default defineJob({
	name: 'zod-docs',
	entry: 'https://zod.dev',
	match: [
		'https://zod.dev/basics',
		'https://zod.dev/api',
		'https://zod.dev/error-customization',
		'https://zod.dev/error-formatting',
		'https://zod.dev/metadata',
		'https://zod.dev/json-schema',
		'https://zod.dev/codecs',
		'https://zod.dev/packages/zod',
		'https://zod.dev/packages/mini',
		'https://zod.dev/packages/core',
	],
	selector: 'article',
});
</file>

<file path="src/job-store.ts">
import Database from "better-sqlite3";
import { PathLike } from "fs";
import { mkdirSync } from "fs";
import { dirname } from "path";
import { Config } from "./schema.js";
export type JobStatus = "pending" | "running" | "completed" | "failed";
export interface Job {
  id: string;
  status: JobStatus;
  config: Config;
  outputFile?: PathLike | null;
  error?: string;
  createdAt: Date;
  completedAt?: Date;
}
export interface JobRecord {
  id: string;
  status: JobStatus;
  config: string;
  outputFile: string | null;
  error: string | null;
  createdAt: string;
  completedAt: string | null;
}
class JobStore {
  private db: Database.Database;
  constructor(dbPath: string = "./data/jobs.db") {
    const dir = dirname(dbPath);
    mkdirSync(dir, { recursive: true });
    this.db = new Database(dbPath);
    this.db.pragma("journal_mode = WAL");
    this.initialize();
  }
  private initialize(): void {
    this.db.exec(`
      CREATE TABLE IF NOT EXISTS jobs (
        id TEXT PRIMARY KEY,
        status TEXT NOT NULL,
        config TEXT NOT NULL,
        outputFile TEXT,
        error TEXT,
        createdAt TEXT NOT NULL,
        completedAt TEXT
      )
    `);
  }
  createJob(jobId: string, config: Config): void {
    const stmt = this.db.prepare(`
      INSERT INTO jobs (id, status, config, createdAt)
      VALUES (?, ?, ?, ?)
    `);
    stmt.run(
      jobId,
      "pending",
      JSON.stringify(config),
      new Date().toISOString(),
    );
  }
  getJobById(jobId: string): Job | undefined {
    const stmt = this.db.prepare("SELECT * FROM jobs WHERE id = ?");
    const record = stmt.get(jobId) as JobRecord | undefined;
    if (!record) {
      return undefined;
    }
    return this.recordToJob(record);
  }
  updateJobStatus(
    jobId: string,
    status: JobStatus,
    data?: {
      outputFile?: PathLike;
      error?: string;
      completedAt?: Date;
    },
  ): void {
    const job = this.getJobById(jobId);
    if (!job) {
      throw new Error(`Job with id ${jobId} not found`);
    }
    const updates: string[] = ["status = ?"];
    const params: (string | null)[] = [status];
    if (data?.outputFile !== undefined) {
      updates.push("outputFile = ?");
      params.push(String(data.outputFile));
    }
    if (data?.error !== undefined) {
      updates.push("error = ?");
      params.push(data.error);
    }
    if (data?.completedAt !== undefined) {
      updates.push("completedAt = ?");
      params.push(data.completedAt.toISOString());
    }
    params.push(jobId);
    const stmt = this.db.prepare(`
      UPDATE jobs
      SET ${updates.join(", ")}
      WHERE id = ?
    `);
    stmt.run(...params);
  }
  getAllJobs(): Job[] {
    const stmt = this.db.prepare("SELECT * FROM jobs ORDER BY createdAt DESC");
    const records = stmt.all() as JobRecord[];
    return records.map((record) => this.recordToJob(record));
  }
  deleteJob(jobId: string): void {
    const stmt = this.db.prepare("DELETE FROM jobs WHERE id = ?");
    stmt.run(jobId);
  }
  close(): void {
    this.db.close();
  }
  private recordToJob(record: JobRecord): Job {
    return {
      id: record.id,
      status: record.status,
      config: JSON.parse(record.config) as Config,
      outputFile: record.outputFile,
      error: record.error ?? undefined,
      createdAt: new Date(record.createdAt),
      completedAt: record.completedAt
        ? new Date(record.completedAt)
        : undefined,
    };
  }
}
export const jobStore = new JobStore();
</file>

<file path="src/logger.ts">
import pino from "pino";
const isDevelopment = process.env.NODE_ENV === "development";
export const logger = pino({
  level: process.env.LOG_LEVEL || "info",
  transport: isDevelopment
    ? {
        target: "pino-pretty",
        options: {
          colorize: true,
          translateTime: "HH:MM:ss Z",
          ignore: "pid,hostname",
        },
      }
    : undefined,
});
export default logger;
</file>

<file path="src/queue.ts">
import Database from "better-sqlite3";
import { mkdir } from "fs/promises";
import { dirname } from "path";
import { Config } from "./schema.js";
export interface CrawlJobData {
  config: Config;
}
export interface QueueJobOptions {
  jobId: string;
  priority?: number;
  maxAttempts?: number;
  backoffDelay?: number;
}
interface QueueJobRecord {
  id: string;
  jobId: string;
  status: "pending" | "claimed" | "completed" | "failed";
  data: string;
  priority: number;
  attempts: number;
  maxAttempts: number;
  nextRetryAt: string | null;
  claimedAt: string | null;
  completedAt: string | null;
  error: string | null;
  createdAt: string;
}
export interface QueueJob {
  id: string;
  jobId: string;
  data: CrawlJobData;
  attempts: number;
  maxAttempts: number;
}
class SQLiteQueue {
  private db: Database.Database;
  private dbPath: string;
  constructor(dbPath: string = "./data/queue.db") {
    this.dbPath = dbPath;
    this.db = new Database(dbPath);
    this.db.pragma("journal_mode = WAL");
  }
  async initialize(): Promise<void> {
    const dir = dirname(this.dbPath);
    await mkdir(dir, { recursive: true });
    this.db.exec(`
      CREATE TABLE IF NOT EXISTS queue (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        jobId TEXT NOT NULL UNIQUE,
        status TEXT NOT NULL DEFAULT 'pending',
        data TEXT NOT NULL,
        priority INTEGER NOT NULL DEFAULT 0,
        attempts INTEGER NOT NULL DEFAULT 0,
        maxAttempts INTEGER NOT NULL DEFAULT 3,
        nextRetryAt TEXT,
        claimedAt TEXT,
        completedAt TEXT,
        error TEXT,
        createdAt TEXT NOT NULL
      );
      CREATE INDEX IF NOT EXISTS idx_queue_status ON queue(status);
      CREATE INDEX IF NOT EXISTS idx_queue_priority ON queue(priority DESC);
      CREATE INDEX IF NOT EXISTS idx_queue_nextRetryAt ON queue(nextRetryAt);
    `);
  }
  async add(
    _queueName: string,
    data: CrawlJobData,
    options: QueueJobOptions,
  ): Promise<void> {
    const {
      jobId,
      priority = 0,
      maxAttempts = 3,
      backoffDelay = 1000,
    } = options;
    const stmt = this.db.prepare(`
      INSERT INTO queue (jobId, status, data, priority, maxAttempts, createdAt)
      VALUES (?, 'pending', ?, ?, ?, ?)
    `);
    stmt.run(
      jobId,
      JSON.stringify(data),
      priority,
      maxAttempts,
      new Date().toISOString(),
    );
  }
  claimNextJob(): QueueJob | null {
    const now = new Date().toISOString();
    const claimJob = this.db.transaction(() => {
      const findStmt = this.db.prepare(`
        SELECT * FROM queue
        WHERE status = 'pending'
          AND (nextRetryAt IS NULL OR nextRetryAt <= ?)
        ORDER BY priority DESC, createdAt ASC
        LIMIT 1
      `);
      const record = findStmt.get(now) as QueueJobRecord | undefined;
      if (!record) {
        return null;
      }
      const updateStmt = this.db.prepare(`
        UPDATE queue
        SET status = 'claimed',
            claimedAt = ?,
            attempts = attempts + 1
        WHERE id = ?
      `);
      updateStmt.run(now, record.id);
      return {
        id: record.id.toString(),
        jobId: record.jobId,
        data: JSON.parse(record.data) as CrawlJobData,
        attempts: record.attempts + 1,
        maxAttempts: record.maxAttempts,
      };
    });
    return claimJob();
  }
  markCompleted(queueJobId: string): void {
    const stmt = this.db.prepare(`
      UPDATE queue
      SET status = 'completed',
          completedAt = ?
      WHERE id = ?
    `);
    stmt.run(new Date().toISOString(), queueJobId);
  }
  markFailed(
    queueJobId: string,
    error: string,
    shouldRetry: boolean,
    backoffDelay: number = 1000,
  ): void {
    const record = this.db
      .prepare("SELECT * FROM queue WHERE id = ?")
      .get(queueJobId) as QueueJobRecord | undefined;
    if (!record) {
      return;
    }
    if (shouldRetry && record.attempts < record.maxAttempts) {
      const delay = backoffDelay * Math.pow(2, record.attempts - 1);
      const nextRetryAt = new Date(Date.now() + delay).toISOString();
      const stmt = this.db.prepare(`
        UPDATE queue
        SET status = 'pending',
            nextRetryAt = ?,
            error = ?
        WHERE id = ?
      `);
      stmt.run(nextRetryAt, error, queueJobId);
    } else {
      const stmt = this.db.prepare(`
        UPDATE queue
        SET status = 'failed',
            completedAt = ?,
            error = ?
        WHERE id = ?
      `);
      stmt.run(new Date().toISOString(), error, queueJobId);
    }
  }
  resetStuckJobs(timeoutMs: number = 30 * 60 * 1000): number {
    const cutoffTime = new Date(Date.now() - timeoutMs).toISOString();
    const stmt = this.db.prepare(`
      UPDATE queue
      SET status = 'pending',
          claimedAt = NULL
      WHERE status = 'claimed'
        AND claimedAt < ?
    `);
    const result = stmt.run(cutoffTime);
    return result.changes;
  }
  cleanupOldJobs(ageMs: number = 7 * 24 * 60 * 60 * 1000): number {
    const cutoffTime = new Date(Date.now() - ageMs).toISOString();
    const stmt = this.db.prepare(`
      DELETE FROM queue
      WHERE status IN ('completed', 'failed')
        AND completedAt < ?
    `);
    const result = stmt.run(cutoffTime);
    return result.changes;
  }
  clearCompletedJobs(): number {
    const stmt = this.db.prepare(`
      DELETE FROM queue
      WHERE status IN ('completed', 'failed')
    `);
    const result = stmt.run();
    return result.changes;
  }
  getStats(): {
    pending: number;
    claimed: number;
    completed: number;
    failed: number;
    total: number;
  } {
    const stmt = this.db.prepare(`
      SELECT
        status,
        COUNT(*) as count
      FROM queue
      GROUP BY status
    `);
    const results = stmt.all() as Array<{ status: string; count: number }>;
    const stats = {
      pending: 0,
      claimed: 0,
      completed: 0,
      failed: 0,
      total: 0,
    };
    results.forEach((row) => {
      const status = row.status as keyof typeof stats;
      if (status in stats) {
        stats[status] = row.count;
      }
      stats.total += row.count;
    });
    return stats;
  }
  close(): void {
    this.db.close();
  }
}
export const crawlQueue = new SQLiteQueue();
await crawlQueue.initialize();
</file>

<file path="src/worker.ts">
#!/usr/bin/env node
import { configDotenv } from "dotenv";
import ContextCrawlerCore from "./core.js";
import { jobStore } from "./job-store.js";
import logger from "./logger.js";
import { crawlQueue, QueueJob } from "./queue.js";
configDotenv();
const WORKER_CONCURRENCY = Number(process.env.WORKER_CONCURRENCY) || 2;
const POLL_INTERVAL_MS = Number(process.env.POLL_INTERVAL_MS) || 1000;
const MAX_POLL_INTERVAL_MS = Number(process.env.MAX_POLL_INTERVAL_MS) || 10000;
const JOB_TIMEOUT_MS = Number(process.env.JOB_TIMEOUT_MS) || 30 * 60 * 1000;
const BACKOFF_DELAY_MS = Number(process.env.BACKOFF_DELAY_MS) || 5000;
const activeJobs = new Set<Promise<void>>();
let isShuttingDown = false;
let pollIntervalId: NodeJS.Timeout | null = null;
let currentPollInterval = POLL_INTERVAL_MS;
async function processCrawlJob(job: QueueJob): Promise<void> {
  const { config } = job.data;
  const { jobId } = job;
  logger.info(
    {
      jobId,
      queueJobId: job.id,
      attempt: job.attempts,
      maxAttempts: job.maxAttempts,
    },
    "Processing crawl job",
  );
  let crawler: ContextCrawlerCore | null = null;
  try {
    jobStore.updateJobStatus(jobId, "running");
    crawler = new ContextCrawlerCore(config);
    await crawler.crawl();
    const outputFileName = await crawler.write();
    await crawler.cleanup();
    crawlQueue.markCompleted(job.id);
    jobStore.updateJobStatus(jobId, "completed", {
      outputFile: outputFileName || undefined,
      completedAt: new Date(),
    });
    const clearedCount = crawlQueue.clearCompletedJobs();
    if (clearedCount > 0) {
      logger.debug(
        { clearedCount },
        "Auto-cleared completed/failed jobs from queue",
      );
    }
    logger.info(
      { jobId, queueJobId: job.id, outputFile: outputFileName },
      "Crawl job completed successfully",
    );
  } catch (error) {
    const errorMessage =
      error instanceof Error ? error.message : "Unknown error occurred";
    logger.error(
      { jobId, queueJobId: job.id, error: errorMessage, attempt: job.attempts },
      "Crawl job failed",
    );
    if (crawler) {
      try {
        await crawler.cleanup();
      } catch (cleanupError) {
        logger.warn(
          {
            jobId,
            error:
              cleanupError instanceof Error
                ? cleanupError.message
                : cleanupError,
          },
          "Failed to cleanup storage after error",
        );
      }
    }
    const shouldRetry = job.attempts < job.maxAttempts;
    const backoffWithJitter =
      BACKOFF_DELAY_MS *
      Math.pow(2, job.attempts - 1) *
      (0.5 + Math.random() * 0.5);
    crawlQueue.markFailed(job.id, errorMessage, shouldRetry, backoffWithJitter);
    if (!shouldRetry) {
      jobStore.updateJobStatus(jobId, "failed", {
        error: errorMessage,
        completedAt: new Date(),
      });
      const clearedCount = crawlQueue.clearCompletedJobs();
      if (clearedCount > 0) {
        logger.debug(
          { clearedCount },
          "Auto-cleared completed/failed jobs from queue",
        );
      }
    }
  }
}
async function pollQueue(): Promise<void> {
  if (isShuttingDown) {
    return;
  }
  if (activeJobs.size >= WORKER_CONCURRENCY) {
    return;
  }
  let jobClaimed = false;
  const jobsToStart: QueueJob[] = [];
  while (activeJobs.size + jobsToStart.length < WORKER_CONCURRENCY) {
    const job = crawlQueue.claimNextJob();
    if (!job) {
      break;
    }
    jobClaimed = true;
    jobsToStart.push(job);
  }
  for (const job of jobsToStart) {
    const jobPromise = processCrawlJob(job)
      .catch((error) => {
        logger.error(
          { error: error instanceof Error ? error.message : error },
          "Unhandled error in job processing",
        );
      })
      .finally(() => {
        activeJobs.delete(jobPromise);
      });
    activeJobs.add(jobPromise);
  }
  if (jobClaimed) {
    currentPollInterval = POLL_INTERVAL_MS;
  } else {
    currentPollInterval = Math.min(
      currentPollInterval * 1.5,
      MAX_POLL_INTERVAL_MS,
    );
  }
}
function startWorker(): void {
  logger.info(
    {
      concurrency: WORKER_CONCURRENCY,
      pollInterval: POLL_INTERVAL_MS,
      jobTimeout: JOB_TIMEOUT_MS,
    },
    "Worker starting...",
  );
  const resetCount = crawlQueue.resetStuckJobs(JOB_TIMEOUT_MS);
  if (resetCount > 0) {
    logger.info({ count: resetCount }, "Reset stuck jobs from previous run");
  }
  const cleanedCount = crawlQueue.cleanupOldJobs(7 * 24 * 60 * 60 * 1000);
  if (cleanedCount > 0) {
    logger.info({ count: cleanedCount }, "Cleaned up old jobs");
  }
  const stats = crawlQueue.getStats();
  logger.info({ stats }, "Queue statistics");
  const poll = () => {
    pollQueue()
      .catch((error) => {
        logger.error(
          { error: error instanceof Error ? error.message : error },
          "Error during queue polling",
        );
      })
      .finally(() => {
        if (!isShuttingDown) {
          pollIntervalId = setTimeout(poll, currentPollInterval);
        }
      });
  };
  poll();
  logger.info("Worker is ready and waiting for jobs");
}
async function shutdown(): Promise<void> {
  if (isShuttingDown) {
    return;
  }
  isShuttingDown = true;
  logger.info("Worker shutting down gracefully...");
  if (pollIntervalId) {
    clearTimeout(pollIntervalId);
    pollIntervalId = null;
  }
  if (activeJobs.size > 0) {
    logger.info(
      { activeJobCount: activeJobs.size },
      "Waiting for active jobs to complete...",
    );
    await Promise.all(Array.from(activeJobs));
  }
  crawlQueue.close();
  jobStore.close();
  logger.info("Worker shut down complete");
  process.exit(0);
}
process.on("SIGTERM", () => {
  logger.info("SIGTERM received");
  shutdown();
});
process.on("SIGINT", () => {
  logger.info("SIGINT received");
  shutdown();
});
process.on("uncaughtException", (error) => {
  logger.error(
    { error: error.message, stack: error.stack },
    "Uncaught exception",
  );
  shutdown();
});
process.on("unhandledRejection", (reason) => {
  logger.error(
    { reason: reason instanceof Error ? reason.message : reason },
    "Unhandled rejection",
  );
  shutdown();
});
startWorker();
</file>

<file path="src/cli.ts">
#!/usr/bin/env node
import { program } from 'commander';
import { randomUUID } from 'crypto';
import inquirer from 'inquirer';
import { createRequire } from 'node:module';
import {
	getAllJobNames,
	getAllTaskNames,
	getTaskByName,
	getTasksByJobName,
	type JobName,
} from './config.js';
import ContextCrawlerCore from './core.js';
import { jobStore } from './job-store.js';
import logger from './logger.js';
import { crawlQueue } from './queue.js';
import { Config, configSchema, NamedConfig } from './schema.js';
const require = createRequire(import.meta.url);
const { version, description } = require('../../package.json');
const messages = {
	urls: 'Enter starting URLs (comma-separated for multiple):',
	match: 'What is the URL pattern you want to match?',
	selector: 'What is the CSS selector you want to match?',
	outputFileName: 'What is the name of the output file?',
	config: 'Name of the crawl configuration to use',
};
async function handler(cliOptions: Partial<Config> & { config?: string }) {
	try {
		let config: Partial<Config> = {};
		if (cliOptions.config) {
			const namedConfig = getTaskByName(cliOptions.config);
			if (!namedConfig) {
				logger.error(
					{ config: cliOptions.config },
					`Task '${cliOptions.config}' not found`
				);
				logger.info(`Available tasks: ${getAllTaskNames().join(', ')}`);
				process.exit(1);
			}
			config = { ...namedConfig };
		} else {
			const availableTasks = getAllTaskNames();
			if (availableTasks.length > 0) {
				const taskAnswer = await inquirer.prompt({
					type: 'list',
					name: 'taskName',
					message: 'Select a task:',
					choices: availableTasks,
				});
				const namedConfig = getTaskByName(taskAnswer.taskName);
				if (namedConfig) {
					config = { ...namedConfig };
				}
			}
		}
		Object.keys(cliOptions).forEach((key) => {
			if (
				cliOptions[key as keyof typeof cliOptions] !== undefined &&
				key !== 'config' &&
				key in configSchema.shape
			) {
				let value = cliOptions[key as keyof typeof cliOptions];
				if (key === 'urls' && typeof value === 'string') {
					value = value
						.split(',')
						.map((url: string) => url.trim())
						.filter((url: string) => url.length > 0) as any;
				}
				config[key as keyof Config] = value as any;
			}
		});
		if (!config.entry || !config.match || !config.selector) {
			const answers: Partial<Config> = {};
			if (!config.entry) {
				const urlsAnswer = await inquirer.prompt({
					type: 'input',
					name: 'urls',
					message: messages.urls,
				});
				answers.entry = urlsAnswer.urls
					.split(',')
					.map((url: string) => url.trim())
					.filter((url: string) => url.length > 0);
			}
			if (!config.match) {
				const matchAnswer = await inquirer.prompt({
					type: 'input',
					name: 'match',
					message: messages.match,
				});
				answers.match = matchAnswer.match;
			}
			if (!config.selector) {
				const selectorAnswer = await inquirer.prompt({
					type: 'input',
					name: 'selector',
					message: messages.selector,
				});
				answers.selector = selectorAnswer.selector;
			}
			config = {
				...config,
				...answers,
			};
		}
		const finalConfig = config as Config;
		const crawler = new ContextCrawlerCore(finalConfig);
		await crawler.crawl();
		await crawler.write();
	} catch (error) {
		logger.error({ error }, 'Error during crawl');
		process.exit(1);
	}
}
program.version(version).description(description);
program
	.command('single')
	.description('Crawl a single configuration')
	.option('-c, --config <string>', messages.config)
	.option('-u, --urls <string>', messages.urls)
	.option('-m, --match <string>', messages.match)
	.option('-s, --selector <string>', messages.selector)
	.option('-o, --outputFileName <string>', messages.outputFileName)
	.option('--no-auto-discover-nav', 'Disable automatic navigation discovery')
	.action(handler);
program
	.command('batch [names...]')
	.description('Run one or more predefined batches of crawl configurations')
	.option('-q, --queue', 'Queue jobs for worker instead of running directly')
	.action(async (names: string[], options: { queue?: boolean }) => {
		let selectedBatches: string[];
		if (!names || names.length === 0) {
			const availableJobs = getAllJobNames();
			if (availableJobs.length === 0) {
				logger.error('No jobs found in configurations');
				process.exit(1);
			}
			const jobChoices = availableJobs.map((name) => {
				const count = getTasksByJobName(name as JobName).length;
				return {
					name: `${name} (${count} ${count === 1 ? 'task' : 'tasks'})`,
					value: name,
				};
			});
			const jobAnswer = await inquirer.prompt({
				type: 'checkbox',
				name: 'jobs',
				message: 'Select jobs to crawl:',
				choices: jobChoices,
				validate: (answer) => {
					if ((answer as unknown as string[]).length === 0) {
						return 'You must select at least one job';
					}
					return true;
				},
			});
			selectedBatches = jobAnswer.jobs;
		} else {
			selectedBatches = names;
		}
		for (const name of selectedBatches) {
			const jobTasks = getTasksByJobName(name as JobName);
			if (!jobTasks || jobTasks.length === 0) {
				logger.error({ job: name }, `Job '${name}' not found or is empty`);
				const availableJobs = getAllJobNames();
				logger.info(`Available jobs: ${availableJobs.join(', ')}`);
				process.exit(1);
			}
		}
		let useQueue = options.queue ?? false;
		if (options.queue === undefined) {
			const modeAnswer = await inquirer.prompt({
				type: 'list',
				name: 'mode',
				message: 'How do you want to run the crawl?',
				choices: [
					{
						name: 'Run directly (wait for completion)',
						value: 'direct',
					},
					{
						name: 'Queue for worker (async)',
						value: 'queue',
					},
				],
			});
			useQueue = modeAnswer.mode === 'queue';
		}
		const allConfigs: Array<{ jobName: string; config: NamedConfig }> = [];
		for (const jobName of selectedBatches) {
			const jobTasks = getTasksByJobName(jobName as JobName);
			for (const config of jobTasks) {
				allConfigs.push({ jobName, config });
			}
		}
		logger.info(
			{
				jobs: selectedBatches.join(', '),
				totalTasks: allConfigs.length,
				mode: useQueue ? 'queue' : 'direct',
			},
			`Starting crawl for ${selectedBatches.length} ${
				selectedBatches.length === 1 ? 'job' : 'jobs'
			} (${allConfigs.length} total ${
				allConfigs.length === 1 ? 'task' : 'tasks'
			})`
		);
		if (useQueue) {
			const jobIds: string[] = [];
			for (const { config } of allConfigs) {
				const jobId = randomUUID();
				jobStore.createJob(jobId, config);
				await crawlQueue.add('crawl', { config }, { jobId });
				jobIds.push(jobId);
				logger.info(
					{ jobId, config: config.name },
					`Queued: ${config.name} (job ID: ${jobId})`
				);
			}
			logger.info(
				{ totalJobs: jobIds.length },
				`Successfully queued ${jobIds.length} ${
					jobIds.length === 1 ? 'job' : 'jobs'
				}`
			);
			logger.info('Worker will process these jobs asynchronously');
			logger.info('Check job status via API: GET /crawl/status/{jobId}');
		} else {
			const jobResults: Record<
				string,
				{
					successful: Array<{ config: NamedConfig; outputFile: string | null }>;
					failed: Array<{ config: NamedConfig; error: string }>;
					crawlers: ContextCrawlerCore[];
				}
			> = {};
			for (const jobName of selectedBatches) {
				jobResults[jobName] = {
					successful: [],
					failed: [],
					crawlers: [],
				};
			}
			for (let i = 0; i < allConfigs.length; i++) {
				const { jobName, config } = allConfigs[i]!;
				logger.info(
					{
						progress: `${i + 1}/${allConfigs.length}`,
						job: jobName,
						task: config.name,
					},
					`Crawling: ${config.name} (from ${jobName})`
				);
				try {
					const { join } = await import('path');
					const tempOutputPath = join(
						process.cwd(),
						'storage',
						'temp',
						`${config.name}-${Date.now()}.json`
					);
					const tempConfig = { ...config, outputFileName: tempOutputPath };
					const crawler = new ContextCrawlerCore(tempConfig);
					jobResults[jobName]!.crawlers.push(crawler);
					await crawler.crawl();
					const outputFile = await crawler.write();
					jobResults[jobName]!.successful.push({
						config,
						outputFile: outputFile ? outputFile.toString() : null,
					});
					logger.info(
						{
							progress: `${i + 1}/${allConfigs.length}`,
							job: jobName,
							task: config.name,
						},
						`Completed: ${config.name}`
					);
				} catch (error) {
					const errorMessage =
						error instanceof Error ? error.message : 'Unknown error';
					jobResults[jobName]!.failed.push({
						config,
						error: errorMessage,
					});
					logger.error(
						{
							progress: `${i + 1}/${allConfigs.length}`,
							job: jobName,
							task: config.name,
							error: errorMessage,
						},
						`Failed: ${config.name}`
					);
				}
			}
			for (const jobName of selectedBatches) {
				const results = jobResults[jobName]!;
				const successCount = results.successful.length;
				const failCount = results.failed.length;
				const totalCount = successCount + failCount;
				logger.info(
					{ job: jobName, successCount, failCount, totalCount },
					`Job '${jobName}' completed: ${successCount}/${totalCount} successful, ${failCount} failed`
				);
				if (successCount > 0) {
					try {
						const aggregatedData: unknown[] = [];
						const tempFilesRead: string[] = [];
						for (const { outputFile } of results.successful) {
							if (outputFile) {
								const { readFileSync, existsSync } = await import('fs');
								if (!existsSync(outputFile)) {
									logger.warn(
										{ job: jobName, file: outputFile },
										`Temp file not found: ${outputFile}`
									);
									continue;
								}
								const content = readFileSync(outputFile, 'utf-8');
								const data = JSON.parse(content);
								tempFilesRead.push(outputFile);
								if (Array.isArray(data)) {
									aggregatedData.push(...data);
								} else {
									aggregatedData.push(data);
								}
							}
						}
						logger.info(
							{
								job: jobName,
								tempFilesFound: tempFilesRead.length,
								tempFilesExpected: results.successful.filter(
									(r) => r.outputFile
								).length,
							},
							`Read ${tempFilesRead.length} temp files for job '${jobName}'`
						);
						if (aggregatedData.length > 0) {
							const aggregatedOutputPath = `output/jobs/${jobName}.json`;
							const { writeFileSync, mkdirSync } = await import('fs');
							const { dirname } = await import('path');
							mkdirSync(dirname(aggregatedOutputPath), { recursive: true });
							writeFileSync(
								aggregatedOutputPath,
								JSON.stringify(aggregatedData, null, 2)
							);
							logger.info(
								{
									job: jobName,
									itemCount: aggregatedData.length,
									outputFile: aggregatedOutputPath,
								},
								`Aggregated ${aggregatedData.length} items to ${aggregatedOutputPath}`
							);
						} else {
							logger.info(
								{ job: jobName },
								`Skipping output file creation for '${jobName}' - no items crawled`
							);
						}
					} catch (error) {
						logger.error(
							{
								job: jobName,
								error: error instanceof Error ? error.message : error,
							},
							`Failed to aggregate outputs for job '${jobName}'`
						);
					}
				} else {
					logger.info(
						{ job: jobName },
						`Skipping aggregation for '${jobName}' - no successful tasks`
					);
				}
				for (const crawler of results.crawlers) {
					try {
						await crawler.cleanup();
					} catch (error) {
						logger.warn(
							{
								error: error instanceof Error ? error.message : error,
							},
							'Failed to cleanup crawler storage'
						);
					}
				}
			}
			try {
				const { rm } = await import('fs/promises');
				const { join } = await import('path');
				const tempDir = join(process.cwd(), 'storage', 'temp');
				await rm(tempDir, { recursive: true, force: true });
				logger.debug({ tempDir }, 'Cleaned up temp output directory');
			} catch (error) {
				logger.warn(
					{
						error: error instanceof Error ? error.message : error,
					},
					'Failed to cleanup temp output directory'
				);
			}
			const totalSuccessful = Object.values(jobResults).reduce(
				(sum, r) => sum + r.successful.length,
				0
			);
			const totalFailed = Object.values(jobResults).reduce(
				(sum, r) => sum + r.failed.length,
				0
			);
			logger.info(
				{
					jobs: selectedBatches.join(', '),
					totalSuccessful,
					totalFailed,
					total: allConfigs.length,
				},
				`All jobs completed: ${totalSuccessful}/${allConfigs.length} successful, ${totalFailed} failed`
			);
		}
	});
program
	.command('list')
	.description('List all available jobs and tasks')
	.action(() => {
		const {
			getAllJobNames,
			getTasksByJobName,
			getAllTaskNames,
		} = require('./config.js');
		const jobNames = getAllJobNames();
		const taskNames = getAllTaskNames();
		console.log('\nAvailable Jobs:');
		if (jobNames.length === 0) {
			console.log('  (none found)');
		} else {
			jobNames.forEach((jobName: string) => {
				const tasks = getTasksByJobName(jobName);
				const taskCount = tasks.length;
				console.log(
					`  - ${jobName} (${taskCount} ${taskCount === 1 ? 'task' : 'tasks'})`
				);
			});
		}
		console.log('\nAll Available Tasks:');
		if (taskNames.length === 0) {
			console.log('  (none found)');
		} else {
			taskNames.forEach((name: string) => {
				console.log(`  - ${name}`);
			});
		}
		console.log();
	});
program.parse();
</file>

<file path="src/schema.ts">
import { configDotenv } from 'dotenv';
import type { Page } from 'playwright';
import { z } from 'zod';
configDotenv();
export interface CrawledData {
	title: string;
	url: string;
	html: string;
	[key: string]: unknown;
}
const Page: z.ZodType<Page> = z.custom<Page>((val) => {
	return (
		typeof val === 'object' &&
		val !== null &&
		'goto' in val &&
		'evaluate' in val &&
		'title' in val
	);
});
export const globalConfigSchema = z.object({
	maxPagesToCrawl: z.union([z.number(), z.literal('unlimited')]),
	maxTokens: z.union([z.number(), z.literal('unlimited')]),
});
export type GlobalConfig = z.infer<typeof globalConfigSchema>;
export const configSchema = z.object({
	name: z.string(),
	entry: z.string().url(),
	match: z.union([z.string(), z.array(z.string())]),
	exclude: z.union([z.string(), z.array(z.string())]).optional(),
	selector: z.string(),
	autoDiscoverNav: z.boolean().optional().default(true),
	discoverySelector: z
		.string()
		.optional()
		.default("nav, aside, [role='navigation']"),
	outputFileName: z.string().optional(),
	cookie: z
		.union([
			z.object({
				name: z.string(),
				value: z.string(),
			}),
			z.array(
				z.object({
					name: z.string(),
					value: z.string(),
				})
			),
		])
		.optional(),
	onVisitPage: z.any().optional(),
	waitForSelectorTimeout: z.number().optional(),
	resourceExclusions: z.array(z.string()).optional(),
	maxFileSize: z.number().optional(),
	storageDir: z.string().optional(),
	datasetName: z.string().optional(),
});
export type Config = z.infer<typeof configSchema>;
export type ConfigInput = z.input<typeof configSchema>;
export type NamedConfig = Config;
export function generateNameFromUrl(url: string): string {
	try {
		const urlObj = new URL(url);
		const domain = urlObj.hostname.replace(/^www\./, '').split('.')[0];
		// Extract only the first path segment (ignore query params and fragments)
		const pathSegments = urlObj.pathname
			.split('/')
			.filter((segment) => segment.length > 0);
		// Combine domain and first path segment only
		const parts =
			pathSegments.length > 0 ? [domain, pathSegments[0]] : [domain];
		return parts.join('-').toLowerCase();
	} catch (error) {
		// Fallback to a simple sanitized version of the URL
		return url
			.replace(/[^a-zA-Z0-9]/g, '-')
			.replace(/-+/g, '-')
			.replace(/^-|-$/g, '')
			.toLowerCase();
	}
}
export function generateOutputFileName(taskName: string): string {
	return `output/jobs/${taskName}.json`;
}
</file>

<file path="src/server.ts">
import cors from 'cors';
import { randomUUID } from 'crypto';
import { configDotenv } from 'dotenv';
import express, { Express, NextFunction, Request, Response } from 'express';
import { createReadStream } from 'fs';
import { readFile, stat } from 'fs/promises';
import swaggerUi from 'swagger-ui-express';
import {
	getAllJobNames,
	getAllTasks,
	getTaskByName,
	getTasksByJobName,
} from './config.js';
import { jobStore } from './job-store.js';
import logger from './logger.js';
import { crawlQueue } from './queue.js';
import { Config, configSchema } from './schema.js';
configDotenv();
const app: Express = express();
const port = Number(process.env.API_PORT) || 3000;
const hostname = process.env.API_HOST || 'localhost';
const swaggerDocument = JSON.parse(
	await readFile(new URL('../swagger-output.json', import.meta.url), 'utf-8')
) as Record<string, unknown>;
app.use(cors());
app.use(express.json());
app.use(
	'/api-docs',
	...(swaggerUi.serve as unknown as express.RequestHandler[]),
	swaggerUi.setup(swaggerDocument) as unknown as express.RequestHandler
);
function authenticateApiKey(req: Request, res: Response, next: NextFunction) {
	const apiKey = process.env.API_KEY;
	if (!apiKey) {
		return next();
	}
	const requestApiKey = req.headers['x-api-key'];
	if (!requestApiKey || requestApiKey !== apiKey) {
		logger.warn({ path: req.path }, 'Unauthorized API access attempt');
		return res.status(401).json({ message: 'Unauthorized' });
	}
	next();
}
app.post('/crawl', authenticateApiKey, async (req, res) => {
	const { name, config: customConfig } = req.body;
	let config: Config | undefined;
	if (name && typeof name === 'string') {
		config = getTaskByName(name);
		if (!config) {
			logger.warn({ name }, 'Task not found');
			return res
				.status(404)
				.json({ message: `Task with name '${name}' not found.` });
		}
	} else if (customConfig && typeof customConfig === 'object') {
		const validationResult = configSchema.safeParse(customConfig);
		if (!validationResult.success) {
			logger.warn(
				{ errors: validationResult.error.issues },
				'Invalid custom config'
			);
			return res.status(400).json({
				message: 'Invalid configuration',
				errors: validationResult.error.issues,
			});
		}
		config = validationResult.data;
	} else {
		return res.status(400).json({
			message: "Invalid request body. Either 'name' or 'config' is required.",
		});
	}
	try {
		const jobId = randomUUID();
		jobStore.createJob(jobId, config);
		await crawlQueue.add('crawl', { config }, { jobId });
		logger.info({ jobId }, 'Crawl job queued');
		return res.status(202).json({
			jobId,
			message: 'Crawl job started',
			statusUrl: `/crawl/status/${jobId}`,
			resultsUrl: `/crawl/results/${jobId}`,
		});
	} catch (error) {
		logger.error({ error }, 'Error starting job');
		return res.status(500).json({ message: 'Failed to start crawl job.' });
	}
});
app.post('/crawl/batch', authenticateApiKey, async (req, res) => {
	const { name } = req.body;
	if (!name || typeof name !== 'string') {
		return res.status(400).json({
			message: "Invalid request body. 'name' (job name) is required.",
		});
	}
	const tasks = getTasksByJobName(name);
	if (!tasks || tasks.length === 0) {
		logger.warn({ jobName: name }, 'Job not found or has no tasks');
		return res.status(404).json({
			message: `Job with name '${name}' not found or has no tasks.`,
			availableJobs: getAllJobNames(),
		});
	}
	try {
		const queuedTasks: Array<{
			configName: string;
			jobId: string;
			statusUrl: string;
			resultsUrl: string;
		}> = [];
		for (const task of tasks) {
			const jobId = randomUUID();
			jobStore.createJob(jobId, task);
			await crawlQueue.add('crawl', { config: task }, { jobId });
			queuedTasks.push({
				configName: task.name,
				jobId,
				statusUrl: `/crawl/status/${jobId}`,
				resultsUrl: `/crawl/results/${jobId}`,
			});
			logger.info(
				{ jobId, configName: task.name },
				'Task queued for batch job'
			);
		}
		logger.info(
			{ jobName: name, taskCount: queuedTasks.length },
			`Batch job '${name}' queued with ${queuedTasks.length} tasks`
		);
		return res.status(202).json({
			message: `Batch job '${name}' queued with ${queuedTasks.length} ${
				queuedTasks.length === 1 ? 'task' : 'tasks'
			}.`,
			jobName: name,
			taskCount: queuedTasks.length,
			tasks: queuedTasks,
		});
	} catch (error) {
		logger.error({ error, jobName: name }, 'Error queuing batch job');
		return res.status(500).json({ message: 'Failed to queue batch job.' });
	}
});
app.get('/crawl/status/:jobId', authenticateApiKey, (req, res) => {
	const jobId = req.params.jobId;
	if (!jobId) {
		return res.status(400).json({ message: 'Job ID is required' });
	}
	const job = jobStore.getJobById(jobId);
	if (!job) {
		logger.warn({ jobId }, 'Job not found');
		return res.status(404).json({ message: 'Job not found' });
	}
	return res.json({
		jobId: job.id,
		status: job.status,
		createdAt: job.createdAt,
		completedAt: job.completedAt,
		...(job.status === 'failed' && { error: job.error }),
	});
});
app.get('/crawl/results/:jobId', authenticateApiKey, async (req, res) => {
	const jobId = req.params.jobId;
	if (!jobId) {
		return res.status(400).json({ message: 'Job ID is required' });
	}
	const job = jobStore.getJobById(jobId);
	if (!job) {
		logger.warn({ jobId }, 'Job not found');
		return res.status(404).json({ message: 'Job not found' });
	}
	if (job.status === 'pending' || job.status === 'running') {
		return res.status(202).json({
			message: 'Job is still processing',
			status: job.status,
			statusUrl: `/crawl/status/${jobId}`,
		});
	}
	if (job.status === 'failed') {
		logger.warn({ jobId, error: job.error }, 'Failed job results requested');
		return res.status(500).json({
			message: 'Job failed',
			error: job.error,
		});
	}
	if (!job.outputFile) {
		logger.warn({ jobId }, 'No output file generated');
		return res.status(404).json({
			message: 'No output file generated',
		});
	}
	try {
		await stat(job.outputFile);
		res.contentType('application/json');
		const fileStream = createReadStream(job.outputFile, 'utf-8');
		return fileStream.pipe(res);
	} catch (error) {
		logger.error({ jobId, error }, 'Error reading output file');
		return res.status(500).json({ message: 'Error reading output file' });
	}
});
app.get('/configurations', authenticateApiKey, async (_req, res) => {
	try {
		const jobNames = getAllJobNames();
		const allTasks = getAllTasks();
		const jobs = jobNames.map((jobName) => {
			const tasks = getTasksByJobName(jobName);
			return {
				name: jobName,
				taskCount: tasks.length,
				tasks: tasks.map((t) => ({
					name: t.name,
					urls: t.entry,
				})),
			};
		});
		return res.json({
			jobs,
			tasks: allTasks.map((t) => ({
				name: t.name,
				urls: t.entry,
				outputFileName: t.outputFileName,
			})),
		});
	} catch (error) {
		logger.error({ error }, 'Error fetching configurations');
		return res.status(500).json({ message: 'Error fetching configurations' });
	}
});
app.listen(port, hostname, () => {
	logger.info(`API server listening at http://${hostname}:${port}`);
});
export default app;
</file>

<file path="src/config.ts">
import type { JobRegistry } from "../configurations/index.js";
import { jobs as configuredJobs } from "../configurations/index.js";
import { globalConfig } from "../configurations/global.config.js";
import { NamedConfig } from "./schema.js";
export { globalConfig };
const jobs = Object.freeze(configuredJobs) as Readonly<JobRegistry>;
const tasks = (() => {
  const entries = Object.entries(jobs) as Array<
    [keyof JobRegistry, readonly NamedConfig[]]
  >;
  const seenNames = new Map<string, string>();
  const aggregated: NamedConfig[] = [];
  for (const [jobName, jobTasks] of entries) {
    for (const task of jobTasks) {
      const existingJob = seenNames.get(task.name);
      if (existingJob) {
        throw new Error(
          `Duplicate task name '${task.name}' found in jobs '${existingJob}' and '${String(jobName)}'. All task names must be unique across all jobs.`,
        );
      }
      seenNames.set(task.name, String(jobName));
      aggregated.push(task);
    }
  }
  return Object.freeze(aggregated) as readonly NamedConfig[];
})();
export type JobName = keyof JobRegistry;
export type TaskName = (typeof tasks)[number]["name"];
const jobNames = Object.freeze(Object.keys(jobs)) as readonly JobName[];
const taskNames = Object.freeze(
  tasks.map((task) => task.name),
) as readonly TaskName[];
const jobNameSet = new Set<string>(jobNames);
const taskNameSet = new Set<string>(taskNames);
export function isJobName(value: string): value is JobName {
  return jobNameSet.has(value);
}
export function isTaskName(value: string): value is TaskName {
  return taskNameSet.has(value);
}
export function getTaskByName(name: TaskName): NamedConfig | undefined;
export function getTaskByName(name: string): NamedConfig | undefined;
export function getTaskByName(name: string): NamedConfig | undefined {
  return tasks.find((task) => task.name === name);
}
export function getTasksByJobName(
  jobName: JobName,
): readonly NamedConfig[];
export function getTasksByJobName(
  jobName: string,
): readonly NamedConfig[];
export function getTasksByJobName(
  jobName: string,
): readonly NamedConfig[] {
  if (!isJobName(jobName)) {
    return [];
  }
  return jobs[jobName];
}
export function getAllTaskNames(): string[] {
  return Array.from(taskNames);
}
export function getAllJobNames(): string[] {
  return Array.from(jobNames);
}
export function getAllTasks(): NamedConfig[] {
  return Array.from(tasks);
}
</file>

<file path="src/core.ts">
import {
	Configuration,
	Dataset,
	downloadListOfUrls,
	PlaywrightCrawler,
} from 'crawlee';
import { randomBytes } from 'crypto';
import { PathLike } from 'fs';
import { mkdir, rm, writeFile } from 'fs/promises';
import { isWithinTokenLimit } from 'gpt-tokenizer';
import { minimatch } from 'minimatch';
import { dirname, join } from 'path';
import { Page } from 'playwright';
import { globalConfig } from './config.js';
import logger from './logger.js';
import {
	Config,
	configSchema,
	CrawledData,
	generateOutputFileName,
} from './schema.js';
let pageCounter = 0;
type ConfigWithDataset = Config & { datasetName: string };
export function getPageHtml(page: Page, selector = 'body') {
	return page.evaluate((selector) => {
		if (selector.startsWith('/')) {
			const elements = document.evaluate(
				selector,
				document,
				null,
				XPathResult.ANY_TYPE,
				null
			);
			let result = elements.iterateNext();
			return result ? result.textContent || '' : '';
		} else {
			// Handle as a CSS selector
			const el = document.querySelector(selector) as HTMLElement | null;
			return el?.innerText || '';
		}
	}, selector);
}
/**
 * Expand exclude patterns to match both the path and all subpaths
 * Example: /support becomes ['/support', '/support/**']
 * Patterns with wildcards are kept as-is to avoid breaking glob semantics
 */
function expandExcludePatterns(patterns: string[]): string[] {
	const expanded = new Set<string>();
	for (const pattern of patterns) {
		expanded.add(pattern);
		if (!pattern.includes('*') && !pattern.endsWith('/')) {
			expanded.add(`${pattern}/**`);
		}
	}
	return Array.from(expanded);
}
function normalizeAndExpandExcludes(exclude?: string | string[]): string[] {
	const patterns = typeof exclude === 'string' ? [exclude] : (exclude ?? []);
	return expandExcludePatterns(patterns);
}
async function discoverNavigationUrls(
	page: Page,
	discoverySelector: string,
	matchPatterns: string[],
	excludePatterns: string[]
): Promise<string[]> {
	const navUrls = await page.evaluate((selector) => {
		const navElements = document.querySelectorAll(selector);
		const urls: string[] = [];
		navElements.forEach((nav) => {
			const links = nav.querySelectorAll('a[href]');
			links.forEach((link) => {
				const href = (link as HTMLAnchorElement).href;
				if (href && !href.startsWith('javascript:') && !href.startsWith('#')) {
					urls.push(href);
				}
			});
		});
		return Array.from(new Set(urls));
	}, discoverySelector);
	const expandedExcludes = expandExcludePatterns(excludePatterns);
	const filteredUrls = navUrls.filter((url) => {
		const matchesInclude = matchPatterns.some((pattern) =>
			minimatch(url, pattern)
		);
		if (!matchesInclude) {
			return false;
		}
		const matchesExclude = expandedExcludes.some((pattern) =>
			minimatch(url, pattern)
		);
		return !matchesExclude;
	});
	return filteredUrls;
}
export async function waitForXPath(page: Page, xpath: string, timeout: number) {
	await page.waitForFunction(
		(xpath) => {
			const elements = document.evaluate(
				xpath,
				document,
				null,
				XPathResult.ANY_TYPE,
				null
			);
			return elements.iterateNext() !== null;
		},
		xpath,
		{ timeout }
	);
}
export async function crawl(config: ConfigWithDataset) {
	configSchema.parse(config);
	if (process.env.NO_CRAWL !== 'true') {
		const storageDir =
			config.storageDir ||
			join(process.cwd(), 'storage', 'jobs', config.datasetName);
		await mkdir(storageDir, { recursive: true });
		const crawler = new PlaywrightCrawler(
			{
				async requestHandler({ request, page, enqueueLinks, log, pushData }) {
					const title = await page.title();
					pageCounter++;
					const maxPages = globalConfig.maxPagesToCrawl === 'unlimited'
						? 'unlimited'
						: globalConfig.maxPagesToCrawl;
					log.info(
						`Crawling: Page ${pageCounter} / ${maxPages} - URL: ${request.loadedUrl}...`
					);
					if (config.selector) {
						if (config.selector.startsWith('/')) {
							await waitForXPath(
								page,
								config.selector,
								config.waitForSelectorTimeout ?? 1000
							);
						} else {
							await page.waitForSelector(config.selector, {
								timeout: config.waitForSelectorTimeout ?? 1000,
							});
						}
					}
					const html = await getPageHtml(page, config.selector);
					await pushData({ title, url: request.loadedUrl, html });
					if (config.onVisitPage) {
						await config.onVisitPage({ page, pushData });
					}
					await enqueueLinks({
						globs:
							typeof config.match === 'string' ? [config.match] : config.match,
						exclude: normalizeAndExpandExcludes(config.exclude),
					});
				},
				...(globalConfig.maxPagesToCrawl !== 'unlimited' && {
					maxRequestsPerCrawl: globalConfig.maxPagesToCrawl,
				}),
				maxConcurrency: 2,
				maxRequestRetries: 2,
				preNavigationHooks: [
					async (crawlingContext, _gotoOptions) => {
						const { request, page, log } = crawlingContext;
						if (config.cookie) {
							const cookies = (
								Array.isArray(config.cookie) ? config.cookie : [config.cookie]
							).map((cookie) => {
								return {
									name: cookie.name,
									value: cookie.value,
									url: request.url,
								};
							});
							await page.context().addCookies(cookies);
						}
						const RESOURCE_EXCLUSTIONS = config.resourceExclusions ?? [];
						if (RESOURCE_EXCLUSTIONS.length === 0) {
							return;
						}
						await page.route(
							`**\/*.{${RESOURCE_EXCLUSTIONS.join()}}`,
							(route) => route.abort('aborted')
						);
						log.info(
							`Aborting requests for as this is a resource excluded route`
						);
					},
				],
			},
			new Configuration({
				purgeOnStart: false,
				defaultDatasetId: config.datasetName,
				persistStorage: true,
				storageClientOptions: {
					localDataDirectory: storageDir,
				},
			})
		);
		let seedUrls = [config.entry];
		const autoDiscoverNav = config.autoDiscoverNav ?? true;
		if (autoDiscoverNav) {
			try {
				logger.info('Starting navigation discovery phase...');
				const { chromium } = await import('playwright');
				const browser = await chromium.launch({ headless: true });
				const context = await browser.newContext();
				const page = await context.newPage();
				if (config.cookie) {
					const cookies = (
						Array.isArray(config.cookie) ? config.cookie : [config.cookie]
					).map((cookie) => ({
						name: cookie.name,
						value: cookie.value,
						url: config.entry,
					}));
					await context.addCookies(cookies);
				}
				await page.goto(config.entry, { waitUntil: 'domcontentloaded' });
				const matchPatterns = Array.isArray(config.match)
					? config.match
					: [config.match];
				const excludePatterns =
					typeof config.exclude === 'string'
						? [config.exclude]
						: (config.exclude ?? []);
				const discoverySelector =
					config.discoverySelector ?? "nav, aside, [role='navigation']";
				const discoveredUrls = await discoverNavigationUrls(
					page,
					discoverySelector,
					matchPatterns,
					excludePatterns
				);
				await browser.close();
				logger.info(
					{ count: discoveredUrls.length },
					`Discovered ${discoveredUrls.length} URLs from navigation`
				);
				seedUrls = Array.from(new Set([...seedUrls, ...discoveredUrls]));
				const expandedExcludes = expandExcludePatterns(excludePatterns);
				seedUrls = seedUrls.filter((url) => {
					return !expandedExcludes.some((pattern) => minimatch(url, pattern));
				});
				logger.info(
					{ total: seedUrls.length },
					`Total seed URLs: ${seedUrls.length}`
				);
			} catch (error) {
				logger.warn(
					{ error: error instanceof Error ? error.message : error },
					'Navigation discovery failed, continuing with provided URLs'
				);
			}
		}
		const sitemapUrls = seedUrls.filter((url) => /sitemap.*\.xml$/.test(url));
		if (sitemapUrls.length > 0) {
			for (const sitemapUrl of sitemapUrls) {
				const listOfUrls = await downloadListOfUrls({ url: sitemapUrl });
				await crawler.addRequests(listOfUrls);
			}
			const regularUrls = seedUrls.filter(
				(url) => !/sitemap.*\.xml$/.test(url)
			);
			if (regularUrls.length > 0) {
				await crawler.addRequests(regularUrls);
			}
			await crawler.run();
		} else {
			await crawler.run(seedUrls);
		}
	}
}
export async function write(
	config: ConfigWithDataset
): Promise<PathLike | null> {
	let nextFileNameString: PathLike | null = null;
	const storageDir =
		config.storageDir ||
		join(process.cwd(), 'storage', 'jobs', config.datasetName);
	const dataset = await Dataset.open(config.datasetName, {
		storageClient: new Configuration({
			storageClientOptions: {
				localDataDirectory: storageDir,
			},
		}).getStorageClient(),
	});
	const itemCount = (await dataset.getInfo())?.itemCount || 0;
	logger.info(
		{ itemCount },
		`Found ${itemCount} items in dataset to process...`
	);
	let currentResults: CrawledData[] = [];
	let currentSize: number = 0;
	let fileCounter: number = 1;
	const maxBytes: number = config.maxFileSize
		? config.maxFileSize * 1024 * 1024
		: Infinity;
	const getStringByteSize = (str: string): number =>
		Buffer.byteLength(str, 'utf-8');
	const nextFileName = (): string =>
		`${config.outputFileName!.replace(/\.json$/, '')}-${fileCounter}.json`;
	const writeBatchToFile = async (): Promise<void> => {
		nextFileNameString = nextFileName();
		const dir = dirname(nextFileNameString as string);
		await mkdir(dir, { recursive: true });
		await writeFile(
			nextFileNameString,
			JSON.stringify(currentResults, null, 2)
		);
		logger.info(
			{ count: currentResults.length, file: nextFileNameString },
			`Wrote ${currentResults.length} items to ${nextFileNameString}`
		);
		currentResults = [];
		currentSize = 0;
		fileCounter++;
	};
	let estimatedTokens: number = 0;
	const addContentOrSplit = async (data: CrawledData): Promise<void> => {
		const contentString: string = JSON.stringify(data);
		if (globalConfig.maxTokens !== 'unlimited') {
			const tokenCount: number | false = isWithinTokenLimit(
				contentString,
				globalConfig.maxTokens
			);
			if (typeof tokenCount === 'number') {
				if (estimatedTokens + tokenCount > globalConfig.maxTokens) {
					if (currentResults.length > 0) {
						await writeBatchToFile();
					}
					estimatedTokens = Math.floor(tokenCount / 2);
					currentResults.push(data);
				} else {
					currentResults.push(data);
					estimatedTokens += tokenCount;
				}
			}
		} else {
			currentResults.push(data);
		}
		currentSize += getStringByteSize(contentString);
		if (currentSize > maxBytes) {
			await writeBatchToFile();
		}
	};
	await dataset.forEach(async (item) => {
		const data: CrawledData = item as CrawledData;
		await addContentOrSplit(data);
	});
	if (currentResults.length > 0) {
		if (fileCounter === 1) {
			const finalFileName = config.outputFileName!;
			const dir = dirname(finalFileName);
			await mkdir(dir, { recursive: true });
			await writeFile(finalFileName, JSON.stringify(currentResults, null, 2));
			logger.info(
				{ count: currentResults.length, file: finalFileName },
				`Wrote ${currentResults.length} items to ${finalFileName}`
			);
			nextFileNameString = finalFileName;
		} else {
			await writeBatchToFile();
		}
	}
	return nextFileNameString;
}
export async function cleanupJobStorage(
	datasetName: string,
	storageDir?: string
): Promise<void> {
	try {
		const targetDir =
			storageDir || join(process.cwd(), 'storage', 'jobs', datasetName);
		try {
			await rm(targetDir, { recursive: true, force: true });
			logger.debug(
				{ storageDir: targetDir },
				'Cleaned up job storage directory'
			);
		} catch (error) {
			if ((error as NodeJS.ErrnoException).code !== 'ENOENT') {
				throw error;
			}
		}
	} catch (error) {
		logger.warn(
			{ datasetName, error: error instanceof Error ? error.message : error },
			'Failed to clean up job storage'
		);
	}
}
class ContextCrawlerCore {
	config: Config;
	datasetName: string;
	storageDir: string;
	constructor(config: Config) {
		this.config = {
			...config,
			outputFileName:
				config.outputFileName || generateOutputFileName(config.name),
		};
		this.datasetName = `ds-${randomBytes(4).toString('hex')}`;
		this.storageDir = join(process.cwd(), 'storage', 'jobs', this.datasetName);
	}
	async crawl() {
		const configWithDataset: ConfigWithDataset = {
			...this.config,
			datasetName: this.datasetName,
			storageDir: this.storageDir,
		};
		await crawl(configWithDataset);
	}
	async write(): Promise<PathLike | null> {
		const configWithDataset: ConfigWithDataset = {
			...this.config,
			datasetName: this.datasetName,
			storageDir: this.storageDir,
		};
		return write(configWithDataset);
	}
	async cleanup(): Promise<void> {
		await cleanupJobStorage(this.datasetName, this.storageDir);
	}
}
export default ContextCrawlerCore;
</file>

</files>

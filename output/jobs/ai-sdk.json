[
  {
    "title": "Foundations: Providers and Models",
    "url": "https://ai-sdk.dev/docs/foundations/providers-and-models",
    "html": "Foundations\nProviders and Models\nCopy markdown\nProviders and Models\n\nCompanies such as OpenAI and Anthropic (providers) offer access to a range of large language models (LLMs) with differing strengths and capabilities through their own APIs.\n\nEach provider typically has its own unique method for interfacing with their models, complicating the process of switching providers and increasing the risk of vendor lock-in.\n\nTo solve these challenges, AI SDK Core offers a standardized approach to interacting with LLMs through a language model specification\n that abstracts differences between providers. This unified interface allows you to switch between providers with ease while using the same API for all providers.\n\nHere is an overview of the AI SDK Provider Architecture:\n\nAI SDK Providers\n\nThe AI SDK comes with a wide range of providers that you can use to interact with different language models:\n\nxAI Grok Provider (@ai-sdk/xai)\nOpenAI Provider (@ai-sdk/openai)\nAzure OpenAI Provider (@ai-sdk/azure)\nAnthropic Provider (@ai-sdk/anthropic)\nAmazon Bedrock Provider (@ai-sdk/amazon-bedrock)\nGoogle Generative AI Provider (@ai-sdk/google)\nGoogle Vertex Provider (@ai-sdk/google-vertex)\nMistral Provider (@ai-sdk/mistral)\nTogether.ai Provider (@ai-sdk/togetherai)\nCohere Provider (@ai-sdk/cohere)\nFireworks Provider (@ai-sdk/fireworks)\nDeepInfra Provider (@ai-sdk/deepinfra)\nDeepSeek Provider (@ai-sdk/deepseek)\nCerebras Provider (@ai-sdk/cerebras)\nGroq Provider (@ai-sdk/groq)\nPerplexity Provider (@ai-sdk/perplexity)\nElevenLabs Provider (@ai-sdk/elevenlabs)\nLMNT Provider (@ai-sdk/lmnt)\nHume Provider (@ai-sdk/hume)\nRev.ai Provider (@ai-sdk/revai)\nDeepgram Provider (@ai-sdk/deepgram)\nGladia Provider (@ai-sdk/gladia)\nLMNT Provider (@ai-sdk/lmnt)\nAssemblyAI Provider (@ai-sdk/assemblyai)\nBaseten\n\nYou can also use the OpenAI Compatible provider with OpenAI-compatible APIs:\n\nLM Studio\nHeroku\n\nOur language model specification\n is published as an open-source package, which you can use to create custom providers.\n\nThe open-source community has created the following providers:\n\nOllama Provider (ollama-ai-provider)\nFriendliAI Provider (@friendliai/ai-provider)\nPortkey Provider (@portkey-ai/vercel-provider)\nCloudflare Workers AI Provider (workers-ai-provider)\nOpenRouter Provider (@openrouter/ai-sdk-provider)\nAihubmix Provider (@aihubmix/ai-sdk-provider)\nRequesty Provider (@requesty/ai-sdk)\nCrosshatch Provider (@crosshatch/ai-provider)\nMixedbread Provider (mixedbread-ai-provider)\nVoyage AI Provider (voyage-ai-provider)\nMem0 Provider(@mem0/vercel-ai-provider)\nLetta Provider(@letta-ai/vercel-ai-sdk-provider)\nSupermemory Provider(@supermemory/tools)\nSpark Provider (spark-ai-provider)\nAnthropicVertex Provider (anthropic-vertex-ai)\nLangDB Provider (@langdb/vercel-provider)\nDify Provider (dify-ai-provider)\nSarvam Provider (sarvam-ai-provider)\nClaude Code Provider (ai-sdk-provider-claude-code)\nBuilt-in AI Provider (built-in-ai)\nGemini CLI Provider (ai-sdk-provider-gemini-cli)\nA2A Provider (a2a-ai-provider)\nSAP-AI Provider (@mymediset/sap-ai-provider)\nAI/ML API Provider (@ai-ml.api/aimlapi-vercel-ai)\nSelf-Hosted Models\n\nYou can access self-hosted models with the following providers:\n\nOllama Provider\nLM Studio\nBaseten\nBuilt-in AI\n\nAdditionally, any self-hosted provider that supports the OpenAI specification can be used with the OpenAI Compatible Provider.\n\nModel Capabilities\n\nThe AI providers support different language models with various capabilities. Here are the capabilities of popular models:\n\nProvider\tModel\tImage Input\tObject Generation\tTool Usage\tTool Streaming\nxAI Grok\tgrok-4\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-3\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-3-fast\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-3-mini\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-3-mini-fast\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-2-1212\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-2-vision-1212\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-beta\t\n\t\n\t\n\t\n\nxAI Grok\tgrok-vision-beta\t\n\t\n\t\n\t\n\nVercel\tv0-1.0-md\t\n\t\n\t\n\t\n\nOpenAI\tgpt-5\t\n\t\n\t\n\t\n\nOpenAI\tgpt-5-mini\t\n\t\n\t\n\t\n\nOpenAI\tgpt-5-nano\t\n\t\n\t\n\t\n\nOpenAI\tgpt-5-codex\t\n\t\n\t\n\t\n\nOpenAI\tgpt-5-chat-latest\t\n\t\n\t\n\t\n\nAnthropic\tclaude-opus-4-1\t\n\t\n\t\n\t\n\nAnthropic\tclaude-opus-4-0\t\n\t\n\t\n\t\n\nAnthropic\tclaude-sonnet-4-0\t\n\t\n\t\n\t\n\nAnthropic\tclaude-3-7-sonnet-latest\t\n\t\n\t\n\t\n\nAnthropic\tclaude-3-5-haiku-latest\t\n\t\n\t\n\t\n\nMistral\tpixtral-large-latest\t\n\t\n\t\n\t\n\nMistral\tmistral-large-latest\t\n\t\n\t\n\t\n\nMistral\tmistral-medium-latest\t\n\t\n\t\n\t\n\nMistral\tmistral-medium-2505\t\n\t\n\t\n\t\n\nMistral\tmistral-small-latest\t\n\t\n\t\n\t\n\nMistral\tpixtral-12b-2409\t\n\t\n\t\n\t\n\nGoogle Generative AI\tgemini-2.0-flash-exp\t\n\t\n\t\n\t\n\nGoogle Generative AI\tgemini-1.5-flash\t\n\t\n\t\n\t\n\nGoogle Generative AI\tgemini-1.5-pro\t\n\t\n\t\n\t\n\nGoogle Vertex\tgemini-2.0-flash-exp\t\n\t\n\t\n\t\n\nGoogle Vertex\tgemini-1.5-flash\t\n\t\n\t\n\t\n\nGoogle Vertex\tgemini-1.5-pro\t\n\t\n\t\n\t\n\nDeepSeek\tdeepseek-chat\t\n\t\n\t\n\t\n\nDeepSeek\tdeepseek-reasoner\t\n\t\n\t\n\t\n\nCerebras\tllama3.1-8b\t\n\t\n\t\n\t\n\nCerebras\tllama3.1-70b\t\n\t\n\t\n\t\n\nCerebras\tllama3.3-70b\t\n\t\n\t\n\t\n\nGroq\tmeta-llama/llama-4-scout-17b-16e-instruct\t\n\t\n\t\n\t\n\nGroq\tllama-3.3-70b-versatile\t\n\t\n\t\n\t\n\nGroq\tllama-3.1-8b-instant\t\n\t\n\t\n\t\n\nGroq\tmixtral-8x7b-32768\t\n\t\n\t\n\t\n\nGroq\tgemma2-9b-it\t\n\t\n\t\n\t\n\nThis table is not exhaustive. Additional models can be found in the provider documentation pages and on the provider websites.\n\nPrevious\nOverview\nNext\nPrompts"
  },
  {
    "title": "Foundations: Overview",
    "url": "https://ai-sdk.dev/docs/foundations/overview",
    "html": "Foundations\nOverview\nCopy markdown\nOverview\n\nThis page is a beginner-friendly introduction to high-level artificial intelligence (AI) concepts. To dive right into implementing the AI SDK, feel free to skip ahead to our quickstarts or learn about our supported models and providers.\n\nThe AI SDK standardizes integrating artificial intelligence (AI) models across supported providers. This enables developers to focus on building great AI applications, not waste time on technical details.\n\nFor example, here’s how you can generate text with various models using the AI SDK:\n\nxAI\nOpenAI\nAnthropic\nGoogle\nCustom\nimport { generateText } from \"ai\"\nimport { xai } from \"@ai-sdk/xai\"\nconst { text } = await generateText({\nmodel: xai(\"grok-4\"),\nprompt: \"What is love?\"\n})\nLove is a universal emotion that is characterized by feelings of affection, attachment, and warmth towards someone or something. It is a complex and multifaceted experience that can take many different forms, including romantic love, familial love, platonic love, and self-love.\n\nTo effectively leverage the AI SDK, it helps to familiarize yourself with the following concepts:\n\nGenerative Artificial Intelligence\n\nGenerative artificial intelligence refers to models that predict and generate various types of outputs (such as text, images, or audio) based on what’s statistically likely, pulling from patterns they’ve learned from their training data. For example:\n\nGiven a photo, a generative model can generate a caption.\nGiven an audio file, a generative model can generate a transcription.\nGiven a text description, a generative model can generate an image.\nLarge Language Models\n\nA large language model (LLM) is a subset of generative models focused primarily on text. An LLM takes a sequence of words as input and aims to predict the most likely sequence to follow. It assigns probabilities to potential next sequences and then selects one. The model continues to generate sequences until it meets a specified stopping criterion.\n\nLLMs learn by training on massive collections of written text, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well.\n\nHowever, it's crucial to understand LLMs' limitations. When asked about less known or absent information, like the birthday of a personal relative, LLMs might \"hallucinate\" or make up information. It's essential to consider how well-represented the information you need is in the model.\n\nEmbedding Models\n\nAn embedding model is used to convert complex data (like words or images) into a dense vector (a list of numbers) representation, known as an embedding. Unlike generative models, embedding models do not generate new text or data. Instead, they provide representations of semantic and syntactic relationships between entities that can be used as input for other models or other natural language processing tasks.\n\nIn the next section, you will learn about the difference between models providers and models, and which ones are available in the AI SDK.\n\nPrevious\nFoundations\nNext\nProviders and Models"
  },
  {
    "title": "Foundations: Tools",
    "url": "https://ai-sdk.dev/docs/foundations/tools",
    "html": "Foundations\nTools\nCopy markdown\nTools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather).\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, when you ask an LLM for the \"weather in London\", and there is a weather tool available, it could call a tool with London as the argument. The tool would then fetch the weather data and return it to the LLM. The LLM can then use this information in its response.\n\nWhat is a tool?\n\nA tool is an object that can be called by the model to perform a specific task. You can use tools with generateText and streamText by passing one or more tools to the tools parameter.\n\nA tool consists of three properties:\n\ndescription: An optional description of the tool that can influence when the tool is picked.\ninputSchema: A Zod schema or a JSON schema that defines the input required for the tool to run. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\nexecute: An optional async function that is called with the arguments from the tool call.\n\nstreamUI uses UI generator tools with a generate function that can return React components.\n\nIf the LLM decides to use a tool, it will generate a tool call. Tools with an execute function are run automatically when these calls are generated. The output of the tool calls are returned using tool result objects.\n\nYou can automatically pass tool results back to the LLM using multi-step calls with streamText and generateText.\n\nSchemas\n\nSchemas are used to define the parameters for tools and to validate the tool calls.\n\nThe AI SDK supports both raw JSON schemas (using the jsonSchema function) and Zod\n schemas (either directly or using the zodSchema function).\n\nZod\n is a popular TypeScript schema validation library. You can install it with:\n\npnpm\nnpm\nyarn\nbun\npnpm add zod\n\nYou can then specify a Zod schema, for example:\n\nimport z from 'zod';\n\n\nconst recipeSchema = z.object({\n  recipe: z.object({\n    name: z.string(),\n    ingredients: z.array(\n      z.object({\n        name: z.string(),\n        amount: z.string(),\n      }),\n    ),\n    steps: z.array(z.string()),\n  }),\n});\n\nYou can also use schemas for structured output generation with generateObject and streamObject.\n\nToolkits\n\nWhen you work with tools, you typically need a mix of application specific tools and general purpose tools. There are several providers that offer pre-built tools as toolkits that you can use out of the box:\n\nagentic\n - A collection of 20+ tools. Most tools connect to access external APIs such as Exa\n or E2B\n.\nbrowserbase\n - Browser tool that runs a headless browser\nbrowserless\n - Browser automation service with AI integration - self hosted or cloud based\nStripe agent tools\n - Tools for interacting with Stripe.\nStackOne ToolSet\n - Agentic integrations for hundreds of enterprise SaaS\nToolhouse\n - AI function-calling in 3 lines of code for over 25 different actions.\nAgent Tools\n - A collection of tools for agents.\nAI Tool Maker\n - A CLI utility to generate AI SDK tools from OpenAPI specs.\nComposio\n - Composio provides 250+ tools like GitHub, Gmail, Salesforce and more\n.\nInterlify\n - Convert APIs into tools so that AI can connect to your backend in minutes.\nJigsawStack\n - JigsawStack provides over 30+ small custom fine tuned models available for specific uses.\nAI Tools Registry\n - A Shadcn compatible tool definitions and components registry for the AI SDK.\nDeepAgent\n - A powerful suite of 50+ AI tools and integrations, seamlessly connecting with APIs like Tavily, E2B, Airtable and more\n to build enterprise-ready AI agents.\nSmithery\n - Smithery provides an open marketplace of 6K+ MCPs, including Browserbase\n and Exa\n.\n\nDo you have open source tools or tool libraries that are compatible with the AI SDK? Please file a pull request\n to add them to this list.\n\nLearn more\n\nThe AI SDK Core Tool Calling and Agents documentation has more information about tools and tool calling.\n\nPrevious\nPrompts\nNext\nStreaming"
  },
  {
    "title": "Foundations: Prompts",
    "url": "https://ai-sdk.dev/docs/foundations/prompts",
    "html": "Foundations\nPrompts\nCopy markdown\nPrompts\n\nPrompts are instructions that you give a large language model (LLM) to tell it what to do. It's like when you ask someone for directions; the clearer your question, the better the directions you'll get.\n\nMany LLM providers offer complex interfaces for specifying prompts. They involve different roles and message types. While these interfaces are powerful, they can be hard to use and understand.\n\nIn order to simplify prompting, the AI SDK supports text, message, and system prompts.\n\nText Prompts\n\nText prompts are strings. They are ideal for simple generation use cases, e.g. repeatedly generating content for variants of the same prompt text.\n\nYou can set text prompts using the prompt property made available by AI SDK functions like streamText or generateObject. You can structure the text in any way and inject variables, e.g. using a template literal.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nYou can also use template literals to provide dynamic data to your prompt.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  prompt:\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n    `Please suggest the best tourist activities for me to do.`,\n});\nSystem Prompts\n\nSystem prompts are the initial set of instructions given to models that help guide and constrain the models' behaviors and responses. You can set system prompts using the system property. System prompts work with both the prompt and the messages properties.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  system:\n    `You help planning travel itineraries. ` +\n    `Respond to the users' request with a list ` +\n    `of the best stops to make in their destination.`,\n  prompt:\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n    `Please suggest the best tourist activities for me to do.`,\n});\n\nWhen you use a message prompt, you can also use system messages instead of a system prompt.\n\nMessage Prompts\n\nA message prompt is an array of user, assistant, and tool messages. They are great for chat interfaces and more complex, multi-modal prompts. You can use the messages property to set message prompts.\n\nEach message has a role and a content property. The content can either be text (for user and assistant messages), or an array of relevant parts (data) for that message type.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hello, how can I help?' },\n    { role: 'user', content: 'Where can I buy the best Currywurst in Berlin?' },\n  ],\n});\n\nInstead of sending a text in the content property, you can send an array of parts that includes a mix of text and other content parts.\n\nNot all language models support all message and content types. For example, some models might not be capable of handling multi-modal inputs or tool messages. Learn more about the capabilities of select models.\n\nProvider Options\n\nYou can pass through additional provider-specific metadata to enable provider-specific functionality at 3 levels.\n\nFunction Call Level\n\nFunctions like streamText or generateText accept a providerOptions property.\n\nAdding provider options at the function call level should be used when you do not need granular control over where the provider options are applied.\n\nconst { text } = await generateText({\n  model: azure('your-deployment-name'),\n  providerOptions: {\n    openai: {\n      reasoningEffort: 'low',\n    },\n  },\n});\nMessage Level\n\nFor granular control over applying provider options at the message level, you can pass providerOptions to the message object:\n\nimport { ModelMessage } from 'ai';\n\n\nconst messages: ModelMessage[] = [\n  {\n    role: 'system',\n    content: 'Cached system message',\n    providerOptions: {\n      // Sets a cache control breakpoint on the system message\n      anthropic: { cacheControl: { type: 'ephemeral' } },\n    },\n  },\n];\nMessage Part Level\n\nCertain provider-specific options require configuration at the message part level:\n\nimport { ModelMessage } from 'ai';\n\n\nconst messages: ModelMessage[] = [\n  {\n    role: 'user',\n    content: [\n      {\n        type: 'text',\n        text: 'Describe the image in detail.',\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n      {\n        type: 'image',\n        image:\n          'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n        // Sets image detail configuration for image part:\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n    ],\n  },\n];\n\nAI SDK UI hooks like useChat return arrays of UIMessage objects, which do not support provider options. We recommend using the convertToModelMessages function to convert UIMessage objects to ModelMessage objects before applying or appending message(s) or message parts with providerOptions.\n\nUser Messages\nText Parts\n\nText content is the most common type of content. It is a string that is passed to the model.\n\nIf you only need to send text content in a message, the content property can be a string, but you can also use it to send multiple content parts.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'Where can I buy the best Currywurst in Berlin?',\n        },\n      ],\n    },\n  ],\n});\nImage Parts\n\nUser messages can include image parts. An image can be one of the following:\n\nbase64-encoded image:\nstring with base-64 encoded content\ndata URL string, e.g. data:image/png;base64,...\nbinary image:\nArrayBuffer\nUint8Array\nBuffer\nURL:\nhttp(s) URL string, e.g. https://example.com/image.png\nURL object, e.g. new URL('https://example.com/image.png')\nExample: Binary image (Buffer)\nconst result = await generateText({\n  model,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image: fs.readFileSync('./data/comic-cat.png'),\n        },\n      ],\n    },\n  ],\n});\nExample: Base-64 encoded image (string)\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image: fs.readFileSync('./data/comic-cat.png').toString('base64'),\n        },\n      ],\n    },\n  ],\n});\nExample: Image URL (string)\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image:\n            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n        },\n      ],\n    },\n  ],\n});\nFile Parts\n\nOnly a few providers and models currently support file parts: Google Generative AI, Google Vertex AI, OpenAI (for wav and mp3 audio with gpt-4o-audio-preview), Anthropic, OpenAI (for pdf).\n\nUser messages can include file parts. A file can be one of the following:\n\nbase64-encoded file:\nstring with base-64 encoded content\ndata URL string, e.g. data:image/png;base64,...\nbinary data:\nArrayBuffer\nUint8Array\nBuffer\nURL:\nhttp(s) URL string, e.g. https://example.com/some.pdf\nURL object, e.g. new URL('https://example.com/some.pdf')\n\nYou need to specify the MIME type of the file you are sending.\n\nExample: PDF file from Buffer\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  model: google('gemini-1.5-flash'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the file about?' },\n        {\n          type: 'file',\n          mediaType: 'application/pdf',\n          data: fs.readFileSync('./data/example.pdf'),\n          filename: 'example.pdf', // optional, not used by all providers\n        },\n      ],\n    },\n  ],\n});\nExample: mp3 audio file from Buffer\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  model: openai('gpt-4o-audio-preview'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the audio saying?' },\n        {\n          type: 'file',\n          mediaType: 'audio/mpeg',\n          data: fs.readFileSync('./data/galileo.mp3'),\n        },\n      ],\n    },\n  ],\n});\nCustom Download Function (Experimental)\n\nYou can use custom download functions to implement throttling, retries, authentication, caching, and more.\n\nThe default download implementation automatically downloads files in parallel when they are not supported by the model.\n\nCustom download function can be passed via the experimental_download property:\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  experimental_download: async (\n    requestedDownloads: Array<{\n      url: URL;\n      isUrlSupportedByModel: boolean;\n    }>,\n  ): PromiseLike<\n    Array<{\n      data: Uint8Array;\n      mediaType: string | undefined;\n    } | null>\n  > => {\n    // ... download the files and return an array with similar order\n  },\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'file',\n          data: new URL('https://api.company.com/private/document.pdf'),\n          mediaType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n\nThe experimental_download option is experimental and may change in future releases.\n\nAssistant Messages\n\nAssistant messages are messages that have a role of assistant. They are typically previous responses from the assistant and can contain text, reasoning, and tool call parts.\n\nExample: Assistant message with text content\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hello, how can I help?' },\n  ],\n});\nExample: Assistant message with text content in array\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    {\n      role: 'assistant',\n      content: [{ type: 'text', text: 'Hello, how can I help?' }],\n    },\n  ],\n});\nExample: Assistant message with tool call content\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'How many calories are in this block of cheese?' },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'tool-call',\n          toolCallId: '12345',\n          toolName: 'get-nutrition-data',\n          input: { cheese: 'Roquefort' },\n        },\n      ],\n    },\n  ],\n});\nExample: Assistant message with file content\n\nThis content part is for model-generated files. Only a few models support this, and only for file types that they can generate.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Generate an image of a roquefort cheese!' },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'file',\n          mediaType: 'image/png',\n          data: fs.readFileSync('./data/roquefort.jpg'),\n        },\n      ],\n    },\n  ],\n});\nTool messages\n\nTools (also known as function calling) are programs that you can provide an LLM to extend its built-in functionality. This can be anything from calling an external API to calling functions within your UI. Learn more about Tools in the next section.\n\nFor models that support tool calls, assistant messages can contain tool call parts, and tool messages can contain tool output parts. A single assistant message can call multiple tools, and a single tool message can contain multiple tool results.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'How many calories are in this block of cheese?',\n        },\n        { type: 'image', image: fs.readFileSync('./data/roquefort.jpg') },\n      ],\n    },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'tool-call',\n          toolCallId: '12345',\n          toolName: 'get-nutrition-data',\n          input: { cheese: 'Roquefort' },\n        },\n        // there could be more tool calls here (parallel calling)\n      ],\n    },\n    {\n      role: 'tool',\n      content: [\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          output: {\n            type: 'json',\n            value: {\n              name: 'Cheese, roquefort',\n              calories: 369,\n              fat: 31,\n              protein: 22,\n            },\n          },\n        },\n        // there could be more tool results here (parallel calling)\n      ],\n    },\n  ],\n});\nMulti-modal Tool Results\n\nMulti-part tool results are experimental and only supported by Anthropic.\n\nTool results can be multi-part and multi-modal, e.g. a text and an image. You can use the experimental_content property on tool parts to specify multi-part tool results.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    // ...\n    {\n      role: 'tool',\n      content: [\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          // for models that do not support multi-part tool results,\n          // you can include a regular output part:\n          output: {\n            type: 'json',\n            value: {\n              name: 'Cheese, roquefort',\n              calories: 369,\n              fat: 31,\n              protein: 22,\n            },\n          },\n        },\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          // for models that support multi-part tool results,\n          // you can include a multi-part content part:\n          output: {\n            type: 'content',\n            value: [\n              {\n                type: 'text',\n                text: 'Here is an image of the nutrition data for the cheese:',\n              },\n              {\n                type: 'media',\n                data: fs\n                  .readFileSync('./data/roquefort-nutrition-data.png')\n                  .toString('base64'),\n                mediaType: 'image/png',\n              },\n            ],\n          },\n        },\n      ],\n    },\n  ],\n});\nSystem Messages\n\nSystem messages are messages that are sent to the model before the user messages to guide the assistant's behavior. You can alternatively use the system property.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'system', content: 'You help planning travel itineraries.' },\n    {\n      role: 'user',\n      content:\n        'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.',\n    },\n  ],\n});\nPrevious\nProviders and Models\nNext\nTools"
  },
  {
    "title": "Foundations: Streaming",
    "url": "https://ai-sdk.dev/docs/foundations/streaming",
    "html": "Foundations\nStreaming\nCopy markdown\nStreaming\n\nStreaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.\n\nLarge language models (LLMs) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you're likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by displaying parts of the response as they become available.\n\nBlocking UI\n\nBlocking responses wait until the full response is available before displaying it.\n\nStreaming UI\n\nStreaming responses can transmit parts of the response as they become available.\n\nReal-world Examples\n\nHere are 2 examples that illustrate how streaming UIs can improve user experiences in a real-world setting – the first uses a blocking UI, while the second uses a streaming UI.\n\nBlocking UI\nGenerate\n...\nStreaming UI\nGenerate\n...\n\nAs you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.\n\nWhile streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren't always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.\n\nHowever, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible. In the example below, we stream text generation from OpenAI's gpt-4.1 in under 10 lines of code using the SDK's streamText function:\n\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n\nconst { textStream } = streamText({\n  model: openai('gpt-4.1'),\n  prompt: 'Write a poem about embedding models.',\n});\n\n\nfor await (const textPart of textStream) {\n  console.log(textPart);\n}\n\nFor an introduction to streaming UIs and the AI SDK, check out our Getting Started guides.\n\nPrevious\nTools\nNext\nGetting Started"
  },
  {
    "title": "Getting Started: Navigating the Library",
    "url": "https://ai-sdk.dev/docs/getting-started/navigating-the-library",
    "html": "Getting Started\nNavigating the Library\nCopy markdown\nNavigating the Library\n\nThe AI SDK is a powerful toolkit for building AI applications. This page will help you pick the right tools for your requirements.\n\nLet’s start with a quick overview of the AI SDK, which is comprised of three parts:\n\nAI SDK Core: A unified, provider agnostic API for generating text, structured objects, and tool calls with LLMs.\nAI SDK UI: A set of framework-agnostic hooks for building chat and generative user interfaces.\nAI SDK RSC: Stream generative user interfaces with React Server Components (RSC). Development is currently experimental and we recommend using AI SDK UI.\nChoosing the Right Tool for Your Environment\n\nWhen deciding which part of the AI SDK to use, your first consideration should be the environment and existing stack you are working with. Different components of the SDK are tailored to specific frameworks and environments.\n\nLibrary\tPurpose\tEnvironment Compatibility\nAI SDK Core\tCall any LLM with unified API (e.g. generateText and generateObject)\tAny JS environment (e.g. Node.js, Deno, Browser)\nAI SDK UI\tBuild streaming chat and generative UIs (e.g. useChat)\tReact & Next.js, Vue & Nuxt, Svelte & SvelteKit\nAI SDK RSC\tStream generative UIs from Server to Client (e.g. streamUI). Development is currently experimental and we recommend using AI SDK UI.\tAny framework that supports React Server Components (e.g. Next.js)\nEnvironment Compatibility\n\nThese tools have been designed to work seamlessly with each other and it's likely that you will be using them together. Let's look at how you could decide which libraries to use based on your application environment, existing stack, and requirements.\n\nThe following table outlines AI SDK compatibility based on environment:\n\nEnvironment\tAI SDK Core\tAI SDK UI\tAI SDK RSC\nNone / Node.js / Deno\t\n\t\n\t\n\nVue / Nuxt\t\n\t\n\t\n\nSvelte / SvelteKit\t\n\t\n\t\n\nNext.js Pages Router\t\n\t\n\t\n\nNext.js App Router\t\n\t\n\t\nWhen to use AI SDK UI\n\nAI SDK UI provides a set of framework-agnostic hooks for quickly building production-ready AI-native applications. It offers:\n\nFull support for streaming chat and client-side generative UI\nUtilities for handling common AI interaction patterns (i.e. chat, completion, assistant)\nProduction-tested reliability and performance\nCompatibility across popular frameworks\nAI SDK UI Framework Compatibility\n\nAI SDK UI supports the following frameworks: React\n, Svelte\n, and Vue.js\n. Here is a comparison of the supported functions across these frameworks:\n\nFunction\tReact\tSvelte\tVue.js\nuseChat\t\n\t\n\t\n\nuseChat tool calling\t\n\t\n\t\n\nuseCompletion\t\n\t\n\t\n\nuseObject\t\n\t\n\t\n\nContributions\n are welcome to implement missing features for non-React frameworks.\n\nWhen to use AI SDK RSC\n\nAI SDK RSC is currently experimental. We recommend using AI SDK UI for production. For guidance on migrating from RSC to UI, see our migration guide.\n\nReact Server Components\n (RSCs) provide a new approach to building React applications that allow components to render on the server, fetch data directly, and stream the results to the client, reducing bundle size and improving performance. They also introduce a new way to call server-side functions from anywhere in your application called Server Actions\n.\n\nAI SDK RSC provides a number of utilities that allow you to stream values and UI directly from the server to the client. However, it's important to be aware of current limitations:\n\nCancellation: currently, it is not possible to abort a stream using Server Actions. This will be improved in future releases of React and Next.js.\nIncreased Data Transfer: using createStreamableUI can lead to quadratic data transfer (quadratic to the length of generated text). You can avoid this using createStreamableValue instead, and rendering the component client-side.\nRe-mounting Issue During Streaming: when using createStreamableUI, components re-mount on .done(), causing flickering\n.\n\nGiven these limitations, we recommend using AI SDK UI for production applications.\n\nPrevious\nGetting Started\nNext\nNext.js App Router"
  },
  {
    "title": "Getting Started: Next.js App Router",
    "url": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router",
    "html": "Getting Started\nNext.js App Router\nCopy markdown\nNext.js App Router Quickstart\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of Prompt Engineering and HTTP Streaming, you can optionally read these documents first.\n\nPrerequisites\n\nTo follow this quickstart, you'll need:\n\nNode.js 18+ and pnpm installed on your local development machine.\nAn OpenAI API key.\n\nIf you haven't obtained your OpenAI API key, you can do so by signing up\n on the OpenAI website.\n\nCreate Your Application\n\nStart by creating a new Next.js application. This command will create a new directory named my-ai-app and set up a basic Next.js application inside it.\n\nBe sure to select yes when prompted to use the App Router and Tailwind CSS. If you are looking for the Next.js Pages Router quickstart guide, you can find it here.\n\npnpm create next-app@latest my-ai-app\n\nNavigate to the newly created directory:\n\ncd my-ai-app\nInstall dependencies\n\nInstall ai, @ai-sdk/react, and @ai-sdk/openai, the AI package, AI SDK's React hooks, and AI SDK's OpenAI provider respectively.\n\nThe AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about available providers and building custom providers in the providers section.\n\npnpm\nnpm\nyarn\nbun\npnpm add ai @ai-sdk/react @ai-sdk/openai zod\nConfigure OpenAI API key\n\nCreate a .env.local file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.\n\ntouch .env.local\n\nEdit the .env.local file:\n\n.env.local\nOPENAI_API_KEY=xxxxxxxxx\n\nReplace xxxxxxxxx with your actual OpenAI API key.\n\nThe AI SDK's OpenAI Provider will default to using the OPENAI_API_KEY environment variable.\n\nCreate a Route Handler\n\nCreate a route handler, app/api/chat/route.ts and add the following code:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nLet's take a look at what is happening in this code:\n\nDefine an asynchronous POST request handler and extract messages from the body of the request. The messages variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The messages are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\nCall streamText, which is imported from the ai package. This function accepts a configuration object that contains a model provider (imported from @ai-sdk/openai) and messages (defined in step 1). You can pass additional settings to further customise the model's behaviour. The messages key expects a ModelMessage[] array. This type is different from UIMessage in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the convertToModelMessages function, which strips the UI-specific metadata and transforms the UIMessage[] array into the ModelMessage[] format that the model expects.\nThe streamText function returns a StreamTextResult. This result object contains the toUIMessageStreamResponse function which converts the result to a streamed response object.\nFinally, return the result to the client to stream the response.\n\nThis Route Handler creates a POST request endpoint at /api/chat.\n\nWire up the UI\n\nNow that you have a Route Handler that can query an LLM, it's time to setup your frontend. The AI SDK's UI package abstracts the complexity of a chat interface into one hook, useChat.\n\nUpdate your root page (app/page.tsx) with the following code to show a list of chat messages and provide a user message input:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\n\nMake sure you add the \"use client\" directive to the top of your file. This allows you to add interactivity with Javascript.\n\nThis page utilizes the useChat hook, which will, by default, use the POST API route you created earlier (/api/chat). The hook provides functions and state for handling user input and form submission. The useChat hook provides multiple utility functions and state variables:\n\nmessages - the current chat messages (an array of objects with id, role, and parts properties).\nsendMessage - a function to send a message to the chat API.\n\nThe component uses local state (useState) to manage the input field value, and handles form submission by calling sendMessage with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message parts array. Each message contains an ordered array of parts that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The parts array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\nRunning Your Application\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\npnpm run dev\n\nHead to your browser and open http://localhost:3000\n. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\n\nEnhance Your Chatbot with Tools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where tools come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\nUpdate Your Route Handler\n\nModify your app/api/chat/route.ts file to include the new weather tool:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\nimport { z } from 'zod';\n\n\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIn this updated code:\n\nYou import the tool function from the ai package and z from zod for schema validation.\n\nYou define a tools object with a weather tool. This tool:\n\nHas a description that helps the model understand when to use it.\nDefines inputSchema using a Zod schema, specifying that it requires a location string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\nDefines an execute function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The execute function will then be automatically run, and the tool output will be added to the messages as a tool message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the tool-weather part of the message.parts array.\n\nTool parts are always named tool-{toolName}, where {toolName} is the key you used when defining the tool. In this case, since we defined the tool as weather, the part type is tool-weather.\n\nUpdate the UI\n\nTo display the tool invocation in your UI, update your app/page.tsx file:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n              case 'tool-weather':\n                return (\n                  <pre key={`${message.id}-${i}`}>\n                    {JSON.stringify(part, null, 2)}\n                  </pre>\n                );\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\nEnabling Multi-Step Tool Calls\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using stopWhen. By default, stopWhen is set to stepCountIs(1), which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\nUpdate Your Route Handler\n\nModify your app/api/chat/route.ts file to include the stopWhen condition:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIn this updated code:\n\nYou set stopWhen to be when stepCountIs 5, allowing the model to use up to 5 \"steps\" for any given generation.\nYou add an onStepFinish callback to log any toolResults from each step of the interaction, helping you understand the model's tool usage. This means we can also delete the toolCall and toolResult console.log statements from the previous example.\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting stopWhen: stepCountIs(5), you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\nAdd another tool\n\nUpdate your app/api/chat/route.ts file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        inputSchema: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nUpdate Your Frontend\n\nupdate your app/page.tsx file to render the new temperature conversion tool:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n              case 'tool-weather':\n              case 'tool-convertFahrenheitToCelsius':\n                return (\n                  <pre key={`${message.id}-${i}`}>\n                    {JSON.stringify(part, null, 2)}\n                  </pre>\n                );\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\n\nThis update handles the new tool-convertFahrenheitToCelsius part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\nThe model will call the weather tool for New York.\nYou'll see the tool output displayed.\nIt will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\nThe model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\nWhere to Next?\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\nTo learn more about the AI SDK, read through the documentation.\nIf you're interested in diving deeper with guides, check out the RAG (retrieval-augmented generation) and multi-modal chatbot guides.\nTo jumpstart your first AI project, explore available templates\n.\nPrevious\nNavigating the Library\nNext\nNext.js Pages Router"
  },
  {
    "title": "Getting Started: Next.js Pages Router",
    "url": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router",
    "html": "Getting Started\nNext.js Pages Router\nCopy markdown\nNext.js Pages Router Quickstart\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of Prompt Engineering and HTTP Streaming, you can optionally read these documents first.\n\nPrerequisites\n\nTo follow this quickstart, you'll need:\n\nNode.js 18+ and pnpm installed on your local development machine.\nAn OpenAI API key.\n\nIf you haven't obtained your OpenAI API key, you can do so by signing up\n on the OpenAI website.\n\nSetup Your Application\n\nStart by creating a new Next.js application. This command will create a new directory named my-ai-app and set up a basic Next.js application inside it.\n\nBe sure to select no when prompted to use the App Router. If you are looking for the Next.js App Router quickstart guide, you can find it here.\n\npnpm create next-app@latest my-ai-app\n\nNavigate to the newly created directory:\n\ncd my-ai-app\nInstall dependencies\n\nInstall ai, @ai-sdk/react, and @ai-sdk/openai, the AI package, AI SDK's React hooks, and AI SDK's OpenAI provider respectively.\n\nThe AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about available providers and building custom providers in the providers section.\n\npnpm\nnpm\nyarn\nbun\npnpm add ai @ai-sdk/react @ai-sdk/openai zod\nConfigure OpenAI API key\n\nCreate a .env.local file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.\n\ntouch .env.local\n\nEdit the .env.local file:\n\n.env.local\nOPENAI_API_KEY=xxxxxxxxx\n\nReplace xxxxxxxxx with your actual OpenAI API key.\n\nThe AI SDK's OpenAI Provider will default to using the OPENAI_API_KEY environment variable.\n\nCreate a Route Handler\n\nAs long as you are on Next.js 13+, you can use Route Handlers (using the App Router) alongside the Pages Router. This is recommended to enable you to use the Web APIs interface/signature and to better support streaming.\n\nCreate a Route Handler (app/api/chat/route.ts) and add the following code:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nLet's take a look at what is happening in this code:\n\nDefine an asynchronous POST request handler and extract messages from the body of the request. The messages variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The messages are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\nCall streamText, which is imported from the ai package. This function accepts a configuration object that contains a model provider (imported from @ai-sdk/openai) and messages (defined in step 1). You can pass additional settings to further customise the model's behaviour. The messages key expects a ModelMessage[] array. This type is different from UIMessage in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the convertToModelMessages function, which strips the UI-specific metadata and transforms the UIMessage[] array into the ModelMessage[] format that the model expects.\nThe streamText function returns a StreamTextResult. This result object contains the toUIMessageStreamResponse function which converts the result to a streamed response object.\nFinally, return the result to the client to stream the response.\n\nThis Route Handler creates a POST request endpoint at /api/chat.\n\nWire up the UI\n\nNow that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's UI package abstract the complexity of a chat interface into one hook, useChat.\n\nUpdate your root page (pages/index.tsx) with the following code to show a list of chat messages and provide a user message input:\n\npages/index.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\n\nThis page utilizes the useChat hook, which will, by default, use the POST API route you created earlier (/api/chat). The hook provides functions and state for handling user input and form submission. The useChat hook provides multiple utility functions and state variables:\n\nmessages - the current chat messages (an array of objects with id, role, and parts properties).\nsendMessage - a function to send a message to the chat API.\n\nThe component uses local state (useState) to manage the input field value, and handles form submission by calling sendMessage with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message parts array. Each message contains an ordered array of parts that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The parts array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\nRunning Your Application\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\npnpm run dev\n\nHead to your browser and open http://localhost:3000\n. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\n\nEnhance Your Chatbot with Tools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where tools come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nUpdate Your Route Handler\n\nLet's start by giving your chatbot a weather tool. Update your Route Handler (app/api/chat/route.ts):\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\nimport { z } from 'zod';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIn this updated code:\n\nYou import the tool function from the ai package and z from zod for schema validation.\n\nYou define a tools object with a weather tool. This tool:\n\nHas a description that helps the model understand when to use it.\nDefines inputSchema using a Zod schema, specifying that it requires a location string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\nDefines an execute function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The execute function will then be automatically run, and the tool output will be added to the messages as a tool message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the tool-weather part of the message.parts array.\n\nTool parts are always named tool-{toolName}, where {toolName} is the key you used when defining the tool. In this case, since we defined the tool as weather, the part type is tool-weather.\n\nUpdate the UI\n\nTo display the tool invocations in your UI, update your pages/index.tsx file:\n\npages/index.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n              case 'tool-weather':\n                return (\n                  <pre key={`${message.id}-${i}`}>\n                    {JSON.stringify(part, null, 2)}\n                  </pre>\n                );\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\nEnabling Multi-Step Tool Calls\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using stopWhen. By default, stopWhen is set to stepCountIs(1), which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\nUpdate Your Route Handler\n\nModify your app/api/chat/route.ts file to include the stopWhen condition:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting stopWhen: stepCountIs(5), you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\nAdd another tool\n\nUpdate your app/api/chat/route.ts file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        inputSchema: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nUpdate Your Frontend\n\nUpdate your pages/index.tsx file to render the new temperature conversion tool:\n\npages/index.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n              case 'tool-weather':\n              case 'tool-convertFahrenheitToCelsius':\n                return (\n                  <pre key={`${message.id}-${i}`}>\n                    {JSON.stringify(part, null, 2)}\n                  </pre>\n                );\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\n\nThis update handles the new tool-convertFahrenheitToCelsius part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\nThe model will call the weather tool for New York.\nYou'll see the tool output displayed.\nIt will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\nThe model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\nWhere to Next?\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\nTo learn more about the AI SDK, read through the documentation.\nIf you're interested in diving deeper with guides, check out the RAG (retrieval-augmented generation) and multi-modal chatbot guides.\nTo jumpstart your first AI project, explore available templates\n.\nPrevious\nNext.js App Router\nNext\nSvelte"
  },
  {
    "title": "Getting Started: Svelte",
    "url": "https://ai-sdk.dev/docs/getting-started/svelte",
    "html": "Getting Started\nSvelte\nCopy markdown\nSvelte Quickstart\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of Prompt Engineering and HTTP Streaming, you can optionally read these documents first.\n\nPrerequisites\n\nTo follow this quickstart, you'll need:\n\nNode.js 18+ and pnpm installed on your local development machine.\nAn OpenAI API key.\n\nIf you haven't obtained your OpenAI API key, you can do so by signing up\n on the OpenAI website.\n\nSet Up Your Application\n\nStart by creating a new SvelteKit application. This command will create a new directory named my-ai-app and set up a basic SvelteKit application inside it.\n\nnpx sv create my-ai-app\n\nNavigate to the newly created directory:\n\ncd my-ai-app\nInstall Dependencies\n\nInstall ai and @ai-sdk/openai, the AI SDK's OpenAI provider.\n\nThe AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about available providers and building custom providers in the providers section.\n\npnpm\nnpm\nyarn\nbun\npnpm add -D ai @ai-sdk/openai @ai-sdk/svelte zod\nConfigure OpenAI API Key\n\nCreate a .env.local file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.\n\ntouch .env.local\n\nEdit the .env.local file:\n\n.env.local\nOPENAI_API_KEY=xxxxxxxxx\n\nReplace xxxxxxxxx with your actual OpenAI API key.\n\nVite does not automatically load environment variables onto process.env, so you'll need to import OPENAI_API_KEY from $env/static/private in your code (see below).\n\nCreate an API route\n\nCreate a SvelteKit Endpoint, src/routes/api/chat/+server.ts and add the following code:\n\nsrc/routes/api/chat/+server.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText, type UIMessage, convertToModelMessages } from 'ai';\n\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\n\nexport async function POST({ request }) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIf you see type errors with OPENAI_API_KEY or your POST function, run the dev server.\n\nLet's take a look at what is happening in this code:\n\nCreate an OpenAI provider instance with the createOpenAI function from the @ai-sdk/openai package.\nDefine a POST request handler and extract messages from the body of the request. The messages variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The messages are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\nCall streamText, which is imported from the ai package. This function accepts a configuration object that contains a model provider (defined in step 1) and messages (defined in step 2). You can pass additional settings to further customise the model's behaviour. The messages key expects a ModelMessage[] array. This type is different from UIMessage in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the convertToModelMessages function, which strips the UI-specific metadata and transforms the UIMessage[] array into the ModelMessage[] format that the model expects.\nThe streamText function returns a StreamTextResult. This result object contains the toUIMessageStreamResponse function which converts the result to a streamed response object.\nReturn the result to the client to stream the response.\nWire up the UI\n\nNow that you have an API route that can query an LLM, it's time to set up your frontend. The AI SDK's UI package abstracts the complexity of a chat interface into one class, Chat. Its properties and API are largely the same as React's useChat.\n\nUpdate your root page (src/routes/+page.svelte) with the following code to show a list of chat messages and provide a user message input:\n\nsrc/routes/+page.svelte\n<script lang=\"ts\">\n  import { Chat } from '@ai-sdk/svelte';\n\n\n  let input = '';\n  const chat = new Chat({});\n\n\n  function handleSubmit(event: SubmitEvent) {\n    event.preventDefault();\n    chat.sendMessage({ text: input });\n    input = '';\n  }\n</script>\n\n\n<main>\n  <ul>\n    {#each chat.messages as message, messageIndex (messageIndex)}\n      <li>\n        <div>{message.role}</div>\n        <div>\n          {#each message.parts as part, partIndex (partIndex)}\n            {#if part.type === 'text'}\n              <div>{part.text}</div>\n            {/if}\n          {/each}\n        </div>\n      </li>\n    {/each}\n  </ul>\n  <form onsubmit={handleSubmit}>\n    <input bind:value={input} />\n    <button type=\"submit\">Send</button>\n  </form>\n</main>\n\nThis page utilizes the Chat class, which will, by default, use the POST route handler you created earlier. The class provides functions and state for handling user input and form submission. The Chat class provides multiple utility functions and state variables:\n\nmessages - the current chat messages (an array of objects with id, role, and parts properties).\nsendMessage - a function to send a message to the chat API.\n\nThe component uses local state to manage the input field value, and handles form submission by calling sendMessage with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message parts array. Each message contains an ordered array of parts that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The parts array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\nRunning Your Application\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\npnpm run dev\n\nHead to your browser and open http://localhost:5173\n. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Svelte.\n\nEnhance Your Chatbot with Tools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where tools come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\nUpdate Your API Route\n\nModify your src/routes/api/chat/+server.ts file to include the new weather tool:\n\nsrc/routes/api/chat/+server.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport {\n  streamText,\n  type UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\n\nexport async function POST({ request }) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIn this updated code:\n\nYou import the tool function from the ai package and z from zod for schema validation.\n\nYou define a tools object with a weather tool. This tool:\n\nHas a description that helps the model understand when to use it.\nDefines inputSchema using a Zod schema, specifying that it requires a location string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\nDefines an execute function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The execute function will then be automatically run, and the tool output will be added to the messages as a tool message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the tool-weather part of the message.parts array.\n\nTool parts are always named tool-{toolName}, where {toolName} is the key you used when defining the tool. In this case, since we defined the tool as weather, the part type is tool-weather.\n\nUpdate the UI\n\nTo display the tool invocation in your UI, update your src/routes/+page.svelte file:\n\nsrc/routes/+page.svelte\n<script lang=\"ts\">\n  import { Chat } from '@ai-sdk/svelte';\n\n\n  let input = '';\n  const chat = new Chat({});\n\n\n  function handleSubmit(event: SubmitEvent) {\n    event.preventDefault();\n    chat.sendMessage({ text: input });\n    input = '';\n  }\n</script>\n\n\n<main>\n  <ul>\n    {#each chat.messages as message, messageIndex (messageIndex)}\n      <li>\n        <div>{message.role}</div>\n        <div>\n          {#each message.parts as part, partIndex (partIndex)}\n            {#if part.type === 'text'}\n              <div>{part.text}</div>\n            {:else if part.type === 'tool-weather'}\n              <pre>{JSON.stringify(part, null, 2)}</pre>\n            {/if}\n          {/each}\n        </div>\n      </li>\n    {/each}\n  </ul>\n  <form onsubmit={handleSubmit}>\n    <input bind:value={input} />\n    <button type=\"submit\">Send</button>\n  </form>\n</main>\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\nEnabling Multi-Step Tool Calls\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using stopWhen. By default, stopWhen is set to stepCountIs(1), which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\nUpdate Your API Route\n\nModify your src/routes/api/chat/+server.ts file to include the stopWhen condition:\n\nsrc/routes/api/chat/+server.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport {\n  streamText,\n  type UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\n\nexport async function POST({ request }) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting stopWhen: stepCountIs(5), you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\n\nAdd another tool\n\nUpdate your src/routes/api/chat/+server.ts file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nsrc/routes/api/chat/+server.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport {\n  streamText,\n  type UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nimport { OPENAI_API_KEY } from '$env/static/private';\n\n\nconst openai = createOpenAI({\n  apiKey: OPENAI_API_KEY,\n});\n\n\nexport async function POST({ request }) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        inputSchema: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nUpdate Your Frontend\n\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\n\nsrc/routes/+page.svelte\n<script lang=\"ts\">\n  import { Chat } from '@ai-sdk/svelte';\n\n\n  let input = '';\n  const chat = new Chat({});\n\n\n  function handleSubmit(event: SubmitEvent) {\n    event.preventDefault();\n    chat.sendMessage({ text: input });\n    input = '';\n  }\n</script>\n\n\n<main>\n  <ul>\n    {#each chat.messages as message, messageIndex (messageIndex)}\n      <li>\n        <div>{message.role}</div>\n        <div>\n          {#each message.parts as part, partIndex (partIndex)}\n            {#if part.type === 'text'}\n              <div>{part.text}</div>\n            {:else if part.type === 'tool-weather' || part.type === 'tool-convertFahrenheitToCelsius'}\n              <pre>{JSON.stringify(part, null, 2)}</pre>\n            {/if}\n          {/each}\n        </div>\n      </li>\n    {/each}\n  </ul>\n  <form onsubmit={handleSubmit}>\n    <input bind:value={input} />\n    <button type=\"submit\">Send</button>\n  </form>\n</main>\n\nThis update handles the new tool-convertFahrenheitToCelsius part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\nThe model will call the weather tool for New York.\nYou'll see the tool output displayed.\nIt will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\nThe model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\nHow does @ai-sdk/svelte differ from @ai-sdk/react?\n\nThe surface-level difference is that Svelte uses classes to manage state, whereas React uses hooks, so useChat in React is Chat in Svelte. Other than that, there are a few things to keep in mind:\n\n1. Arguments to classes aren't reactive by default\n\nUnlike in React, where hooks are rerun any time their containing component is invalidated, code in the script block of a Svelte component is only run once when the component is created. This means that, if you want arguments to your class to be reactive, you need to make sure you pass a reference into the class, rather than a value:\n\n<script>\n  import { Chat } from '@ai-sdk/svelte';\n\n\n  let { id } = $props();\n\n\n  // won't work; the class instance will be created once, `id` will be copied by value, and won't update when $props.id changes\n  let chat = new Chat({ id });\n\n\n  // will work; passes `id` by reference, so `Chat` always has the latest value\n  let chat = new Chat({\n    get id() {\n      return id;\n    },\n  });\n</script>\n\nKeep in mind that this normally doesn't matter; most parameters you'll pass into the Chat class are static (for example, you typically wouldn't expect your onError handler to change).\n\n2. You can't destructure class properties\n\nIn vanilla JavaScript, destructuring class properties copies them by value and \"disconnects\" them from their class instance:\n\nconst classInstance = new Whatever();\nclassInstance.foo = 'bar';\nconst { foo } = classInstance;\nclassInstance.foo = 'baz';\n\n\nconsole.log(foo); // 'bar'\n\nThe same is true of classes in Svelte:\n\n<script>\n  import { Chat } from '@ai-sdk/svelte';\n\n\n  const chat = new Chat({});\n  let { messages } = chat;\n\n\n  chat.append({ content: 'Hello, world!', role: 'user' }).then(() => {\n    console.log(messages); // []\n    console.log(chat.messages); // [{ content: 'Hello, world!', role: 'user' }] (plus some other stuff)\n  });\n</script>\n3. Instance synchronization requires context\n\nIn React, hook instances with the same id are synchronized -- so two instances of useChat will have the same messages, status, etc. if they have the same id. For most use cases, you probably don't need this behavior -- but if you do, you can create a context in your root layout file using createAIContext:\n\n<script>\n  import { createAIContext } from '@ai-sdk/svelte';\n\n\n  let { children } = $props();\n\n\n  createAIContext();\n  // all hooks created after this or in components that are children of this component\n  // will have synchronized state\n</script>\n\n\n{@render children()}\nWhere to Next?\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\nTo learn more about the AI SDK, read through the documentation.\nIf you're interested in diving deeper with guides, check out the RAG (retrieval-augmented generation) and multi-modal chatbot guides.\nTo jumpstart your first AI project, explore available templates\n.\nTo learn more about Svelte, check out the official documentation\n.\nPrevious\nNext.js Pages Router\nNext\nVue.js (Nuxt)"
  },
  {
    "title": "Getting Started: Vue.js (Nuxt)",
    "url": "https://ai-sdk.dev/docs/getting-started/nuxt",
    "html": "Getting Started\nVue.js (Nuxt)\nCopy markdown\nVue.js (Nuxt) Quickstart\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of Prompt Engineering and HTTP Streaming, you can optionally read these documents first.\n\nPrerequisites\n\nTo follow this quickstart, you'll need:\n\nNode.js 18+ and pnpm installed on your local development machine.\nAn OpenAI API key.\n\nIf you haven't obtained your OpenAI API key, you can do so by signing up\n on the OpenAI website.\n\nSetup Your Application\n\nStart by creating a new Nuxt application. This command will create a new directory named my-ai-app and set up a basic Nuxt application inside it.\n\npnpm create nuxt my-ai-app\n\nNavigate to the newly created directory:\n\ncd my-ai-app\nInstall dependencies\n\nInstall ai and @ai-sdk/openai, the AI SDK's OpenAI provider.\n\nThe AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about available providers and building custom providers in the providers section.\n\npnpm\nnpm\nyarn\nbun\npnpm add ai @ai-sdk/openai @ai-sdk/vue zod\nConfigure OpenAI API key\n\nCreate a .env file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.\n\ntouch .env\n\nEdit the .env file:\n\n.env\nNUXT_OPENAI_API_KEY=xxxxxxxxx\n\nReplace xxxxxxxxx with your actual OpenAI API key and configure the environment variable in nuxt.config.ts:\n\nnuxt.config.ts\nexport default defineNuxtConfig({\n  // rest of your nuxt config\n  runtimeConfig: {\n    openaiApiKey: '',\n  },\n});\n\nThe AI SDK's OpenAI Provider will default to using the OPENAI_API_KEY environment variable.\n\nCreate an API route\n\nCreate an API route, server/api/chat.ts and add the following code:\n\nserver/api/chat.ts\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\n\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n\n  return defineEventHandler(async (event: any) => {\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages: convertToModelMessages(messages),\n    });\n\n\n    return result.toUIMessageStreamResponse();\n  });\n});\n\nLet's take a look at what is happening in this code:\n\nCreate an OpenAI provider instance with the createOpenAI function from the @ai-sdk/openai package.\nDefine an Event Handler and extract messages from the body of the request. The messages variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The messages are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\nCall streamText, which is imported from the ai package. This function accepts a configuration object that contains a model provider (defined in step 1) and messages (defined in step 2). You can pass additional settings to further customise the model's behaviour. The messages key expects a ModelMessage[] array. This type is different from UIMessage in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the convertToModelMessages function, which strips the UI-specific metadata and transforms the UIMessage[] array into the ModelMessage[] format that the model expects.\nThe streamText function returns a StreamTextResult. This result object contains the toDataStreamResponse function which converts the result to a streamed response object.\nReturn the result to the client to stream the response.\nWire up the UI\n\nNow that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's UI package abstract the complexity of a chat interface into one hook, useChat.\n\nUpdate your root page (pages/index.vue) with the following code to show a list of chat messages and provide a user message input:\n\npages/index.vue\n<script setup lang=\"ts\">\nimport { Chat } from \"@ai-sdk/vue\";\nimport { ref } from \"vue\";\n\n\nconst input = ref(\"\");\nconst chat = new Chat({});\n\n\nconst handleSubmit = (e: Event) => {\n    e.preventDefault();\n    chat.sendMessage({ text: input.value });\n    input.value = \"\";\n};\n</script>\n\n\n<template>\n    <div>\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\n            <div\n                v-for=\"(part, index) in m.parts\"\n                :key=\"`${m.id}-${part.type}-${index}`\"\n            >\n                <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n            </div>\n        </div>\n\n\n        <form @submit=\"handleSubmit\">\n            <input v-model=\"input\" placeholder=\"Say something...\" />\n        </form>\n    </div>\n</template>\n\nIf your project has app.vue instead of pages/index.vue, delete the app.vue file and create a new pages/index.vue file with the code above.\n\nThis page utilizes the useChat hook, which will, by default, use the API route you created earlier (/api/chat). The hook provides functions and state for handling user input and form submission. The useChat hook provides multiple utility functions and state variables:\n\nmessages - the current chat messages (an array of objects with id, role, and parts properties).\nsendMessage - a function to send a message to the chat API.\n\nThe component uses local state (ref) to manage the input field value, and handles form submission by calling sendMessage with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message parts array. Each message contains an ordered array of parts that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The parts array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\nRunning Your Application\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\npnpm run dev\n\nHead to your browser and open http://localhost:3000\n. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Nuxt.\n\nEnhance Your Chatbot with Tools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where tools come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\nUpdate Your API Route\n\nModify your server/api/chat.ts file to include the new weather tool:\n\nserver/api/chat.ts\nimport { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n\n  return defineEventHandler(async (event: any) => {\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages: convertToModelMessages(messages),\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n      },\n    });\n\n\n    return result.toUIMessageStreamResponse();\n  });\n});\n\nIn this updated code:\n\nYou import the tool function from the ai package and z from zod for schema validation.\n\nYou define a tools object with a weather tool. This tool:\n\nHas a description that helps the model understand when to use it.\nDefines inputSchema using a Zod schema, specifying that it requires a location string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\nDefines an execute function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The execute function will then be automatically run, and the tool output will be added to the messages as a tool message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the tool-weather part of the message.parts array.\n\nTool parts are always named tool-{toolName}, where {toolName} is the key you used when defining the tool. In this case, since we defined the tool as weather, the part type is tool-weather.\n\nUpdate the UI\n\nTo display the tool invocation in your UI, update your pages/index.vue file:\n\npages/index.vue\n<script setup lang=\"ts\">\nimport { Chat } from \"@ai-sdk/vue\";\nimport { ref } from \"vue\";\n\n\nconst input = ref(\"\");\nconst chat = new Chat({});\n\n\nconst handleSubmit = (e: Event) => {\n    e.preventDefault();\n    chat.sendMessage({ text: input.value });\n    input.value = \"\";\n};\n</script>\n\n\n<template>\n    <div>\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\n            <div\n                v-for=\"(part, index) in m.parts\"\n                :key=\"`${m.id}-${part.type}-${index}`\"\n            >\n                <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n                <pre v-if=\"part.type === 'tool-weather'\">{{ JSON.stringify(part, null, 2) }}</pre>\n            </div>\n        </div>\n\n\n        <form @submit=\"handleSubmit\">\n            <input v-model=\"input\" placeholder=\"Say something...\" />\n        </form>\n    </div>\n</template>\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\nEnabling Multi-Step Tool Calls\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using stopWhen. By default, stopWhen is set to stepCountIs(1), which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\nUpdate Your API Route\n\nModify your server/api/chat.ts file to include the stopWhen condition:\n\nserver/api/chat.ts\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n\n  return defineEventHandler(async (event: any) => {\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages: convertToModelMessages(messages),\n      stopWhen: stepCountIs(5),\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n      },\n    });\n\n\n    return result.toUIMessageStreamResponse();\n  });\n});\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting stopWhen: stepCountIs(5), you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\n\nAdd another tool\n\nUpdate your server/api/chat.ts file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nserver/api/chat.ts\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { z } from 'zod';\n\n\nexport default defineLazyEventHandler(async () => {\n  const apiKey = useRuntimeConfig().openaiApiKey;\n  if (!apiKey) throw new Error('Missing OpenAI API key');\n  const openai = createOpenAI({\n    apiKey: apiKey,\n  });\n\n\n  return defineEventHandler(async (event: any) => {\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages: convertToModelMessages(messages),\n      stopWhen: stepCountIs(5),\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n        convertFahrenheitToCelsius: tool({\n          description: 'Convert a temperature in fahrenheit to celsius',\n          inputSchema: z.object({\n            temperature: z\n              .number()\n              .describe('The temperature in fahrenheit to convert'),\n          }),\n          execute: async ({ temperature }) => {\n            const celsius = Math.round((temperature - 32) * (5 / 9));\n            return {\n              celsius,\n            };\n          },\n        }),\n      },\n    });\n\n\n    return result.toUIMessageStreamResponse();\n  });\n});\nUpdate Your Frontend\n\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\n\npages/index.vue\n<script setup lang=\"ts\">\nimport { Chat } from \"@ai-sdk/vue\";\nimport { ref } from \"vue\";\n\n\nconst input = ref(\"\");\nconst chat = new Chat({});\n\n\nconst handleSubmit = (e: Event) => {\n    e.preventDefault();\n    chat.sendMessage({ text: input.value });\n    input.value = \"\";\n};\n</script>\n\n\n<template>\n    <div>\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\n            <div\n                v-for=\"(part, index) in m.parts\"\n                :key=\"`${m.id}-${part.type}-${index}`\"\n            >\n                <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n                <pre\n                    v-if=\"\n                        part.type === 'tool-weather' ||\n                        part.type === 'tool-convertFahrenheitToCelsius'\n                    \"\n                    >{{ JSON.stringify(part, null, 2) }}</pre\n                >\n            </div>\n        </div>\n\n\n        <form @submit=\"handleSubmit\">\n            <input v-model=\"input\" placeholder=\"Say something...\" />\n        </form>\n    </div>\n</template>\n\nThis update handles the new tool-convertFahrenheitToCelsius part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\nThe model will call the weather tool for New York.\nYou'll see the tool output displayed.\nIt will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\nThe model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\nWhere to Next?\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\nTo learn more about the AI SDK, read through the documentation.\nIf you're interested in diving deeper with guides, check out the RAG (retrieval-augmented generation) and multi-modal chatbot guides.\nTo jumpstart your first AI project, explore available templates\n.\nPrevious\nSvelte\nNext\nNode.js"
  },
  {
    "title": "Getting Started: Node.js",
    "url": "https://ai-sdk.dev/docs/getting-started/nodejs",
    "html": "Getting Started\nNode.js\nCopy markdown\nNode.js Quickstart\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of Prompt Engineering and HTTP Streaming, you can optionally read these documents first.\n\nPrerequisites\n\nTo follow this quickstart, you'll need:\n\nNode.js 18+ and pnpm installed on your local development machine.\nAn OpenAI API key.\n\nIf you haven't obtained your OpenAI API key, you can do so by signing up\n on the OpenAI website.\n\nSetup Your Application\n\nStart by creating a new directory using the mkdir command. Change into your new directory and then run the pnpm init command. This will create a package.json in your new directory.\n\nmkdir my-ai-app\ncd my-ai-app\npnpm init\nInstall Dependencies\n\nInstall ai and @ai-sdk/openai, the AI SDK's OpenAI provider, along with other necessary dependencies.\n\nThe AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about available providers and building custom providers in the providers section.\n\npnpm add ai@beta @ai-sdk/openai@beta zod dotenv\npnpm add -D @types/node tsx typescript\n\nThe ai and @ai-sdk/openai packages contain the AI SDK and the AI SDK OpenAI provider, respectively. You will use zod to define type-safe schemas that you will pass to the large language model (LLM). You will use dotenv to access environment variables (your OpenAI key) within your application. There are also three development dependencies, installed with the -D flag, that are necessary to run your Typescript code.\n\nConfigure OpenAI API key\n\nCreate a .env file in your project's root directory and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.\n\ntouch .env\n\nEdit the .env file:\n\n.env\nOPENAI_API_KEY=xxxxxxxxx\n\nReplace xxxxxxxxx with your actual OpenAI API key.\n\nThe AI SDK's OpenAI Provider will default to using the OPENAI_API_KEY environment variable.\n\nCreate Your Application\n\nCreate an index.ts file in the root of your project and add the following code:\n\nindex.ts\nimport { openai } from '@ai-sdk/openai';\nimport { ModelMessage, streamText } from 'ai';\nimport 'dotenv/config';\nimport * as readline from 'node:readline/promises';\n\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\n\nconst messages: ModelMessage[] = [];\n\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n\n    messages.push({ role: 'user', content: userInput });\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n    });\n\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\n\nmain().catch(console.error);\n\nLet's take a look at what is happening in this code:\n\nSet up a readline interface to take input from the terminal, enabling interactive sessions directly from the command line.\nInitialize an array called messages to store the history of your conversation. This history allows the model to maintain context in ongoing dialogues.\nIn the main function:\nPrompt for and capture user input, storing it in userInput.\nAdd user input to the messages array as a user message.\nCall streamText, which is imported from the ai package. This function accepts a configuration object that contains a model provider and messages.\nIterate over the text stream returned by the streamText function (result.textStream) and print the contents of the stream to the terminal.\nAdd the assistant's response to the messages array.\nRunning Your Application\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\npnpm tsx index.ts\n\nYou should see a prompt in your terminal. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Node.js.\n\nEnhance Your Chatbot with Tools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where tools come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\nUpdate Your Application\n\nModify your index.ts file to include the new weather tool:\n\nindex.ts\nimport { openai } from '@ai-sdk/openai';\nimport { ModelMessage, streamText, tool } from 'ai';\nimport 'dotenv/config';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\n\nconst messages: ModelMessage[] = [];\n\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n\n    messages.push({ role: 'user', content: userInput });\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n      },\n    });\n\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\n\nmain().catch(console.error);\n\nIn this updated code:\n\nYou import the tool function from the ai package.\n\nYou define a tools object with a weather tool. This tool:\n\nHas a description that helps the model understand when to use it.\nDefines inputSchema using a Zod schema, specifying that it requires a location string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\nDefines an execute function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The execute function will then be automatically run, and the results will be used by the model to generate its response.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank \"assistant\" response? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result in the toolCall and toolResult keys of the result object.\n\nimport { openai } from '@ai-sdk/openai';\nimport { ModelMessage, streamText, tool } from 'ai';\nimport 'dotenv/config';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\n\nconst messages: ModelMessage[] = [];\n\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n\n    messages.push({ role: 'user', content: userInput });\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n      },\n    });\n\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n\n    console.log(await result.toolCalls);\n    console.log(await result.toolResults);\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\n\nmain().catch(console.error);\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\nEnabling Multi-Step Tool Calls\n\nYou may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using stopWhen. This feature will automatically send tool results back to the model to trigger an additional generation until the stopping condition you define is met. In this case, you want the model to answer your question using the results from the weather tool.\n\nUpdate Your Application\n\nModify your index.ts file to configure stopping conditions with stopWhen:\n\nindex.ts\nimport { openai } from '@ai-sdk/openai';\nimport { ModelMessage, streamText, tool, stepCountIs } from 'ai';\nimport 'dotenv/config';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\n\nconst messages: ModelMessage[] = [];\n\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n\n    messages.push({ role: 'user', content: userInput });\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n      },\n      stopWhen: stepCountIs(5),\n      onStepFinish: async ({ toolResults }) => {\n        if (toolResults.length) {\n          console.log(JSON.stringify(toolResults, null, 2));\n        }\n      },\n    });\n\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\n\nmain().catch(console.error);\n\nIn this updated code:\n\nYou set stopWhen to be when stepCountIs 5, allowing the model to use up to 5 \"steps\" for any given generation.\nYou add an onStepFinish callback to log any toolResults from each step of the interaction, helping you understand the model's tool usage. This means we can also delete the toolCall and toolResult console.log statements from the previous example.\n\nNow, when you ask about the weather in a location, you should see the model using the weather tool results to answer your question.\n\nBy setting stopWhen: stepCountIs(5), you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\nAdding a second tool\n\nUpdate your index.ts file to add a new tool to convert the temperature from Celsius to Fahrenheit:\n\nindex.ts\nimport { openai } from '@ai-sdk/openai';\nimport { ModelMessage, streamText, tool, stepCountIs } from 'ai';\nimport 'dotenv/config';\nimport { z } from 'zod';\nimport * as readline from 'node:readline/promises';\n\n\nconst terminal = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\n\nconst messages: ModelMessage[] = [];\n\n\nasync function main() {\n  while (true) {\n    const userInput = await terminal.question('You: ');\n\n\n    messages.push({ role: 'user', content: userInput });\n\n\n    const result = streamText({\n      model: openai('gpt-4o'),\n      messages,\n      tools: {\n        weather: tool({\n          description: 'Get the weather in a location (fahrenheit)',\n          inputSchema: z.object({\n            location: z\n              .string()\n              .describe('The location to get the weather for'),\n          }),\n          execute: async ({ location }) => {\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n            return {\n              location,\n              temperature,\n            };\n          },\n        }),\n        convertFahrenheitToCelsius: tool({\n          description: 'Convert a temperature in fahrenheit to celsius',\n          inputSchema: z.object({\n            temperature: z\n              .number()\n              .describe('The temperature in fahrenheit to convert'),\n          }),\n          execute: async ({ temperature }) => {\n            const celsius = Math.round((temperature - 32) * (5 / 9));\n            return {\n              celsius,\n            };\n          },\n        }),\n      },\n      stopWhen: stepCountIs(5),\n      onStepFinish: async ({ toolResults }) => {\n        if (toolResults.length) {\n          console.log(JSON.stringify(toolResults, null, 2));\n        }\n      },\n    });\n\n\n    let fullResponse = '';\n    process.stdout.write('\\nAssistant: ');\n    for await (const delta of result.textStream) {\n      fullResponse += delta;\n      process.stdout.write(delta);\n    }\n    process.stdout.write('\\n\\n');\n\n\n    messages.push({ role: 'assistant', content: fullResponse });\n  }\n}\n\n\nmain().catch(console.error);\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\nThe model will call the weather tool for New York.\nYou'll see the tool result logged.\nIt will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\nThe model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time and perform actions that interact with the outside world. Tools bridge the gap between the model's knowledge cutoff and current information, while also enabling it to take meaningful actions beyond just generating text responses.\n\nWhere to Next?\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\nTo learn more about the AI SDK, read through the documentation.\nIf you're interested in diving deeper with guides, check out the RAG (retrieval-augmented generation) and multi-modal chatbot guides.\nTo jumpstart your first AI project, explore available templates\n.\nPrevious\nVue.js (Nuxt)\nNext\nExpo"
  },
  {
    "title": "Getting Started: Expo",
    "url": "https://ai-sdk.dev/docs/getting-started/expo",
    "html": "Getting Started\nExpo\nCopy markdown\nExpo Quickstart\n\nIn this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface with Expo\n. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of Prompt Engineering and HTTP Streaming, you can optionally read these documents first.\n\nPrerequisites\n\nTo follow this quickstart, you'll need:\n\nNode.js 18+ and pnpm installed on your local development machine.\nAn OpenAI API key.\n\nIf you haven't obtained your OpenAI API key, you can do so by signing up\n on the OpenAI website.\n\nCreate Your Application\n\nStart by creating a new Expo application. This command will create a new directory named my-ai-app and set up a basic Expo application inside it.\n\npnpm create expo-app@latest my-ai-app\n\nNavigate to the newly created directory:\n\ncd my-ai-app\nThis guide requires Expo 52 or higher.\nInstall dependencies\n\nInstall ai, @ai-sdk/react and @ai-sdk/openai, the AI package, the AI React package and AI SDK's OpenAI provider respectively.\n\nThe AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about available providers and building custom providers in the providers section.\n\npnpm\nnpm\nyarn\nbun\npnpm add ai @ai-sdk/openai @ai-sdk/react zod\nConfigure OpenAI API key\n\nCreate a .env.local file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.\n\ntouch .env.local\n\nEdit the .env.local file:\n\n.env.local\nOPENAI_API_KEY=xxxxxxxxx\n\nReplace xxxxxxxxx with your actual OpenAI API key.\n\nThe AI SDK's OpenAI Provider will default to using the OPENAI_API_KEY environment variable.\n\nCreate an API Route\n\nCreate a route handler, app/api/chat+api.ts and add the following code:\n\napp/api/chat+api.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n\nLet's take a look at what is happening in this code:\n\nDefine an asynchronous POST request handler and extract messages from the body of the request. The messages variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.\nCall streamText, which is imported from the ai package. This function accepts a configuration object that contains a model provider (imported from @ai-sdk/openai) and messages (defined in step 1). You can pass additional settings to further customise the model's behaviour.\nThe streamText function returns a StreamTextResult. This result object contains the toDataStreamResponse function which converts the result to a streamed response object.\nFinally, return the result to the client to stream the response.\n\nThis API route creates a POST request endpoint at /api/chat.\n\nWire up the UI\n\nNow that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's UI package abstracts the complexity of a chat interface into one hook, useChat.\n\nUpdate your root page (app/(tabs)/index.tsx) with the following code to show a list of chat messages and provide a user message input:\n\napp/(tabs)/index.tsx\nimport { generateAPIUrl } from '@/utils';\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { fetch as expoFetch } from 'expo/fetch';\nimport { useState } from 'react';\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\n\nexport default function App() {\n  const [input, setInput] = useState('');\n  const { messages, error, sendMessage } = useChat({\n    transport: new DefaultChatTransport({\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\n      api: generateAPIUrl('/api/chat'),\n    }),\n    onError: error => console.error(error, 'ERROR'),\n  });\n\n\n  if (error) return <Text>{error.message}</Text>;\n\n\n  return (\n    <SafeAreaView style={{ height: '100%' }}>\n      <View\n        style={{\n          height: '95%',\n          display: 'flex',\n          flexDirection: 'column',\n          paddingHorizontal: 8,\n        }}\n      >\n        <ScrollView style={{ flex: 1 }}>\n          {messages.map(m => (\n            <View key={m.id} style={{ marginVertical: 8 }}>\n              <View>\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n                {m.parts.map((part, i) => {\n                  switch (part.type) {\n                    case 'text':\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\n                  }\n                })}\n              </View>\n            </View>\n          ))}\n        </ScrollView>\n\n\n        <View style={{ marginTop: 8 }}>\n          <TextInput\n            style={{ backgroundColor: 'white', padding: 8 }}\n            placeholder=\"Say something...\"\n            value={input}\n            onChange={e => setInput(e.nativeEvent.text)}\n            onSubmitEditing={e => {\n              e.preventDefault();\n              sendMessage({ text: input });\n              setInput('');\n            }}\n            autoFocus={true}\n          />\n        </View>\n      </View>\n    </SafeAreaView>\n  );\n}\n\nThis page utilizes the useChat hook, which will, by default, use the POST API route you created earlier (/api/chat). The hook provides functions and state for handling user input and form submission. The useChat hook provides multiple utility functions and state variables:\n\nmessages - the current chat messages (an array of objects with id, role, and parts properties).\nsendMessage - a function to send a message to the chat API.\n\nThe component uses local state (useState) to manage the input field value, and handles form submission by calling sendMessage with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message parts array. Each message contains an ordered array of parts that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The parts array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\nYou use the expo/fetch function instead of the native node fetch to enable streaming of chat responses. This requires Expo 52 or higher.\n\nCreate the API URL Generator\n\nBecause you're using expo/fetch for streaming responses instead of the native fetch function, you'll need an API URL generator to ensure you are using the correct base url and format depending on the client environment (e.g. web or mobile). Create a new file called utils.ts in the root of your project and add the following code:\n\nutils.ts\nimport Constants from 'expo-constants';\n\n\nexport const generateAPIUrl = (relativePath: string) => {\n  const origin = Constants.experienceUrl.replace('exp://', 'http://');\n\n\n  const path = relativePath.startsWith('/') ? relativePath : `/${relativePath}`;\n\n\n  if (process.env.NODE_ENV === 'development') {\n    return origin.concat(path);\n  }\n\n\n  if (!process.env.EXPO_PUBLIC_API_BASE_URL) {\n    throw new Error(\n      'EXPO_PUBLIC_API_BASE_URL environment variable is not defined',\n    );\n  }\n\n\n  return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);\n};\n\nThis utility function handles URL generation for both development and production environments, ensuring your API calls work correctly across different devices and configurations.\n\nBefore deploying to production, you must set the EXPO_PUBLIC_API_BASE_URL environment variable in your production environment. This variable should point to the base URL of your API server.\n\nRunning Your Application\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\npnpm expo\n\nHead to your browser and open http://localhost:8081\n. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Expo.\n\nIf you experience \"Property structuredClone doesn't exist\" errors on mobile, add the polyfills described below.\n\nEnhance Your Chatbot with Tools\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where tools come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\nUpdate Your API route\n\nModify your app/api/chat+api.ts file to include the new weather tool:\n\napp/api/chat+api.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n\nIn this updated code:\n\nYou import the tool function from the ai package and z from zod for schema validation.\n\nYou define a tools object with a weather tool. This tool:\n\nHas a description that helps the model understand when to use it.\nDefines inputSchema using a Zod schema, specifying that it requires a location string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\nDefines an execute function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The execute function will then be automatically run, and the tool output will be added to the messages as a tool message.\n\nYou may need to restart your development server for the changes to take effect.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the tool-weather part of the message.parts array.\n\nTool parts are always named tool-{toolName}, where {toolName} is the key you used when defining the tool. In this case, since we defined the tool as weather, the part type is tool-weather.\n\nUpdate the UI\n\nTo display the weather tool invocation in your UI, update your app/(tabs)/index.tsx file:\n\napp/(tabs)/index.tsx\nimport { generateAPIUrl } from '@/utils';\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { fetch as expoFetch } from 'expo/fetch';\nimport { useState } from 'react';\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\n\nexport default function App() {\n  const [input, setInput] = useState('');\n  const { messages, error, sendMessage } = useChat({\n    transport: new DefaultChatTransport({\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\n      api: generateAPIUrl('/api/chat'),\n    }),\n    onError: error => console.error(error, 'ERROR'),\n  });\n\n\n  if (error) return <Text>{error.message}</Text>;\n\n\n  return (\n    <SafeAreaView style={{ height: '100%' }}>\n      <View\n        style={{\n          height: '95%',\n          display: 'flex',\n          flexDirection: 'column',\n          paddingHorizontal: 8,\n        }}\n      >\n        <ScrollView style={{ flex: 1 }}>\n          {messages.map(m => (\n            <View key={m.id} style={{ marginVertical: 8 }}>\n              <View>\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n                {m.parts.map((part, i) => {\n                  switch (part.type) {\n                    case 'text':\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\n                    case 'tool-weather':\n                      return (\n                        <Text key={`${m.id}-${i}`}>\n                          {JSON.stringify(part, null, 2)}\n                        </Text>\n                      );\n                  }\n                })}\n              </View>\n            </View>\n          ))}\n        </ScrollView>\n\n\n        <View style={{ marginTop: 8 }}>\n          <TextInput\n            style={{ backgroundColor: 'white', padding: 8 }}\n            placeholder=\"Say something...\"\n            value={input}\n            onChange={e => setInput(e.nativeEvent.text)}\n            onSubmitEditing={e => {\n              e.preventDefault();\n              sendMessage({ text: input });\n              setInput('');\n            }}\n            autoFocus={true}\n          />\n        </View>\n      </View>\n    </SafeAreaView>\n  );\n}\n\nYou may need to restart your development server for the changes to take effect.\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\nEnabling Multi-Step Tool Calls\n\nYou may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using stopWhen. By default, stopWhen is set to stepCountIs(1), which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\nUpdate Your API Route\n\nModify your app/api/chat+api.ts file to include the stopWhen condition:\n\napp/api/chat+api.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n\nYou may need to restart your development server for the changes to take effect.\n\nHead back to the Expo app and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting stopWhen: stepCountIs(5), you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\n\nAdd More Tools\n\nUpdate your app/api/chat+api.ts file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\napp/api/chat+api.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  streamText,\n  UIMessage,\n  convertToModelMessages,\n  tool,\n  stepCountIs,\n} from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location (fahrenheit)',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }) => {\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n          return {\n            location,\n            temperature,\n          };\n        },\n      }),\n      convertFahrenheitToCelsius: tool({\n        description: 'Convert a temperature in fahrenheit to celsius',\n        inputSchema: z.object({\n          temperature: z\n            .number()\n            .describe('The temperature in fahrenheit to convert'),\n        }),\n        execute: async ({ temperature }) => {\n          const celsius = Math.round((temperature - 32) * (5 / 9));\n          return {\n            celsius,\n          };\n        },\n      }),\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    headers: {\n      'Content-Type': 'application/octet-stream',\n      'Content-Encoding': 'none',\n    },\n  });\n}\n\nYou may need to restart your development server for the changes to take effect.\n\nUpdate the UI for the new tool\n\nTo display the temperature conversion tool invocation in your UI, update your app/(tabs)/index.tsx file to handle the new tool part:\n\napp/(tabs)/index.tsx\nimport { generateAPIUrl } from '@/utils';\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { fetch as expoFetch } from 'expo/fetch';\nimport { useState } from 'react';\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\n\nexport default function App() {\n  const [input, setInput] = useState('');\n  const { messages, error, sendMessage } = useChat({\n    transport: new DefaultChatTransport({\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\n      api: generateAPIUrl('/api/chat'),\n    }),\n    onError: error => console.error(error, 'ERROR'),\n  });\n\n\n  if (error) return <Text>{error.message}</Text>;\n\n\n  return (\n    <SafeAreaView style={{ height: '100%' }}>\n      <View\n        style={{\n          height: '95%',\n          display: 'flex',\n          flexDirection: 'column',\n          paddingHorizontal: 8,\n        }}\n      >\n        <ScrollView style={{ flex: 1 }}>\n          {messages.map(m => (\n            <View key={m.id} style={{ marginVertical: 8 }}>\n              <View>\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n                {m.parts.map((part, i) => {\n                  switch (part.type) {\n                    case 'text':\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\n                    case 'tool-weather':\n                    case 'tool-convertFahrenheitToCelsius':\n                      return (\n                        <Text key={`${m.id}-${i}`}>\n                          {JSON.stringify(part, null, 2)}\n                        </Text>\n                      );\n                  }\n                })}\n              </View>\n            </View>\n          ))}\n        </ScrollView>\n\n\n        <View style={{ marginTop: 8 }}>\n          <TextInput\n            style={{ backgroundColor: 'white', padding: 8 }}\n            placeholder=\"Say something...\"\n            value={input}\n            onChange={e => setInput(e.nativeEvent.text)}\n            onSubmitEditing={e => {\n              e.preventDefault();\n              sendMessage({ text: input });\n              setInput('');\n            }}\n            autoFocus={true}\n          />\n        </View>\n      </View>\n    </SafeAreaView>\n  );\n}\n\nYou may need to restart your development server for the changes to take effect.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\nThe model will call the weather tool for New York.\nYou'll see the tool result displayed.\nIt will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\nThe model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\nPolyfills\n\nSeveral functions that are internally used by the AI SDK might not available in the Expo runtime depending on your configuration and the target platform.\n\nFirst, install the following packages:\n\npnpm\nnpm\nyarn\nbun\npnpm add @ungap/structured-clone @stardazed/streams-text-encoding\n\nThen create a new file in the root of your project with the following polyfills:\n\npolyfills.js\nimport { Platform } from 'react-native';\nimport structuredClone from '@ungap/structured-clone';\n\n\nif (Platform.OS !== 'web') {\n  const setupPolyfills = async () => {\n    const { polyfillGlobal } = await import(\n      'react-native/Libraries/Utilities/PolyfillFunctions'\n    );\n\n\n    const { TextEncoderStream, TextDecoderStream } = await import(\n      '@stardazed/streams-text-encoding'\n    );\n\n\n    if (!('structuredClone' in global)) {\n      polyfillGlobal('structuredClone', () => structuredClone);\n    }\n\n\n    polyfillGlobal('TextEncoderStream', () => TextEncoderStream);\n    polyfillGlobal('TextDecoderStream', () => TextDecoderStream);\n  };\n\n\n  setupPolyfills();\n}\n\n\nexport {};\n\nFinally, import the polyfills in your root _layout.tsx:\n\n_layout.tsx\nimport '@/polyfills';\nWhere to Next?\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\nTo learn more about the AI SDK, read through the documentation.\nIf you're interested in diving deeper with guides, check out the RAG (retrieval-augmented generation) and multi-modal chatbot guides.\nTo jumpstart your first AI project, explore available templates\n.\nPrevious\nNode.js\nNext\nAgents"
  },
  {
    "title": "Agents: Agents",
    "url": "https://ai-sdk.dev/docs/agents/overview",
    "html": "Agents\nOverview\nCopy markdown\nAgents\n\nAgents are large language models (LLMs) that use tools in a loop to accomplish tasks.\n\nThese components work together:\n\nLLMs process input and decide the next action\nTools extend capabilities beyond text generation (reading files, calling APIs, writing to databases)\nLoop orchestrates execution through:\nContext management - Maintaining conversation history and deciding what the model sees (input) at each step\nStopping conditions - Determining when the loop (task) is complete\nAgent Class\n\nThe Agent class handles these three components. Here's an agent that uses multiple tools in a loop to accomplish a task:\n\nimport { Experimental_Agent as Agent, stepCountIs, tool } from 'ai';\nimport { z } from 'zod';\n\n\nconst weatherAgent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location (in Fahrenheit)',\n      inputSchema: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n    convertFahrenheitToCelsius: tool({\n      description: 'Convert temperature from Fahrenheit to Celsius',\n      inputSchema: z.object({\n        temperature: z.number().describe('Temperature in Fahrenheit'),\n      }),\n      execute: async ({ temperature }) => {\n        const celsius = Math.round((temperature - 32) * (5 / 9));\n        return { celsius };\n      },\n    }),\n  },\n  stopWhen: stepCountIs(20),\n});\n\n\nconst result = await weatherAgent.generate({\n  prompt: 'What is the weather in San Francisco in celsius?',\n});\n\n\nconsole.log(result.text); // agent's final answer\nconsole.log(result.steps); // steps taken by the agent\n\nThe agent automatically:\n\nCalls the weather tool to get the temperature in Fahrenheit\nCalls convertFahrenheitToCelsius to convert it\nGenerates a final text response with the result\n\nThe Agent class handles the loop, context management, and stopping conditions.\n\nWhy Use the Agent Class?\n\nThe Agent class is the recommended approach for building agents with the AI SDK because it:\n\nReduces boilerplate - Manages loops and message arrays\nImproves reusability - Define once, use throughout your application\nSimplifies maintenance - Single place to update agent configuration\n\nFor most use cases, start with the Agent class. Use core functions (generateText, streamText) when you need explicit control over each step for complex structured workflows.\n\nStructured Workflows\n\nAgents are flexible and powerful, but non-deterministic. When you need reliable, repeatable outcomes with explicit control flow, use core functions with structured workflow patterns combining:\n\nConditional statements for explicit branching\nStandard functions for reusable logic\nError handling for robustness\nExplicit control flow for predictability\n\nExplore workflow patterns to learn more about building structured, reliable systems.\n\nNext Steps\nBuilding Agents - Guide to creating agents with the Agent class\nWorkflow Patterns - Structured patterns using core functions for complex workflows\nLoop Control - Execution control with stopWhen and prepareStep\nPrevious\nAgents\nNext\nBuilding Agents"
  },
  {
    "title": "Agents: Building Agents",
    "url": "https://ai-sdk.dev/docs/agents/building-agents",
    "html": "Agents\nBuilding Agents\nCopy markdown\nBuilding Agents\n\nThe Agent class provides a structured way to encapsulate LLM configuration, tools, and behavior into reusable components. It handles the agent loop for you, allowing the LLM to call tools multiple times in sequence to accomplish complex tasks. Define agents once and use them across your application.\n\nWhy Use the Agent Class?\n\nWhen building AI applications, you often need to:\n\nReuse configurations - Same model settings, tools, and prompts across different parts of your application\nMaintain consistency - Ensure the same behavior and capabilities throughout your codebase\nSimplify API routes - Reduce boilerplate in your endpoints\nType safety - Get full TypeScript support for your agent's tools and outputs\n\nThe Agent class provides a single place to define your agent's behavior.\n\nCreating an Agent\n\nDefine an agent by instantiating the Agent class with your desired configuration:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst myAgent = new Agent({\n  model: 'openai/gpt-4o',\n  system: 'You are a helpful assistant.',\n  tools: {\n    // Your tools here\n  },\n});\nConfiguration Options\n\nThe Agent class accepts all the same settings as generateText and streamText. Configure:\n\nModel and System Prompt\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  system: 'You are an expert software engineer.',\n});\nTools\n\nProvide tools that the agent can use to accomplish tasks:\n\nimport { Experimental_Agent as Agent, tool } from 'ai';\nimport { z } from 'zod';\n\n\nconst codeAgent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    runCode: tool({\n      description: 'Execute Python code',\n      inputSchema: z.object({\n        code: z.string(),\n      }),\n      execute: async ({ code }) => {\n        // Execute code and return result\n        return { output: 'Code executed successfully' };\n      },\n    }),\n  },\n});\nLoop Control\n\nBy default, agents run for a single step (stopWhen: stepCountIs(1)). In each step, the model either generates text or calls a tool. If it generates text, the agent completes. If it calls a tool, the AI SDK executes that tool.\n\nTo let agents call multiple tools in sequence, configure stopWhen to allow more steps. After each tool execution, the agent triggers a new generation where the model can call another tool or generate text:\n\nimport { Experimental_Agent as Agent, stepCountIs } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  stopWhen: stepCountIs(20), // Allow up to 20 steps\n});\n\nEach step represents one generation (which results in either text or a tool call). The loop continues until:\n\nThe model generates text instead of calling a tool, or\nA stop condition is met\n\nYou can combine multiple conditions:\n\nimport { Experimental_Agent as Agent, stepCountIs } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  stopWhen: [\n    stepCountIs(20), // Maximum 20 steps\n    yourCustomCondition(), // Custom logic for when to stop\n  ],\n});\n\nLearn more about loop control and stop conditions.\n\nTool Choice\n\nControl how the agent uses tools:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    // your tools here\n  },\n  toolChoice: 'required', // Force tool use\n  // or toolChoice: 'none' to disable tools\n  // or toolChoice: 'auto' (default) to let the model decide\n});\n\nYou can also force the use of a specific tool:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    weather: weatherTool,\n    cityAttractions: attractionsTool,\n  },\n  toolChoice: {\n    type: 'tool',\n    toolName: 'weather', // Force the weather tool to be used\n  },\n});\nStructured Output\n\nDefine structured output schemas:\n\nimport { Experimental_Agent as Agent, Output, stepCountIs } from 'ai';\nimport { z } from 'zod';\n\n\nconst analysisAgent = new Agent({\n  model: 'openai/gpt-4o',\n  experimental_output: Output.object({\n    schema: z.object({\n      sentiment: z.enum(['positive', 'neutral', 'negative']),\n      summary: z.string(),\n      keyPoints: z.array(z.string()),\n    }),\n  }),\n  stopWhen: stepCountIs(10),\n});\n\n\nconst { experimental_output: output } = await analysisAgent.generate({\n  prompt: 'Analyze customer feedback from the last quarter',\n});\nDefine Agent Behavior with System Prompts\n\nSystem prompts define your agent's behavior, personality, and constraints. They set the context for all interactions and guide how the agent responds to user queries and uses tools.\n\nBasic System Prompts\n\nSet the agent's role and expertise:\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  system:\n    'You are an expert data analyst. You provide clear insights from complex data.',\n});\nDetailed Behavioral Instructions\n\nProvide specific guidelines for agent behavior:\n\nconst codeReviewAgent = new Agent({\n  model: 'openai/gpt-4o',\n  system: `You are a senior software engineer conducting code reviews.\n\n\n  Your approach:\n  - Focus on security vulnerabilities first\n  - Identify performance bottlenecks\n  - Suggest improvements for readability and maintainability\n  - Be constructive and educational in your feedback\n  - Always explain why something is an issue and how to fix it`,\n});\nConstrain Agent Behavior\n\nSet boundaries and ensure consistent behavior:\n\nconst customerSupportAgent = new Agent({\n  model: 'openai/gpt-4o',\n  system: `You are a customer support specialist for an e-commerce platform.\n\n\n  Rules:\n  - Never make promises about refunds without checking the policy\n  - Always be empathetic and professional\n  - If you don't know something, say so and offer to escalate\n  - Keep responses concise and actionable\n  - Never share internal company information`,\n  tools: {\n    checkOrderStatus,\n    lookupPolicy,\n    createTicket,\n  },\n});\nTool Usage Instructions\n\nGuide how the agent should use available tools:\n\nconst researchAgent = new Agent({\n  model: 'openai/gpt-4o',\n  system: `You are a research assistant with access to search and document tools.\n\n\n  When researching:\n  1. Always start with a broad search to understand the topic\n  2. Use document analysis for detailed information\n  3. Cross-reference multiple sources before drawing conclusions\n  4. Cite your sources when presenting information\n  5. If information conflicts, present both viewpoints`,\n  tools: {\n    webSearch,\n    analyzeDocument,\n    extractQuotes,\n  },\n});\nFormat and Style Instructions\n\nControl the output format and communication style:\n\nconst technicalWriterAgent = new Agent({\n  model: 'openai/gpt-4o',\n  system: `You are a technical documentation writer.\n\n\n  Writing style:\n  - Use clear, simple language\n  - Avoid jargon unless necessary\n  - Structure information with headers and bullet points\n  - Include code examples where relevant\n  - Write in second person (\"you\" instead of \"the user\")\n\n\n  Always format responses in Markdown.`,\n});\nUsing an Agent\n\nOnce defined, you can use your agent in three ways:\n\nGenerate Text\n\nUse generate() for one-time text generation:\n\nconst result = await myAgent.generate({\n  prompt: 'What is the weather like?',\n});\n\n\nconsole.log(result.text);\nStream Text\n\nUse stream() for streaming responses:\n\nconst stream = myAgent.stream({\n  prompt: 'Tell me a story',\n});\n\n\nfor await (const chunk of stream.textStream) {\n  console.log(chunk);\n}\nRespond to UI Messages\n\nUse respond() to create API responses for client applications:\n\n// In your API route (e.g., app/api/chat/route.ts)\nimport { validateUIMessages } from 'ai';\n\n\nexport async function POST(request: Request) {\n  const { messages } = await request.json();\n\n\n  return myAgent.respond({\n    messages: await validateUIMessages({ messages }),\n  });\n}\nEnd-to-end Type Safety\n\nYou can infer types for your Agent's UIMessages:\n\nimport {\n  Experimental_Agent as Agent,\n  Experimental_InferAgentUIMessage as InferAgentUIMessage,\n} from 'ai';\n\n\nconst myAgent = new Agent({\n  // ... configuration\n});\n\n\n// Infer the UIMessage type for UI components or persistence\nexport type MyAgentUIMessage = InferAgentUIMessage<typeof myAgent>;\n\nUse this type in your client components with useChat:\n\ncomponents/chat.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport type { MyAgentUIMessage } from '@/agent/my-agent';\n\n\nexport function Chat() {\n  const { messages } = useChat<MyAgentUIMessage>();\n  // Full type safety for your messages and tools\n}\nNext Steps\n\nNow that you understand building agents, you can:\n\nExplore workflow patterns for structured patterns using core functions\nLearn about loop control for advanced execution control\nSee manual loop examples for custom workflow implementations\nPrevious\nAgents\nNext\nWorkflow Patterns"
  },
  {
    "title": "Agents: Workflow Patterns",
    "url": "https://ai-sdk.dev/docs/agents/workflows",
    "html": "Agents\nWorkflow Patterns\nCopy markdown\nWorkflow Patterns\n\nCombine the building blocks from the overview with these patterns to add structure and reliability to your agents:\n\nSequential Processing - Steps executed in order\nParallel Processing - Independent tasks run simultaneously\nEvaluation/Feedback Loops - Results checked and improved iteratively\nOrchestration - Coordinating multiple components\nRouting - Directing work based on context\nChoose Your Approach\n\nConsider these key factors:\n\nFlexibility vs Control - How much freedom does the LLM need vs how tightly you must constrain its actions?\nError Tolerance - What are the consequences of mistakes in your use case?\nCost Considerations - More complex systems typically mean more LLM calls and higher costs\nMaintenance - Simpler architectures are easier to debug and modify\n\nStart with the simplest approach that meets your needs. Add complexity only when required by:\n\nBreaking down tasks into clear steps\nAdding tools for specific capabilities\nImplementing feedback loops for quality control\nIntroducing multiple agents for complex workflows\n\nLet's look at examples of these patterns in action.\n\nPatterns with Examples\n\nThese patterns, adapted from Anthropic's guide on building effective agents\n, serve as building blocks you can combine to create comprehensive workflows. Each pattern addresses specific aspects of task execution. Combine them thoughtfully to build reliable solutions for complex problems.\n\nSequential Processing (Chains)\n\nThe simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. Use this pattern for tasks with well-defined sequences, like content generation pipelines or data transformation processes.\n\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\n\nasync function generateMarketingCopy(input: string) {\n  const model = 'openai/gpt-4o';\n\n\n  // First step: Generate marketing copy\n  const { text: copy } = await generateText({\n    model,\n    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,\n  });\n\n\n  // Perform quality check on copy\n  const { object: qualityMetrics } = await generateObject({\n    model,\n    schema: z.object({\n      hasCallToAction: z.boolean(),\n      emotionalAppeal: z.number().min(1).max(10),\n      clarity: z.number().min(1).max(10),\n    }),\n    prompt: `Evaluate this marketing copy for:\n    1. Presence of call to action (true/false)\n    2. Emotional appeal (1-10)\n    3. Clarity (1-10)\n\n\n    Copy to evaluate: ${copy}`,\n  });\n\n\n  // If quality check fails, regenerate with more specific instructions\n  if (\n    !qualityMetrics.hasCallToAction ||\n    qualityMetrics.emotionalAppeal < 7 ||\n    qualityMetrics.clarity < 7\n  ) {\n    const { text: improvedCopy } = await generateText({\n      model,\n      prompt: `Rewrite this marketing copy with:\n      ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}\n      ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}\n      ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}\n\n\n      Original copy: ${copy}`,\n    });\n    return { copy: improvedCopy, qualityMetrics };\n  }\n\n\n  return { copy, qualityMetrics };\n}\nRouting\n\nThis pattern lets the model decide which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. Use this when handling varied inputs that require different processing approaches. In the example below, the first LLM call's results determine the second call's model size and system prompt.\n\nimport { generateObject, generateText } from 'ai';\nimport { z } from 'zod';\n\n\nasync function handleCustomerQuery(query: string) {\n  const model = 'openai/gpt-4o';\n\n\n  // First step: Classify the query type\n  const { object: classification } = await generateObject({\n    model,\n    schema: z.object({\n      reasoning: z.string(),\n      type: z.enum(['general', 'refund', 'technical']),\n      complexity: z.enum(['simple', 'complex']),\n    }),\n    prompt: `Classify this customer query:\n    ${query}\n\n\n    Determine:\n    1. Query type (general, refund, or technical)\n    2. Complexity (simple or complex)\n    3. Brief reasoning for classification`,\n  });\n\n\n  // Route based on classification\n  // Set model and system prompt based on query type and complexity\n  const { text: response } = await generateText({\n    model:\n      classification.complexity === 'simple'\n        ? 'openai/gpt-4o-mini'\n        : 'openai/o4-mini',\n    system: {\n      general:\n        'You are an expert customer service agent handling general inquiries.',\n      refund:\n        'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',\n      technical:\n        'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',\n    }[classification.type],\n    prompt: query,\n  });\n\n\n  return { response, classification };\n}\nParallel Processing\n\nBreak down tasks into independent subtasks that execute simultaneously. This pattern uses parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyze multiple documents or process different aspects of a single input concurrently (like code review).\n\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\n\n// Example: Parallel code review with multiple specialized reviewers\nasync function parallelCodeReview(code: string) {\n  const model = 'openai/gpt-4o';\n\n\n  // Run parallel reviews\n  const [securityReview, performanceReview, maintainabilityReview] =\n    await Promise.all([\n      generateObject({\n        model,\n        system:\n          'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',\n        schema: z.object({\n          vulnerabilities: z.array(z.string()),\n          riskLevel: z.enum(['low', 'medium', 'high']),\n          suggestions: z.array(z.string()),\n        }),\n        prompt: `Review this code:\n      ${code}`,\n      }),\n\n\n      generateObject({\n        model,\n        system:\n          'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',\n        schema: z.object({\n          issues: z.array(z.string()),\n          impact: z.enum(['low', 'medium', 'high']),\n          optimizations: z.array(z.string()),\n        }),\n        prompt: `Review this code:\n      ${code}`,\n      }),\n\n\n      generateObject({\n        model,\n        system:\n          'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',\n        schema: z.object({\n          concerns: z.array(z.string()),\n          qualityScore: z.number().min(1).max(10),\n          recommendations: z.array(z.string()),\n        }),\n        prompt: `Review this code:\n      ${code}`,\n      }),\n    ]);\n\n\n  const reviews = [\n    { ...securityReview.object, type: 'security' },\n    { ...performanceReview.object, type: 'performance' },\n    { ...maintainabilityReview.object, type: 'maintainability' },\n  ];\n\n\n  // Aggregate results using another model instance\n  const { text: summary } = await generateText({\n    model,\n    system: 'You are a technical lead summarizing multiple code reviews.',\n    prompt: `Synthesize these code review results into a concise summary with key actions:\n    ${JSON.stringify(reviews, null, 2)}`,\n  });\n\n\n  return { reviews, summary };\n}\nOrchestrator-Worker\n\nA primary model (orchestrator) coordinates the execution of specialized workers. Each worker optimizes for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.\n\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\n\nasync function implementFeature(featureRequest: string) {\n  // Orchestrator: Plan the implementation\n  const { object: implementationPlan } = await generateObject({\n    model: 'openai/o4-mini',\n    schema: z.object({\n      files: z.array(\n        z.object({\n          purpose: z.string(),\n          filePath: z.string(),\n          changeType: z.enum(['create', 'modify', 'delete']),\n        }),\n      ),\n      estimatedComplexity: z.enum(['low', 'medium', 'high']),\n    }),\n    system:\n      'You are a senior software architect planning feature implementations.',\n    prompt: `Analyze this feature request and create an implementation plan:\n    ${featureRequest}`,\n  });\n\n\n  // Workers: Execute the planned changes\n  const fileChanges = await Promise.all(\n    implementationPlan.files.map(async file => {\n      // Each worker is specialized for the type of change\n      const workerSystemPrompt = {\n        create:\n          'You are an expert at implementing new files following best practices and project patterns.',\n        modify:\n          'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',\n        delete:\n          'You are an expert at safely removing code while ensuring no breaking changes.',\n      }[file.changeType];\n\n\n      const { object: change } = await generateObject({\n        model: 'openai/gpt-4o',\n        schema: z.object({\n          explanation: z.string(),\n          code: z.string(),\n        }),\n        system: workerSystemPrompt,\n        prompt: `Implement the changes for ${file.filePath} to support:\n        ${file.purpose}\n\n\n        Consider the overall feature context:\n        ${featureRequest}`,\n      });\n\n\n      return {\n        file,\n        implementation: change,\n      };\n    }),\n  );\n\n\n  return {\n    plan: implementationPlan,\n    changes: fileChanges,\n  };\n}\nEvaluator-Optimizer\n\nAdd quality control to workflows with dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow proceeds, retries with adjusted parameters, or takes corrective action. This creates robust workflows capable of self-improvement and error recovery.\n\nimport { generateText, generateObject } from 'ai';\nimport { z } from 'zod';\n\n\nasync function translateWithFeedback(text: string, targetLanguage: string) {\n  let currentTranslation = '';\n  let iterations = 0;\n  const MAX_ITERATIONS = 3;\n\n\n  // Initial translation\n  const { text: translation } = await generateText({\n    model: 'openai/gpt-4o-mini', // use small model for first attempt\n    system: 'You are an expert literary translator.',\n    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:\n    ${text}`,\n  });\n\n\n  currentTranslation = translation;\n\n\n  // Evaluation-optimization loop\n  while (iterations < MAX_ITERATIONS) {\n    // Evaluate current translation\n    const { object: evaluation } = await generateObject({\n      model: 'openai/gpt-4o', // use a larger model to evaluate\n      schema: z.object({\n        qualityScore: z.number().min(1).max(10),\n        preservesTone: z.boolean(),\n        preservesNuance: z.boolean(),\n        culturallyAccurate: z.boolean(),\n        specificIssues: z.array(z.string()),\n        improvementSuggestions: z.array(z.string()),\n      }),\n      system: 'You are an expert in evaluating literary translations.',\n      prompt: `Evaluate this translation:\n\n\n      Original: ${text}\n      Translation: ${currentTranslation}\n\n\n      Consider:\n      1. Overall quality\n      2. Preservation of tone\n      3. Preservation of nuance\n      4. Cultural accuracy`,\n    });\n\n\n    // Check if quality meets threshold\n    if (\n      evaluation.qualityScore >= 8 &&\n      evaluation.preservesTone &&\n      evaluation.preservesNuance &&\n      evaluation.culturallyAccurate\n    ) {\n      break;\n    }\n\n\n    // Generate improved translation based on feedback\n    const { text: improvedTranslation } = await generateText({\n      model: 'openai/gpt-4o', // use a larger model\n      system: 'You are an expert literary translator.',\n      prompt: `Improve this translation based on the following feedback:\n      ${evaluation.specificIssues.join('\\n')}\n      ${evaluation.improvementSuggestions.join('\\n')}\n\n\n      Original: ${text}\n      Current Translation: ${currentTranslation}`,\n    });\n\n\n    currentTranslation = improvedTranslation;\n    iterations++;\n  }\n\n\n  return {\n    finalTranslation: currentTranslation,\n    iterationsRequired: iterations,\n  };\n}\nPrevious\nBuilding Agents\nNext\nLoop Control"
  },
  {
    "title": "Agents: Loop Control",
    "url": "https://ai-sdk.dev/docs/agents/loop-control",
    "html": "Agents\nLoop Control\nCopy markdown\nLoop Control\n\nYou can control both the execution flow and the settings at each step of the agent loop. The AI SDK provides built-in loop control through two parameters: stopWhen for defining stopping conditions and prepareStep for modifying settings (model, tools, messages, and more) between steps.\n\nStop Conditions\n\nThe stopWhen parameter controls when to stop execution when there are tool results in the last step. By default, agents stop after a single step using stepCountIs(1).\n\nWhen you provide stopWhen, the agent continues executing after tool calls until a stopping condition is met. When the condition is an array, execution stops when any of the conditions are met.\n\nUse Built-in Conditions\n\nThe AI SDK provides several built-in stopping conditions:\n\nimport { Experimental_Agent as Agent, stepCountIs } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    // your tools\n  },\n  stopWhen: stepCountIs(20), // Stop after 20 steps maximum\n});\n\n\nconst result = await agent.generate({\n  prompt: 'Analyze this dataset and create a summary report',\n});\nCombine Multiple Conditions\n\nCombine multiple stopping conditions. The loop stops when it meets any condition:\n\nimport { Experimental_Agent as Agent, stepCountIs, hasToolCall } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    // your tools\n  },\n  stopWhen: [\n    stepCountIs(20), // Maximum 20 steps\n    hasToolCall('someTool'), // Stop after calling 'someTool'\n  ],\n});\n\n\nconst result = await agent.generate({\n  prompt: 'Research and analyze the topic',\n});\nCreate Custom Conditions\n\nBuild custom stopping conditions for specific requirements:\n\nimport { Experimental_Agent as Agent, StopCondition, ToolSet } from 'ai';\n\n\nconst tools = {\n  // your tools\n} satisfies ToolSet;\n\n\nconst hasAnswer: StopCondition<typeof tools> = ({ steps }) => {\n  // Stop when the model generates text containing \"ANSWER:\"\n  return steps.some(step => step.text?.includes('ANSWER:')) ?? false;\n};\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools,\n  stopWhen: hasAnswer,\n});\n\n\nconst result = await agent.generate({\n  prompt: 'Find the answer and respond with \"ANSWER: [your answer]\"',\n});\n\nCustom conditions receive step information across all steps:\n\nconst budgetExceeded: StopCondition<typeof tools> = ({ steps }) => {\n  const totalUsage = steps.reduce(\n    (acc, step) => ({\n      inputTokens: acc.inputTokens + (step.usage?.inputTokens ?? 0),\n      outputTokens: acc.outputTokens + (step.usage?.outputTokens ?? 0),\n    }),\n    { inputTokens: 0, outputTokens: 0 },\n  );\n\n\n  const costEstimate =\n    (totalUsage.inputTokens * 0.01 + totalUsage.outputTokens * 0.03) / 1000;\n  return costEstimate > 0.5; // Stop if cost exceeds $0.50\n};\nPrepare Step\n\nThe prepareStep callback runs before each step in the loop and defaults to the initial settings if you don't return any changes. Use it to modify settings, manage context, or implement dynamic behavior based on execution history.\n\nDynamic Model Selection\n\nSwitch models based on step requirements:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o-mini', // Default model\n  tools: {\n    // your tools\n  },\n  prepareStep: async ({ stepNumber, messages }) => {\n    // Use a stronger model for complex reasoning after initial steps\n    if (stepNumber > 2 && messages.length > 10) {\n      return {\n        model: 'openai/gpt-4o',\n      };\n    }\n    // Continue with default settings\n    return {};\n  },\n});\n\n\nconst result = await agent.generate({\n  prompt: '...',\n});\nContext Management\n\nManage growing conversation history in long-running loops:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    // your tools\n  },\n  prepareStep: async ({ messages }) => {\n    // Keep only recent messages to stay within context limits\n    if (messages.length > 20) {\n      return {\n        messages: [\n          messages[0], // Keep system message\n          ...messages.slice(-10), // Keep last 10 messages\n        ],\n      };\n    }\n    return {};\n  },\n});\n\n\nconst result = await agent.generate({\n  prompt: '...',\n});\nTool Selection\n\nControl which tools are available at each step:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    search: searchTool,\n    analyze: analyzeTool,\n    summarize: summarizeTool,\n  },\n  prepareStep: async ({ stepNumber, steps }) => {\n    // Search phase (steps 0-2)\n    if (stepNumber <= 2) {\n      return {\n        activeTools: ['search'],\n        toolChoice: 'required',\n      };\n    }\n\n\n    // Analysis phase (steps 3-5)\n    if (stepNumber <= 5) {\n      return {\n        activeTools: ['analyze'],\n      };\n    }\n\n\n    // Summary phase (step 6+)\n    return {\n      activeTools: ['summarize'],\n      toolChoice: 'required',\n    };\n  },\n});\n\n\nconst result = await agent.generate({\n  prompt: '...',\n});\n\nYou can also force a specific tool to be used:\n\nprepareStep: async ({ stepNumber }) => {\n  if (stepNumber === 0) {\n    // Force the search tool to be used first\n    return {\n      toolChoice: { type: 'tool', toolName: 'search' },\n    };\n  }\n\n\n  if (stepNumber === 5) {\n    // Force the summarize tool after analysis\n    return {\n      toolChoice: { type: 'tool', toolName: 'summarize' },\n    };\n  }\n\n\n  return {};\n};\nMessage Modification\n\nTransform messages before sending them to the model:\n\nimport { Experimental_Agent as Agent } from 'ai';\n\n\nconst agent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    // your tools\n  },\n  prepareStep: async ({ messages, stepNumber }) => {\n    // Summarize tool results to reduce token usage\n    const processedMessages = messages.map(msg => {\n      if (msg.role === 'tool' && msg.content.length > 1000) {\n        return {\n          ...msg,\n          content: summarizeToolResult(msg.content),\n        };\n      }\n      return msg;\n    });\n\n\n    return { messages: processedMessages };\n  },\n});\n\n\nconst result = await agent.generate({\n  prompt: '...',\n});\nAccess Step Information\n\nBoth stopWhen and prepareStep receive detailed information about the current execution:\n\nprepareStep: async ({\n  model, // Current model configuration\n  stepNumber, // Current step number (0-indexed)\n  steps, // All previous steps with their results\n  messages, // Messages to be sent to the model\n}) => {\n  // Access previous tool calls and results\n  const previousToolCalls = steps.flatMap(step => step.toolCalls);\n  const previousResults = steps.flatMap(step => step.toolResults);\n\n\n  // Make decisions based on execution history\n  if (previousToolCalls.some(call => call.toolName === 'dataAnalysis')) {\n    return {\n      toolChoice: { type: 'tool', toolName: 'reportGenerator' },\n    };\n  }\n\n\n  return {};\n},\nManual Loop Control\n\nFor scenarios requiring complete control over the agent loop, you can use AI SDK Core functions (generateText and streamText) to implement your own loop management instead of using stopWhen and prepareStep. This approach provides maximum flexibility for complex workflows.\n\nImplementing a Manual Loop\n\nBuild your own agent loop when you need full control over execution:\n\nimport { generateText, ModelMessage } from 'ai';\n\n\nconst messages: ModelMessage[] = [{ role: 'user', content: '...' }];\n\n\nlet step = 0;\nconst maxSteps = 10;\n\n\nwhile (step < maxSteps) {\n  const result = await generateText({\n    model: 'openai/gpt-4o',\n    messages,\n    tools: {\n      // your tools here\n    },\n  });\n\n\n  messages.push(...result.response.messages);\n\n\n  if (result.text) {\n    break; // Stop when model generates text\n  }\n\n\n  step++;\n}\n\nThis manual approach gives you complete control over:\n\nMessage history management\nStep-by-step decision making\nCustom stopping conditions\nDynamic tool and model selection\nError handling and recovery\n\nLearn more about manual agent loops in the cookbook.\n\nPrevious\nWorkflow Patterns\nNext\nAI SDK Core"
  },
  {
    "title": "AI SDK Core: Overview",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/overview",
    "html": "AI SDK Core\nOverview\nCopy markdown\nAI SDK Core\n\nLarge Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale. They are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.\n\nAI SDK Core simplifies working with LLMs by offering a standardized way of integrating them into your app - so you can focus on building great AI applications for your users, not waste time on technical details.\n\nFor example, here’s how you can generate text with various models using the AI SDK:\n\nxAI\nOpenAI\nAnthropic\nGoogle\nCustom\nimport { generateText } from \"ai\"\nimport { xai } from \"@ai-sdk/xai\"\nconst { text } = await generateText({\nmodel: xai(\"grok-4\"),\nprompt: \"What is love?\"\n})\nLove is a universal emotion that is characterized by feelings of affection, attachment, and warmth towards someone or something. It is a complex and multifaceted experience that can take many different forms, including romantic love, familial love, platonic love, and self-love.\nAI SDK Core Functions\n\nAI SDK Core has various functions designed for text generation, structured data generation, and tool usage. These functions take a standardized approach to setting up prompts and settings, making it easier to work with different models.\n\ngenerateText: Generates text and tool calls. This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\nstreamText: Stream text and tool calls. You can use the streamText function for interactive use cases such as chat bots and content streaming.\ngenerateObject: Generates a typed, structured object that matches a Zod\n schema. You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\nstreamObject: Stream a structured object that matches a Zod schema. You can use this function to stream generated UIs.\nAPI Reference\n\nPlease check out the AI SDK Core API Reference for more details on each function.\n\nPrevious\nAI SDK Core\nNext\nGenerating Text"
  },
  {
    "title": "AI SDK Core: Generating Text",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text",
    "html": "AI SDK Core\nGenerating Text\nCopy markdown\nGenerating and Streaming Text\n\nLarge language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process. For example, you can ask a model to come up with a recipe, draft an email, or summarize a document.\n\nThe AI SDK Core provides two functions to generate text and stream it from LLMs:\n\ngenerateText: Generates text for a given prompt and model.\nstreamText: Streams text from a given prompt and model.\n\nAdvanced LLM features such as tool calling and structured data generation are built on top of text generation.\n\ngenerateText\n\nYou can generate text using the generateText function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\n\nimport { generateText } from 'ai';\n\n\nconst { text } = await generateText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n\nYou can use more advanced prompts to generate text with more complex instructions and content:\n\nimport { generateText } from 'ai';\n\n\nconst { text } = await generateText({\n  model: 'openai/gpt-4.1',\n  system:\n    'You are a professional writer. ' +\n    'You write simple, clear, and concise content.',\n  prompt: `Summarize the following article in 3-5 sentences: ${article}`,\n});\n\nThe result object of generateText contains several promises that resolve when all required data is available:\n\nresult.content: The content that was generated in the last step.\nresult.text: The generated text.\nresult.reasoning: The full reasoning that the model has generated in the last step.\nresult.reasoningText: The reasoning text of the model (only available for some models).\nresult.files: The files that were generated in the last step.\nresult.sources: Sources that have been used as references in the last step (only available for some models).\nresult.toolCalls: The tool calls that were made in the last step.\nresult.toolResults: The results of the tool calls from the last step.\nresult.finishReason: The reason the model finished generating text.\nresult.usage: The usage of the model during the final step of text generation.\nresult.totalUsage: The total usage across all steps (for multi-step generations).\nresult.warnings: Warnings from the model provider (e.g. unsupported settings).\nresult.request: Additional request information.\nresult.response: Additional response information, including response messages and body.\nresult.providerMetadata: Additional provider-specific metadata.\nresult.steps: Details for all steps, useful for getting information about intermediate steps.\nresult.experimental_output: The generated structured output using the experimental_output specification.\nAccessing response headers & body\n\nSometimes you need access to the full response from the model provider, e.g. to access some provider-specific headers or body content.\n\nYou can access the raw response headers and body using the response property:\n\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  // ...\n});\n\n\nconsole.log(JSON.stringify(result.response.headers, null, 2));\nconsole.log(JSON.stringify(result.response.body, null, 2));\nstreamText\n\nDepending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.\n\nAI SDK Core provides the streamText function which simplifies streaming text from LLMs:\n\nimport { streamText } from 'ai';\n\n\nconst result = streamText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\n\n// example: use textStream as an async iterable\nfor await (const textPart of result.textStream) {\n  console.log(textPart);\n}\n\nresult.textStream is both a ReadableStream and an AsyncIterable.\n\nstreamText immediately starts streaming and suppresses errors to prevent server crashes. Use the onError callback to log errors.\n\nYou can use streamText on its own or in combination with AI SDK UI and AI SDK RSC. The result object contains several helper functions to make the integration into AI SDK UI easier:\n\nresult.toUIMessageStreamResponse(): Creates a UI Message stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.\nresult.pipeUIMessageStreamToResponse(): Writes UI Message stream delta output to a Node.js response-like object.\nresult.toTextStreamResponse(): Creates a simple text stream HTTP response.\nresult.pipeTextStreamToResponse(): Writes text delta output to a Node.js response-like object.\n\nstreamText is using backpressure and only generates tokens as they are requested. You need to consume the stream in order for it to finish.\n\nIt also provides several promises that resolve when the stream is finished:\n\nresult.content: The content that was generated in the last step.\nresult.text: The generated text.\nresult.reasoning: The full reasoning that the model has generated.\nresult.reasoningText: The reasoning text of the model (only available for some models).\nresult.files: Files that have been generated by the model in the last step.\nresult.sources: Sources that have been used as references in the last step (only available for some models).\nresult.toolCalls: The tool calls that have been executed in the last step.\nresult.toolResults: The tool results that have been generated in the last step.\nresult.finishReason: The reason the model finished generating text.\nresult.usage: The usage of the model during the final step of text generation.\nresult.totalUsage: The total usage across all steps (for multi-step generations).\nresult.warnings: Warnings from the model provider (e.g. unsupported settings).\nresult.steps: Details for all steps, useful for getting information about intermediate steps.\nresult.request: Additional request information from the last step.\nresult.response: Additional response information from the last step.\nresult.providerMetadata: Additional provider-specific metadata from the last step.\nonError callback\n\nstreamText immediately starts streaming to enable sending data without waiting for the model. Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.\n\nTo log errors, you can provide an onError callback that is triggered when an error occurs.\n\nimport { streamText } from 'ai';\n\n\nconst result = streamText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onError({ error }) {\n    console.error(error); // your error logging logic here\n  },\n});\nonChunk callback\n\nWhen using streamText, you can provide an onChunk callback that is triggered for each chunk of the stream.\n\nIt receives the following chunk types:\n\ntext\nreasoning\nsource\ntool-call\ntool-input-start\ntool-input-delta\ntool-result\nraw\nimport { streamText } from 'ai';\n\n\nconst result = streamText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onChunk({ chunk }) {\n    // implement your own logic here, e.g.:\n    if (chunk.type === 'text') {\n      console.log(chunk.text);\n    }\n  },\n});\nonFinish callback\n\nWhen using streamText, you can provide an onFinish callback that is triggered when the stream is finished ( API Reference ). It contains the text, usage information, finish reason, messages, steps, total usage, and more:\n\nimport { streamText } from 'ai';\n\n\nconst result = streamText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Invent a new holiday and describe its traditions.',\n  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\n    // your own logic, e.g. for saving the chat history or recording usage\n\n\n    const messages = response.messages; // messages that were generated\n  },\n});\nfullStream property\n\nYou can read a stream with all events using the fullStream property. This can be useful if you want to implement your own UI or handle the stream in a different way. Here is an example of how to use the fullStream property:\n\nimport { streamText } from 'ai';\nimport { z } from 'zod';\n\n\nconst result = streamText({\n  model: 'openai/gpt-4.1',\n  tools: {\n    cityAttractions: {\n      inputSchema: z.object({ city: z.string() }),\n      execute: async ({ city }) => ({\n        attractions: ['attraction1', 'attraction2', 'attraction3'],\n      }),\n    },\n  },\n  prompt: 'What are some San Francisco tourist attractions?',\n});\n\n\nfor await (const part of result.fullStream) {\n  switch (part.type) {\n    case 'start': {\n      // handle start of stream\n      break;\n    }\n    case 'start-step': {\n      // handle start of step\n      break;\n    }\n    case 'text-start': {\n      // handle text start\n      break;\n    }\n    case 'text-delta': {\n      // handle text delta here\n      break;\n    }\n    case 'text-end': {\n      // handle text end\n      break;\n    }\n    case 'reasoning-start': {\n      // handle reasoning start\n      break;\n    }\n    case 'reasoning-delta': {\n      // handle reasoning delta here\n      break;\n    }\n    case 'reasoning-end': {\n      // handle reasoning end\n      break;\n    }\n    case 'source': {\n      // handle source here\n      break;\n    }\n    case 'file': {\n      // handle file here\n      break;\n    }\n    case 'tool-call': {\n      switch (part.toolName) {\n        case 'cityAttractions': {\n          // handle tool call here\n          break;\n        }\n      }\n      break;\n    }\n    case 'tool-input-start': {\n      // handle tool input start\n      break;\n    }\n    case 'tool-input-delta': {\n      // handle tool input delta\n      break;\n    }\n    case 'tool-input-end': {\n      // handle tool input end\n      break;\n    }\n    case 'tool-result': {\n      switch (part.toolName) {\n        case 'cityAttractions': {\n          // handle tool result here\n          break;\n        }\n      }\n      break;\n    }\n    case 'tool-error': {\n      // handle tool error\n      break;\n    }\n    case 'finish-step': {\n      // handle finish step\n      break;\n    }\n    case 'finish': {\n      // handle finish here\n      break;\n    }\n    case 'error': {\n      // handle error here\n      break;\n    }\n    case 'raw': {\n      // handle raw value\n      break;\n    }\n  }\n}\nStream transformation\n\nYou can use the experimental_transform option to transform the stream. This is useful for e.g. filtering, changing, or smoothing the text stream.\n\nThe transformations are applied before the callbacks are invoked and the promises are resolved. If you e.g. have a transformation that changes all text to uppercase, the onFinish callback will receive the transformed text.\n\nSmoothing streams\n\nThe AI SDK Core provides a smoothStream function that can be used to smooth out text streaming.\n\nimport { smoothStream, streamText } from 'ai';\n\n\nconst result = streamText({\n  model,\n  prompt,\n  experimental_transform: smoothStream(),\n});\nCustom transformations\n\nYou can also implement your own custom transformations. The transformation function receives the tools that are available to the model, and returns a function that is used to transform the stream. Tools can either be generic or limited to the tools that you are using.\n\nHere is an example of how to implement a custom transformation that converts all text to uppercase:\n\nconst upperCaseTransform =\n  <TOOLS extends ToolSet>() =>\n  (options: { tools: TOOLS; stopStream: () => void }) =>\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n      transform(chunk, controller) {\n        controller.enqueue(\n          // for text chunks, convert the text to uppercase:\n          chunk.type === 'text'\n            ? { ...chunk, text: chunk.text.toUpperCase() }\n            : chunk,\n        );\n      },\n    });\n\nYou can also stop the stream using the stopStream function. This is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.\n\nWhen you invoke stopStream, it is important to simulate the step-finish and finish events to guarantee that a well-formed stream is returned and all callbacks are invoked.\n\nconst stopWordTransform =\n  <TOOLS extends ToolSet>() =>\n  ({ stopStream }: { stopStream: () => void }) =>\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n      // note: this is a simplified transformation for testing;\n      // in a real-world version more there would need to be\n      // stream buffering and scanning to correctly emit prior text\n      // and to detect all STOP occurrences.\n      transform(chunk, controller) {\n        if (chunk.type !== 'text') {\n          controller.enqueue(chunk);\n          return;\n        }\n\n\n        if (chunk.text.includes('STOP')) {\n          // stop the stream\n          stopStream();\n\n\n          // simulate the finish-step event\n          controller.enqueue({\n            type: 'finish-step',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: {\n              completionTokens: NaN,\n              promptTokens: NaN,\n              totalTokens: NaN,\n            },\n            request: {},\n            response: {\n              id: 'response-id',\n              modelId: 'mock-model-id',\n              timestamp: new Date(0),\n            },\n            warnings: [],\n            isContinued: false,\n          });\n\n\n          // simulate the finish event\n          controller.enqueue({\n            type: 'finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: {\n              completionTokens: NaN,\n              promptTokens: NaN,\n              totalTokens: NaN,\n            },\n            response: {\n              id: 'response-id',\n              modelId: 'mock-model-id',\n              timestamp: new Date(0),\n            },\n          });\n\n\n          return;\n        }\n\n\n        controller.enqueue(chunk);\n      },\n    });\nMultiple transformations\n\nYou can also provide multiple transformations. They are applied in the order they are provided.\n\nconst result = streamText({\n  model,\n  prompt,\n  experimental_transform: [firstTransform, secondTransform],\n});\nSources\n\nSome providers such as Perplexity and Google Generative AI include sources in the response.\n\nCurrently sources are limited to web pages that ground the response. You can access them using the sources property of the result.\n\nEach url source contains the following properties:\n\nid: The ID of the source.\nurl: The URL of the source.\ntitle: The optional title of the source.\nproviderMetadata: Provider metadata for the source.\n\nWhen you use generateText, you can access the sources using the sources property:\n\nconst result = await generateText({\n  model: google('gemini-2.5-flash'),\n  tools: {\n    google_search: google.tools.googleSearch({}),\n  },\n  prompt: 'List the top 5 San Francisco news from the past week.',\n});\n\n\nfor (const source of result.sources) {\n  if (source.sourceType === 'url') {\n    console.log('ID:', source.id);\n    console.log('Title:', source.title);\n    console.log('URL:', source.url);\n    console.log('Provider metadata:', source.providerMetadata);\n    console.log();\n  }\n}\n\nWhen you use streamText, you can access the sources using the fullStream property:\n\nconst result = streamText({\n  model: google('gemini-2.5-flash'),\n  tools: {\n    google_search: google.tools.googleSearch({}),\n  },\n  prompt: 'List the top 5 San Francisco news from the past week.',\n});\n\n\nfor await (const part of result.fullStream) {\n  if (part.type === 'source' && part.sourceType === 'url') {\n    console.log('ID:', part.id);\n    console.log('Title:', part.title);\n    console.log('URL:', part.url);\n    console.log('Provider metadata:', part.providerMetadata);\n    console.log();\n  }\n}\n\nThe sources are also available in the result.sources promise.\n\nExamples\n\nYou can see generateText and streamText in action using various frameworks in the following examples:\n\ngenerateText\nLearn to generate text in Node.js\nLearn to generate text in Next.js with Route Handlers (AI SDK UI)\nLearn to generate text in Next.js with Server Actions (AI SDK RSC)\nstreamText\nLearn to stream text in Node.js\nLearn to stream text in Next.js with Route Handlers (AI SDK UI)\nLearn to stream text in Next.js with Server Actions (AI SDK RSC)\nPrevious\nOverview\nNext\nGenerating Structured Data"
  },
  {
    "title": "AI SDK Core: Generating Structured Data",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data",
    "html": "AI SDK Core\nGenerating Structured Data\nCopy markdown\nGenerating Structured Data\n\nWhile text generation can be useful, your use case will likely call for generating structured data. For example, you might want to extract information from text, classify data, or generate synthetic data.\n\nMany language models are capable of generating structured data, often defined as using \"JSON modes\" or \"tools\". However, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.\n\nThe AI SDK standardises structured object generation across model providers with the generateObject and streamObject functions. You can use both functions with different output strategies, e.g. array, object, enum, or no-schema, and with different generation modes, e.g. auto, tool, or json. You can use Zod schemas, Valibot, or JSON schemas to specify the shape of the data that you want, and the AI model will generate data that conforms to that structure.\n\nYou can pass Zod objects directly to the AI SDK functions or use the zodSchema helper function.\n\nGenerate Object\n\nThe generateObject generates structured data from a prompt. The schema is also used to validate the generated data, ensuring type safety and correctness.\n\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\n\nconst { object } = await generateObject({\n  model: 'openai/gpt-4.1',\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n\nSee generateObject in action with these examples\n\nAccessing response headers & body\n\nSometimes you need access to the full response from the model provider, e.g. to access some provider-specific headers or body content.\n\nYou can access the raw response headers and body using the response property:\n\nimport { generateObject } from 'ai';\n\n\nconst result = await generateObject({\n  // ...\n});\n\n\nconsole.log(JSON.stringify(result.response.headers, null, 2));\nconsole.log(JSON.stringify(result.response.body, null, 2));\nStream Object\n\nGiven the added complexity of returning structured data, model response time can be unacceptable for your interactive use case. With the streamObject function, you can stream the model's response as it is generated.\n\nimport { streamObject } from 'ai';\n\n\nconst { partialObjectStream } = streamObject({\n  // ...\n});\n\n\n// use partialObjectStream as an async iterable\nfor await (const partialObject of partialObjectStream) {\n  console.log(partialObject);\n}\n\nYou can use streamObject to stream generated UIs in combination with React Server Components (see Generative UI)) or the useObject hook.\n\nSee streamObject in action with these examples\nonError callback\n\nstreamObject immediately starts streaming. Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.\n\nTo log errors, you can provide an onError callback that is triggered when an error occurs.\n\nimport { streamObject } from 'ai';\n\n\nconst result = streamObject({\n  // ...\n  onError({ error }) {\n    console.error(error); // your error logging logic here\n  },\n});\nOutput Strategy\n\nYou can use both functions with different output strategies, e.g. array, object, enum, or no-schema.\n\nObject\n\nThe default output strategy is object, which returns the generated data as an object. You don't need to specify the output strategy if you want to use the default.\n\nArray\n\nIf you want to generate an array of objects, you can set the output strategy to array. When you use the array output strategy, the schema specifies the shape of an array element. With streamObject, you can also stream the generated array elements using elementStream.\n\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\n\nconst { elementStream } = streamObject({\n  model: openai('gpt-4.1'),\n  output: 'array',\n  schema: z.object({\n    name: z.string(),\n    class: z\n      .string()\n      .describe('Character class, e.g. warrior, mage, or thief.'),\n    description: z.string(),\n  }),\n  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',\n});\n\n\nfor await (const hero of elementStream) {\n  console.log(hero);\n}\nEnum\n\nIf you want to generate a specific enum value, e.g. for classification tasks, you can set the output strategy to enum and provide a list of possible values in the enum parameter.\n\nEnum output is only available with generateObject.\nimport { generateObject } from 'ai';\n\n\nconst { object } = await generateObject({\n  model: 'openai/gpt-4.1',\n  output: 'enum',\n  enum: ['action', 'comedy', 'drama', 'horror', 'sci-fi'],\n  prompt:\n    'Classify the genre of this movie plot: ' +\n    '\"A group of astronauts travel through a wormhole in search of a ' +\n    'new habitable planet for humanity.\"',\n});\nNo Schema\n\nIn some cases, you might not want to use a schema, for example when the data is a dynamic user request. You can use the output setting to set the output format to no-schema in those cases and omit the schema parameter.\n\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\n\n\nconst { object } = await generateObject({\n  model: openai('gpt-4.1'),\n  output: 'no-schema',\n  prompt: 'Generate a lasagna recipe.',\n});\nSchema Name and Description\n\nYou can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.\n\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\n\nconst { object } = await generateObject({\n  model: 'openai/gpt-4.1',\n  schemaName: 'Recipe',\n  schemaDescription: 'A recipe for a dish.',\n  schema: z.object({\n    name: z.string(),\n    ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n    steps: z.array(z.string()),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\nAccessing Reasoning\n\nYou can access the reasoning used by the language model to generate the object via the reasoning property on the result. This property contains a string with the model's thought process, if available.\n\nimport { openai, OpenAIResponsesProviderOptions } from '@ai-sdk/openai';\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\n\nconst result = await generateObject({\n  model: openai('gpt-5'),\n  schema: z.object({\n    recipe: z.object({\n      name: z.string(),\n      ingredients: z.array(\n        z.object({\n          name: z.string(),\n          amount: z.string(),\n        }),\n      ),\n      steps: z.array(z.string()),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n  providerOptions: {\n    openai: {\n      strictJsonSchema: true,\n      reasoningSummary: 'detailed',\n    } satisfies OpenAIResponsesProviderOptions,\n  },\n});\n\n\nconsole.log(result.reasoning);\nError Handling\n\nWhen generateObject cannot generate a valid object, it throws a AI_NoObjectGeneratedError.\n\nThis error occurs when the AI provider fails to generate a parsable object that conforms to the schema. It can arise due to the following reasons:\n\nThe model failed to generate a response.\nThe model generated a response that could not be parsed.\nThe model generated a response that could not be validated against the schema.\n\nThe error preserves the following information to help you log the issue:\n\ntext: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.\nresponse: Metadata about the language model response, including response id, timestamp, and model.\nusage: Request token usage.\ncause: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.\nimport { generateObject, NoObjectGeneratedError } from 'ai';\n\n\ntry {\n  await generateObject({ model, schema, prompt });\n} catch (error) {\n  if (NoObjectGeneratedError.isInstance(error)) {\n    console.log('NoObjectGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Text:', error.text);\n    console.log('Response:', error.response);\n    console.log('Usage:', error.usage);\n  }\n}\nRepairing Invalid or Malformed JSON\n\nThe repairText function is experimental and may change in the future.\n\nSometimes the model will generate invalid or malformed JSON. You can use the repairText function to attempt to repair the JSON.\n\nIt receives the error, either a JSONParseError or a TypeValidationError, and the text that was generated by the model. You can then attempt to repair the text and return the repaired text.\n\nimport { generateObject } from 'ai';\n\n\nconst { object } = await generateObject({\n  model,\n  schema,\n  prompt,\n  experimental_repairText: async ({ text, error }) => {\n    // example: add a closing brace to the text\n    return text + '}';\n  },\n});\nStructured outputs with generateText and streamText\n\nYou can generate structured data with generateText and streamText by using the experimental_output setting.\n\nSome models, e.g. those by OpenAI, support structured outputs and tool calling at the same time. This is only possible with generateText and streamText.\n\nStructured output generation with generateText and streamText is experimental and may change in the future.\n\ngenerateText\n// experimental_output is a structured object that matches the schema:\nconst { experimental_output } = await generateText({\n  // ...\n  experimental_output: Output.object({\n    schema: z.object({\n      name: z.string(),\n      age: z.number().nullable().describe('Age of the person.'),\n      contact: z.object({\n        type: z.literal('email'),\n        value: z.string(),\n      }),\n      occupation: z.object({\n        type: z.literal('employed'),\n        company: z.string(),\n        position: z.string(),\n      }),\n    }),\n  }),\n  prompt: 'Generate an example person for testing.',\n});\nstreamText\n// experimental_partialOutputStream contains generated partial objects:\nconst { experimental_partialOutputStream } = await streamText({\n  // ...\n  experimental_output: Output.object({\n    schema: z.object({\n      name: z.string(),\n      age: z.number().nullable().describe('Age of the person.'),\n      contact: z.object({\n        type: z.literal('email'),\n        value: z.string(),\n      }),\n      occupation: z.object({\n        type: z.literal('employed'),\n        company: z.string(),\n        position: z.string(),\n      }),\n    }),\n  }),\n  prompt: 'Generate an example person for testing.',\n});\nMore Examples\n\nYou can see generateObject and streamObject in action using various frameworks in the following examples:\n\ngenerateObject\nLearn to generate objects in Node.js\nLearn to generate objects in Next.js with Route Handlers (AI SDK UI)\nLearn to generate objects in Next.js with Server Actions (AI SDK RSC)\nstreamObject\nLearn to stream objects in Node.js\nLearn to stream objects in Next.js with Route Handlers (AI SDK UI)\nLearn to stream objects in Next.js with Server Actions (AI SDK RSC)\nPrevious\nGenerating Text\nNext\nTool Calling"
  },
  {
    "title": "AI SDK Core: Model Context Protocol (MCP) Tools",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools",
    "html": "AI SDK Core\nModel Context Protocol (MCP) Tools\nCopy markdown\nModel Context Protocol (MCP) Tools\n\nThe MCP tools feature is experimental and may change in the future.\n\nThe AI SDK supports connecting to Model Context Protocol (MCP)\n servers to access their tools. This enables your AI applications to discover and use tools across various services through a standardized interface.\n\nInitializing an MCP Client\n\nWe recommend using HTTP transport (like StreamableHTTPClientTransport) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.\n\nCreate an MCP client using one of the following transport options:\n\nHTTP transport (Recommended): Either configure HTTP directly via the client using transport: { type: 'http', ... }, or use MCP's official TypeScript SDK StreamableHTTPClientTransport\nSSE (Server-Sent Events): An alternative HTTP-based transport\nstdio: For local development only. Uses standard input/output streams for local MCP servers\nHTTP Transport (Recommended)\n\nFor production deployments, we recommend using the HTTP transport. You can configure it directly on the client:\n\nimport { experimental_createMCPClient as createMCPClient } from '@ai-sdk/mcp';\n\n\nconst mcpClient = await createMCPClient({\n  transport: {\n    type: 'http',\n    url: 'https://your-server.com/mcp',\n\n\n    // optional: configure HTTP headers\n    headers: { Authorization: 'Bearer my-api-key' },\n\n\n    // optional: provide an OAuth client provider for automatic authorization\n    authProvider: myOAuthClientProvider,\n  },\n});\n\nAlternatively, you can use StreamableHTTPClientTransport from MCP's official TypeScript SDK:\n\nimport { experimental_createMCPClient as createMCPClient } from '@ai-sdk/mcp';\nimport { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/streamableHttp.js';\n\n\nconst url = new URL('https://your-server.com/mcp');\nconst mcpClient = await createMCPClient({\n  transport: new StreamableHTTPClientTransport(url, {\n    sessionId: 'session_123',\n  }),\n});\nSSE Transport\n\nSSE provides an alternative HTTP-based transport option. Configure it with a type and url property. You can also provide an authProvider for OAuth:\n\nimport { experimental_createMCPClient as createMCPClient } from '@ai-sdk/mcp';\n\n\nconst mcpClient = await createMCPClient({\n  transport: {\n    type: 'sse',\n    url: 'https://my-server.com/sse',\n\n\n    // optional: configure HTTP headers\n    headers: { Authorization: 'Bearer my-api-key' },\n\n\n    // optional: provide an OAuth client provider for automatic authorization\n    authProvider: myOAuthClientProvider,\n  },\n});\nStdio Transport (Local Servers)\n\nThe stdio transport should only be used for local servers.\n\nThe Stdio transport can be imported from either the MCP SDK or the AI SDK:\n\nimport { experimental_createMCPClient as createMCPClient } from '@ai-sdk/mcp';\nimport { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';\n// Or use the AI SDK's stdio transport:\n// import { Experimental_StdioMCPTransport as StdioClientTransport } from '@ai-sdk/mcp/mcp-stdio';\n\n\nconst mcpClient = await createMCPClient({\n  transport: new StdioClientTransport({\n    command: 'node',\n    args: ['src/stdio/dist/server.js'],\n  }),\n});\nCustom Transport\n\nYou can also bring your own transport by implementing the MCPTransport interface for specific requirements not covered by the standard transports.\n\nThe client returned by the experimental_createMCPClient function is a lightweight client intended for use in tool conversion. It currently does not support all features of the full MCP client, such as: session management, resumable streams, and receiving notifications.\n\nAuthorization via OAuth is supported when using the AI SDK MCP HTTP or SSE transports by providing an authProvider.\n\nClosing the MCP Client\n\nAfter initialization, you should close the MCP client based on your usage pattern:\n\nFor short-lived usage (e.g., single requests), close the client when the response is finished\nFor long-running clients (e.g., command line apps), keep the client open but ensure it's closed when the application terminates\n\nWhen streaming responses, you can close the client when the LLM response has finished. For example, when using streamText, you should use the onFinish callback:\n\nconst mcpClient = await experimental_createMCPClient({\n  // ...\n});\n\n\nconst tools = await mcpClient.tools();\n\n\nconst result = await streamText({\n  model: 'openai/gpt-4.1',\n  tools,\n  prompt: 'What is the weather in Brooklyn, New York?',\n  onFinish: async () => {\n    await mcpClient.close();\n  },\n});\n\nWhen generating responses without streaming, you can use try/finally or cleanup functions in your framework:\n\nlet mcpClient: MCPClient | undefined;\n\n\ntry {\n  mcpClient = await experimental_createMCPClient({\n    // ...\n  });\n} finally {\n  await mcpClient?.close();\n}\nUsing MCP Tools\n\nThe client's tools method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:\n\nSchema Discovery\n\nWith schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:\n\nconst tools = await mcpClient.tools();\n\nThis approach is simpler to implement and automatically stays in sync with server changes. However, you won't have TypeScript type safety during development, and all tools from the server will be loaded\n\nSchema Definition\n\nFor better type safety and control, you can define the tools and their input schemas explicitly in your client code:\n\nimport { z } from 'zod';\n\n\nconst tools = await mcpClient.tools({\n  schemas: {\n    'get-data': {\n      inputSchema: z.object({\n        query: z.string().describe('The data query'),\n        format: z.enum(['json', 'text']).optional(),\n      }),\n    },\n    // For tools with zero inputs, you should use an empty object:\n    'tool-with-no-args': {\n      inputSchema: z.object({}),\n    },\n  },\n});\n\nThis approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define schemas, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs\n\nExamples\n\nYou can see MCP tools in action in the following example:\n\nLearn to use MCP tools in Node.js\nPrevious\nTool Calling\nNext\nPrompt Engineering"
  },
  {
    "title": "AI SDK Core: Tool Calling",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
    "html": "AI SDK Core\nTool Calling\nCopy markdown\nTool Calling\n\nAs covered under Foundations, tools are objects that can be called by the model to perform a specific task. AI SDK Core tools contain three elements:\n\ndescription: An optional description of the tool that can influence when the tool is picked.\ninputSchema: A Zod schema or a JSON schema that defines the input parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\nexecute: An optional async function that is called with the inputs from the tool call. It produces a value of type RESULT (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.\n\nYou can use the tool helper function to infer the types of the execute parameters.\n\nThe tools parameter of generateText and streamText is an object that has the tool names as keys and the tools as values:\n\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\n\nconst result = await generateText({\n  model: 'openai/gpt-4o',\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      inputSchema: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n\nWhen a model uses a tool, it is called a \"tool call\" and the output of the tool is called a \"tool result\".\n\nTool calling is not restricted to only text generation. You can also use it to render user interfaces (Generative UI).\n\nMulti-Step Calls (using stopWhen)\n\nWith the stopWhen setting, you can enable multi-step calls in generateText and streamText. When stopWhen is set and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the stopping condition is met.\n\nThe stopWhen conditions are only evaluated when the last step contains tool results.\n\nBy default, when you use generateText or streamText, it triggers a single generation. This works well for many use cases where you can rely on the model's training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, it's generation is complete and that step is finished.\n\nYou may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.\n\nYou can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model's training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.\n\nExample\n\nIn the following example, there are two steps:\n\nStep 1\nThe prompt 'What is the weather in San Francisco?' is sent to the model.\nThe model generates a tool call.\nThe tool call is executed.\nStep 2\nThe tool result is sent to the model.\nThe model generates a response considering the tool result.\nimport { z } from 'zod';\nimport { generateText, tool, stepCountIs } from 'ai';\n\n\nconst { text, steps } = await generateText({\n  model: 'openai/gpt-4o',\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      inputSchema: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  stopWhen: stepCountIs(5), // stop after a maximum of 5 steps if tools were called\n  prompt: 'What is the weather in San Francisco?',\n});\nYou can use streamText in a similar way.\nSteps\n\nTo access intermediate tool calls and results, you can use the steps property in the result object or the streamText onFinish callback. It contains all the text, tool calls, tool results, and more from each step.\n\nExample: Extract tool results from all steps\nimport { generateText } from 'ai';\n\n\nconst { steps } = await generateText({\n  model: openai('gpt-4o'),\n  stopWhen: stepCountIs(10),\n  // ...\n});\n\n\n// extract all tool calls from the steps:\nconst allToolCalls = steps.flatMap(step => step.toolCalls);\nonStepFinish callback\n\nWhen using generateText or streamText, you can provide an onStepFinish callback that is triggered when a step is finished, i.e. all text deltas, tool calls, and tool results for the step are available. When you have multiple steps, the callback is triggered for each step.\n\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  // ...\n  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {\n    // your own logic, e.g. for saving the chat history or recording usage\n  },\n});\nprepareStep callback\n\nThe prepareStep callback is called before a step is started.\n\nIt is called with the following parameters:\n\nmodel: The model that was passed into generateText.\nstopWhen: The stopping condition that was passed into generateText.\nstepNumber: The number of the step that is being executed.\nsteps: The steps that have been executed so far.\nmessages: The messages that will be sent to the model for the current step.\n\nYou can use it to provide different settings for a step, including modifying the input messages.\n\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  // ...\n  prepareStep: async ({ model, stepNumber, steps, messages }) => {\n    if (stepNumber === 0) {\n      return {\n        // use a different model for this step:\n        model: modelForThisParticularStep,\n        // force a tool choice for this step:\n        toolChoice: { type: 'tool', toolName: 'tool1' },\n        // limit the tools that are available for this step:\n        activeTools: ['tool1'],\n      };\n    }\n\n\n    // when nothing is returned, the default settings are used\n  },\n});\nMessage Modification for Longer Agentic Loops\n\nIn longer agentic loops, you can use the messages parameter to modify the input messages for each step. This is particularly useful for prompt compression:\n\nprepareStep: async ({ stepNumber, steps, messages }) => {\n  // Compress conversation history for longer loops\n  if (messages.length > 20) {\n    return {\n      messages: messages.slice(-10),\n    };\n  }\n\n\n  return {};\n},\nResponse Messages\n\nAdding the generated assistant and tool messages to your conversation history is a common task, especially if you are using multi-step tool calls.\n\nBoth generateText and streamText have a response.messages property that you can use to add the assistant and tool messages to your conversation history. It is also available in the onFinish callback of streamText.\n\nThe response.messages property contains an array of ModelMessage objects that you can add to your conversation history:\n\nimport { generateText, ModelMessage } from 'ai';\n\n\nconst messages: ModelMessage[] = [\n  // ...\n];\n\n\nconst { response } = await generateText({\n  // ...\n  messages,\n});\n\n\n// add the response messages to your conversation history:\nmessages.push(...response.messages); // streamText: ...((await response).messages)\nDynamic Tools\n\nAI SDK Core supports dynamic tools for scenarios where tool schemas are not known at compile time. This is useful for:\n\nMCP (Model Context Protocol) tools without schemas\nUser-defined functions at runtime\nTools loaded from external sources\nUsing dynamicTool\n\nThe dynamicTool helper creates tools with unknown input/output types:\n\nimport { dynamicTool } from 'ai';\nimport { z } from 'zod';\n\n\nconst customTool = dynamicTool({\n  description: 'Execute a custom function',\n  inputSchema: z.object({}),\n  execute: async input => {\n    // input is typed as 'unknown'\n    // You need to validate/cast it at runtime\n    const { action, parameters } = input as any;\n\n\n    // Execute your dynamic logic\n    return { result: `Executed ${action}` };\n  },\n});\nType-Safe Handling\n\nWhen using both static and dynamic tools, use the dynamic flag for type narrowing:\n\nconst result = await generateText({\n  model: 'openai/gpt-4o',\n  tools: {\n    // Static tool with known types\n    weather: weatherTool,\n    // Dynamic tool\n    custom: dynamicTool({\n      /* ... */\n    }),\n  },\n  onStepFinish: ({ toolCalls, toolResults }) => {\n    // Type-safe iteration\n    for (const toolCall of toolCalls) {\n      if (toolCall.dynamic) {\n        // Dynamic tool: input is 'unknown'\n        console.log('Dynamic:', toolCall.toolName, toolCall.input);\n        continue;\n      }\n\n\n      // Static tool: full type inference\n      switch (toolCall.toolName) {\n        case 'weather':\n          console.log(toolCall.input.location); // typed as string\n          break;\n      }\n    }\n  },\n});\nPreliminary Tool Results\n\nYou can return an AsyncIterable over multiple results. In this case, the last value from the iterable is the final tool result.\n\nThis can be used in combination with generator functions to e.g. stream status information during the tool execution:\n\ntool({\n  description: 'Get the current weather.',\n  inputSchema: z.object({\n    location: z.string(),\n  }),\n  async *execute({ location }) {\n    yield {\n      status: 'loading' as const,\n      text: `Getting weather for ${location}`,\n      weather: undefined,\n    };\n\n\n    await new Promise(resolve => setTimeout(resolve, 3000));\n\n\n    const temperature = 72 + Math.floor(Math.random() * 21) - 10;\n\n\n    yield {\n      status: 'success' as const,\n      text: `The weather in ${location} is ${temperature}°F`,\n      temperature,\n    };\n  },\n});\nTool Choice\n\nYou can use the toolChoice setting to influence when a tool is selected. It supports the following settings:\n\nauto (default): the model can choose whether and which tools to call.\nrequired: the model must call a tool. It can choose which tool to call.\nnone: the model must not call tools\n{ type: 'tool', toolName: string (typed) }: the model must call the specified tool\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\n\nconst result = await generateText({\n  model: 'openai/gpt-4o',\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      inputSchema: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n  },\n  toolChoice: 'required', // force the model to call a tool\n  prompt: 'What is the weather in San Francisco?',\n});\nTool Execution Options\n\nWhen tools are called, they receive additional options as a second parameter.\n\nTool Call ID\n\nThe ID of the tool call is forwarded to the tool execution. You can use it e.g. when sending tool-call related information with stream data.\n\nimport {\n  streamText,\n  tool,\n  createUIMessageStream,\n  createUIMessageStreamResponse,\n} from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n\n  const stream = createUIMessageStream({\n    execute: ({ writer }) => {\n      const result = streamText({\n        // ...\n        messages,\n        tools: {\n          myTool: tool({\n            // ...\n            execute: async (args, { toolCallId }) => {\n              // return e.g. custom status for tool call\n              writer.write({\n                type: 'data-tool-status',\n                id: toolCallId,\n                data: {\n                  name: 'myTool',\n                  status: 'in-progress',\n                },\n              });\n              // ...\n            },\n          }),\n        },\n      });\n\n\n      writer.merge(result.toUIMessageStream());\n    },\n  });\n\n\n  return createUIMessageStreamResponse({ stream });\n}\nMessages\n\nThe messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution. You can access them in the second parameter of the execute function. In multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.\n\nimport { generateText, tool } from 'ai';\n\n\nconst result = await generateText({\n  // ...\n  tools: {\n    myTool: tool({\n      // ...\n      execute: async (args, { messages }) => {\n        // use the message history in e.g. calls to other language models\n        return { ... };\n      },\n    }),\n  },\n});\nAbort Signals\n\nThe abort signals from generateText and streamText are forwarded to the tool execution. You can access them in the second parameter of the execute function and e.g. abort long-running computations or forward them to fetch calls inside tools.\n\nimport { z } from 'zod';\nimport { generateText, tool } from 'ai';\n\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  abortSignal: myAbortSignal, // signal that will be forwarded to tools\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      inputSchema: z.object({ location: z.string() }),\n      execute: async ({ location }, { abortSignal }) => {\n        return fetch(\n          `https://api.weatherapi.com/v1/current.json?q=${location}`,\n          { signal: abortSignal }, // forward the abort signal to fetch\n        );\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\nContext (experimental)\n\nYou can pass in arbitrary context from generateText or streamText via the experimental_context setting. This context is available in the experimental_context tool execution option.\n\nconst result = await generateText({\n  // ...\n  tools: {\n    someTool: tool({\n      // ...\n      execute: async (input, { experimental_context: context }) => {\n        const typedContext = context as { example: string }; // or use type validation library\n        // ...\n      },\n    }),\n  },\n  experimental_context: { example: '123' },\n});\nTypes\n\nModularizing your code often requires defining types to ensure type safety and reusability. To enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.\n\nYou can use them to strongly type your variables, function parameters, and return types in parts of the code that are not directly related to streamText or generateText.\n\nEach tool call is typed with ToolCall<NAME extends string, ARGS>, depending on the tool that has been invoked. Similarly, the tool results are typed with ToolResult<NAME extends string, ARGS, RESULT>.\n\nThe tools in streamText and generateText are defined as a ToolSet. The type inference helpers TypedToolCall<TOOLS extends ToolSet> and TypedToolResult<TOOLS extends ToolSet> can be used to extract the tool call and tool result types from the tools.\n\nimport { openai } from '@ai-sdk/openai';\nimport { TypedToolCall, TypedToolResult, generateText, tool } from 'ai';\nimport { z } from 'zod';\n\n\nconst myToolSet = {\n  firstTool: tool({\n    description: 'Greets the user',\n    inputSchema: z.object({ name: z.string() }),\n    execute: async ({ name }) => `Hello, ${name}!`,\n  }),\n  secondTool: tool({\n    description: 'Tells the user their age',\n    inputSchema: z.object({ age: z.number() }),\n    execute: async ({ age }) => `You are ${age} years old!`,\n  }),\n};\n\n\ntype MyToolCall = TypedToolCall<typeof myToolSet>;\ntype MyToolResult = TypedToolResult<typeof myToolSet>;\n\n\nasync function generateSomething(prompt: string): Promise<{\n  text: string;\n  toolCalls: Array<MyToolCall>; // typed tool calls\n  toolResults: Array<MyToolResult>; // typed tool results\n}> {\n  return generateText({\n    model: openai('gpt-4.1'),\n    tools: myToolSet,\n    prompt,\n  });\n}\nHandling Errors\n\nThe AI SDK has three tool-call related errors:\n\nNoSuchToolError: the model tries to call a tool that is not defined in the tools object\nInvalidToolInputError: the model calls a tool with inputs that do not match the tool's input schema\nToolCallRepairError: an error that occurred during tool call repair\n\nWhen tool execution fails (errors thrown by your tool's execute function), the AI SDK adds them as tool-error content parts to enable automated LLM roundtrips in multi-step scenarios.\n\ngenerateText\n\ngenerateText throws errors for tool schema validation issues and other errors, and can be handled using a try/catch block. Tool execution errors appear as tool-error parts in the result steps:\n\ntry {\n  const result = await generateText({\n    //...\n  });\n} catch (error) {\n  if (NoSuchToolError.isInstance(error)) {\n    // handle the no such tool error\n  } else if (InvalidToolInputError.isInstance(error)) {\n    // handle the invalid tool inputs error\n  } else {\n    // handle other errors\n  }\n}\n\nTool execution errors are available in the result steps:\n\nconst { steps } = await generateText({\n  // ...\n});\n\n\n// check for tool errors in the steps\nconst toolErrors = steps.flatMap(step =>\n  step.content.filter(part => part.type === 'tool-error'),\n);\n\n\ntoolErrors.forEach(toolError => {\n  console.log('Tool error:', toolError.error);\n  console.log('Tool name:', toolError.toolName);\n  console.log('Tool input:', toolError.input);\n});\nstreamText\n\nstreamText sends errors as part of the full stream. Tool execution errors appear as tool-error parts, while other errors appear as error parts.\n\nWhen using toUIMessageStreamResponse, you can pass an onError function to extract the error message from the error part and forward it as part of the stream response:\n\nconst result = streamText({\n  // ...\n});\n\n\nreturn result.toUIMessageStreamResponse({\n  onError: error => {\n    if (NoSuchToolError.isInstance(error)) {\n      return 'The model tried to call a unknown tool.';\n    } else if (InvalidToolInputError.isInstance(error)) {\n      return 'The model called a tool with invalid inputs.';\n    } else {\n      return 'An unknown error occurred.';\n    }\n  },\n});\nTool Call Repair\n\nThe tool call repair feature is experimental and may change in the future.\n\nLanguage models sometimes fail to generate valid tool calls, especially when the input schema is complex or the model is smaller.\n\nIf you use multiple steps, those failed tool calls will be sent back to the LLM in the next step to give it an opportunity to fix it. However, you may want to control how invalid tool calls are repaired without requiring additional steps that pollute the message history.\n\nYou can use the experimental_repairToolCall function to attempt to repair the tool call with a custom function.\n\nYou can use different strategies to repair the tool call:\n\nUse a model with structured outputs to generate the inputs.\nSend the messages, system prompt, and tool schema to a stronger model to generate the inputs.\nProvide more specific repair instructions based on which tool was called.\nExample: Use a model with structured outputs for repair\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject, generateText, NoSuchToolError, tool } from 'ai';\n\n\nconst result = await generateText({\n  model,\n  tools,\n  prompt,\n\n\n  experimental_repairToolCall: async ({\n    toolCall,\n    tools,\n    inputSchema,\n    error,\n  }) => {\n    if (NoSuchToolError.isInstance(error)) {\n      return null; // do not attempt to fix invalid tool names\n    }\n\n\n    const tool = tools[toolCall.toolName as keyof typeof tools];\n\n\n    const { object: repairedArgs } = await generateObject({\n      model: openai('gpt-4.1'),\n      schema: tool.inputSchema,\n      prompt: [\n        `The model tried to call the tool \"${toolCall.toolName}\"` +\n          ` with the following inputs:`,\n        JSON.stringify(toolCall.input),\n        `The tool accepts the following schema:`,\n        JSON.stringify(inputSchema(toolCall)),\n        'Please fix the inputs.',\n      ].join('\\n'),\n    });\n\n\n    return { ...toolCall, input: JSON.stringify(repairedArgs) };\n  },\n});\nExample: Use the re-ask strategy for repair\nimport { openai } from '@ai-sdk/openai';\nimport { generateObject, generateText, NoSuchToolError, tool } from 'ai';\n\n\nconst result = await generateText({\n  model,\n  tools,\n  prompt,\n\n\n  experimental_repairToolCall: async ({\n    toolCall,\n    tools,\n    error,\n    messages,\n    system,\n  }) => {\n    const result = await generateText({\n      model,\n      system,\n      messages: [\n        ...messages,\n        {\n          role: 'assistant',\n          content: [\n            {\n              type: 'tool-call',\n              toolCallId: toolCall.toolCallId,\n              toolName: toolCall.toolName,\n              input: toolCall.input,\n            },\n          ],\n        },\n        {\n          role: 'tool' as const,\n          content: [\n            {\n              type: 'tool-result',\n              toolCallId: toolCall.toolCallId,\n              toolName: toolCall.toolName,\n              output: error.message,\n            },\n          ],\n        },\n      ],\n      tools,\n    });\n\n\n    const newToolCall = result.toolCalls.find(\n      newToolCall => newToolCall.toolName === toolCall.toolName,\n    );\n\n\n    return newToolCall != null\n      ? {\n          toolCallType: 'function' as const,\n          toolCallId: toolCall.toolCallId,\n          toolName: toolCall.toolName,\n          input: JSON.stringify(newToolCall.input),\n        }\n      : null;\n  },\n});\nActive Tools\n\nLanguage models can only handle a limited number of tools at a time, depending on the model. To allow for static typing using a large number of tools and limiting the available tools to the model at the same time, the AI SDK provides the activeTools property.\n\nIt is an array of tool names that are currently active. By default, the value is undefined and all tools are active.\n\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\n\nconst { text } = await generateText({\n  model: openai('gpt-4.1'),\n  tools: myToolSet,\n  activeTools: ['firstTool'],\n});\nMulti-modal Tool Results\n\nMulti-modal tool results are experimental and only supported by Anthropic and OpenAI.\n\nIn order to send multi-modal tool results, e.g. screenshots, back to the model, they need to be converted into a specific format.\n\nAI SDK Core tools have an optional toModelOutput function that converts the tool result into a content part.\n\nHere is an example for converting a screenshot into a content part:\n\nconst result = await generateText({\n  model: anthropic('claude-3-5-sonnet-20241022'),\n  tools: {\n    computer: anthropic.tools.computer_20241022({\n      // ...\n      async execute({ action, coordinate, text }) {\n        switch (action) {\n          case 'screenshot': {\n            return {\n              type: 'image',\n              data: fs\n                .readFileSync('./data/screenshot-editor.png')\n                .toString('base64'),\n            };\n          }\n          default: {\n            return `executed ${action}`;\n          }\n        }\n      },\n\n\n      // map to tool result content for LLM consumption:\n      toModelOutput(result) {\n        return {\n          type: 'content',\n          value:\n            typeof result === 'string'\n              ? [{ type: 'text', text: result }]\n              : [{ type: 'media', data: result.data, mediaType: 'image/png' }],\n        };\n      },\n    }),\n  },\n  // ...\n});\nExtracting Tools\n\nOnce you start having many tools, you might want to extract them into separate files. The tool helper function is crucial for this, because it ensures correct type inference.\n\nHere is an example of an extracted tool:\n\ntools/weather-tool.ts\nimport { tool } from 'ai';\nimport { z } from 'zod';\n\n\n// the `tool` helper function ensures correct type inference:\nexport const weatherTool = tool({\n  description: 'Get the weather in a location',\n  inputSchema: z.object({\n    location: z.string().describe('The location to get the weather for'),\n  }),\n  execute: async ({ location }) => ({\n    location,\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\n  }),\n});\nMCP Tools\n\nThe AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools. MCP enables your AI applications to discover and use tools across various services through a standardized interface.\n\nFor detailed information about MCP tools, including initialization, transport options, and usage patterns, see the MCP Tools documentation.\n\nAI SDK Tools vs MCP Tools\n\nIn most cases, you should define your own AI SDK tools for production applications. They provide full control, type safety, and optimal performance. MCP tools are best suited for rapid development iteration and scenarios where users bring their own tools.\n\nAspect\tAI SDK Tools\tMCP Tools\nType Safety\tFull static typing end-to-end\tDynamic discovery at runtime\nExecution\tSame process as your request (low latency)\tSeparate server (network overhead)\nPrompt Control\tFull control over descriptions and schemas\tControlled by MCP server owner\nSchema Control\tYou define and optimize for your model\tControlled by MCP server owner\nVersion Management\tFull visibility over updates\tCan update independently (version skew risk)\nAuthentication\tSame process, no additional auth required\tSeparate server introduces additional auth complexity\nBest For\tProduction applications requiring control and performance\tDevelopment iteration, user-provided tools\nExamples\n\nYou can see tools in action using various frameworks in the following examples:\n\nLearn to use tools in Node.js\nLearn to use tools in Next.js with Route Handlers\nLearn to use MCP tools in Node.js\nPrevious\nGenerating Structured Data\nNext\nModel Context Protocol (MCP) Tools"
  },
  {
    "title": "AI SDK Core: Prompt Engineering",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering",
    "html": "AI SDK Core\nPrompt Engineering\nCopy markdown\nPrompt Engineering\nTips\nPrompts for Tools\n\nWhen you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.\n\nHere are a few tips to help you get the best results:\n\nUse a model that is strong at tool calling, such as gpt-5 or gpt-4.1. Weaker models will often struggle to call tools effectively and flawlessly.\nKeep the number of tools low, e.g. to 5 or less.\nKeep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.\nUse semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.\nAdd .describe(\"...\") to your Zod schema properties to give the model hints about what a particular property is for.\nWhen the output of a tool might be unclear to the model and there are dependencies between tools, use the description field of a tool to provide information about the output of the tool execution.\nYou can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.\n\nIn general, the goal should be to give the model all information it needs in a clear way.\n\nTool & Structured Data Schemas\n\nThe mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.\n\nZod Dates\n\nZod expects JavaScript Date objects, but models return dates as strings. You can specify and validate the date format using z.string().datetime() or z.string().date(), and then use a Zod transformer to convert the string to a Date object.\n\nconst result = await generateObject({\n  model: openai('gpt-4.1'),\n  schema: z.object({\n    events: z.array(\n      z.object({\n        event: z.string(),\n        date: z\n          .string()\n          .date()\n          .transform(value => new Date(value)),\n      }),\n    ),\n  }),\n  prompt: 'List 5 important events from the year 2000.',\n});\nOptional Parameters\n\nWhen working with tools that have optional parameters, you may encounter compatibility issues with certain providers that use strict schema validation.\n\nThis is particularly relevant for OpenAI models with structured outputs (strict mode).\n\nFor maximum compatibility, optional parameters should use .nullable() instead of .optional():\n\n// This may fail with strict schema validation\nconst failingTool = tool({\n  description: 'Execute a command',\n  inputSchema: z.object({\n    command: z.string(),\n    workdir: z.string().optional(), // This can cause errors\n    timeout: z.string().optional(),\n  }),\n});\n\n\n// This works with strict schema validation\nconst workingTool = tool({\n  description: 'Execute a command',\n  inputSchema: z.object({\n    command: z.string(),\n    workdir: z.string().nullable(), // Use nullable instead\n    timeout: z.string().nullable(),\n  }),\n});\nTemperature Settings\n\nFor tool calls and object generation, it's recommended to use temperature: 0 to ensure deterministic and consistent results:\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  temperature: 0, // Recommended for tool calls\n  tools: {\n    myTool: tool({\n      description: 'Execute a command',\n      inputSchema: z.object({\n        command: z.string(),\n      }),\n    }),\n  },\n  prompt: 'Execute the ls command',\n});\n\nLower temperature values reduce randomness in model outputs, which is particularly important when the model needs to:\n\nGenerate structured data with specific formats\nMake precise tool calls with correct parameters\nFollow strict schemas consistently\nDebugging\nInspecting Warnings\n\nNot all providers support all AI SDK features. Providers either throw exceptions or return warnings when they do not support a feature. To check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Hello, world!',\n});\n\n\nconsole.log(result.warnings);\nHTTP Request Bodies\n\nYou can inspect the raw HTTP request bodies for models that expose them, e.g. OpenAI. This allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.\n\nRequest bodies are available via the request.body property of the response:\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Hello, world!',\n});\n\n\nconsole.log(result.request.body);\nPrevious\nModel Context Protocol (MCP) Tools\nNext\nSettings"
  },
  {
    "title": "AI SDK Core: Settings",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/settings",
    "html": "AI SDK Core\nSettings\nCopy markdown\nSettings\n\nLarge language models (LLMs) typically provide settings to augment their output.\n\nAll AI SDK functions support the following common settings in addition to the model, the prompt, and additional provider-specific settings:\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  maxOutputTokens: 512,\n  temperature: 0.3,\n  maxRetries: 5,\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nSome providers do not support all common settings. If you use a setting with a provider that does not support it, a warning will be generated. You can check the warnings property in the result object to see if any warnings were generated.\n\nmaxOutputTokens\n\nMaximum number of tokens to generate.\n\ntemperature\n\nTemperature setting.\n\nThe value is passed through to the provider. The range depends on the provider and model. For most providers, 0 means almost deterministic results, and higher values mean more randomness.\n\nIt is recommended to set either temperature or topP, but not both.\n\nIn AI SDK 5.0, temperature is no longer set to 0 by default.\ntopP\n\nNucleus sampling.\n\nThe value is passed through to the provider. The range depends on the provider and model. For most providers, nucleus sampling is a number between 0 and 1. E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.\n\nIt is recommended to set either temperature or topP, but not both.\n\ntopK\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\n\npresencePenalty\n\nThe presence penalty affects the likelihood of the model to repeat information that is already in the prompt.\n\nThe value is passed through to the provider. The range depends on the provider and model. For most providers, 0 means no penalty.\n\nfrequencyPenalty\n\nThe frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.\n\nThe value is passed through to the provider. The range depends on the provider and model. For most providers, 0 means no penalty.\n\nstopSequences\n\nThe stop sequences to use for stopping the text generation.\n\nIf set, the model will stop generating text when one of the stop sequences is generated. Providers may have limits on the number of stop sequences.\n\nseed\n\nIt is the seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\n\nmaxRetries\n\nMaximum number of retries. Set to 0 to disable retries. Default: 2.\n\nabortSignal\n\nAn optional abort signal that can be used to cancel the call.\n\nThe abort signal can e.g. be forwarded from a user interface to cancel the call, or to define a timeout.\n\nExample: Timeout\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  abortSignal: AbortSignal.timeout(5000), // 5 seconds\n});\nheaders\n\nAdditional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\nYou can use the request headers to provide additional information to the provider, depending on what the provider supports. For example, some observability providers support headers such as Prompt-Id.\n\nimport { generateText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  headers: {\n    'Prompt-Id': 'my-prompt-id',\n  },\n});\n\nThe headers setting is for request-specific headers. You can also set headers in the provider configuration. These headers will be sent with every request made by the provider.\n\nPrevious\nPrompt Engineering\nNext\nEmbeddings"
  },
  {
    "title": "AI SDK Core: Embeddings",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings",
    "html": "AI SDK Core\nEmbeddings\nCopy markdown\nEmbeddings\n\nEmbeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space. In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.\n\nEmbedding a Single Value\n\nThe AI SDK provides the embed function to embed single values, which is useful for tasks such as finding similar words or phrases or clustering text. You can use it with embeddings models, e.g. openai.textEmbeddingModel('text-embedding-3-large') or mistral.textEmbeddingModel('mistral-embed').\n\nimport { embed } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\n// 'embedding' is a single embedding object (number[])\nconst { embedding } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\nEmbedding Many Values\n\nWhen loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG), it is often useful to embed many values at once (batch embedding).\n\nThe AI SDK provides the embedMany function for this purpose. Similar to embed, you can use it with embeddings models, e.g. openai.textEmbeddingModel('text-embedding-3-large') or mistral.textEmbeddingModel('mistral-embed').\n\nimport { openai } from '@ai-sdk/openai';\nimport { embedMany } from 'ai';\n\n\n// 'embeddings' is an array of embedding objects (number[][]).\n// It is sorted in the same order as the input values.\nconst { embeddings } = await embedMany({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  values: [\n    'sunny day at the beach',\n    'rainy afternoon in the city',\n    'snowy night in the mountains',\n  ],\n});\nEmbedding Similarity\n\nAfter embedding values, you can calculate the similarity between them using the cosineSimilarity function. This is useful to e.g. find similar words or phrases in a dataset. You can also rank and filter related items based on their similarity.\n\nimport { openai } from '@ai-sdk/openai';\nimport { cosineSimilarity, embedMany } from 'ai';\n\n\nconst { embeddings } = await embedMany({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  values: ['sunny day at the beach', 'rainy afternoon in the city'],\n});\n\n\nconsole.log(\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\n);\nToken Usage\n\nMany providers charge based on the number of tokens used to generate embeddings. Both embed and embedMany provide token usage information in the usage property of the result object:\n\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\n\nconst { embedding, usage } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n\n\nconsole.log(usage); // { tokens: 10 }\nSettings\nProvider Options\n\nEmbedding model settings can be configured using providerOptions for provider-specific parameters:\n\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\n\nconst { embedding } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  providerOptions: {\n    openai: {\n      dimensions: 512, // Reduce embedding dimensions\n    },\n  },\n});\nParallel Requests\n\nThe embedMany function now supports parallel processing with configurable maxParallelCalls to optimize performance:\n\nimport { openai } from '@ai-sdk/openai';\nimport { embedMany } from 'ai';\n\n\nconst { embeddings, usage } = await embedMany({\n  maxParallelCalls: 2, // Limit parallel requests\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  values: [\n    'sunny day at the beach',\n    'rainy afternoon in the city',\n    'snowy night in the mountains',\n  ],\n});\nRetries\n\nBoth embed and embedMany accept an optional maxRetries parameter of type number that you can use to set the maximum number of retries for the embedding process. It defaults to 2 retries (3 attempts in total). You can set it to 0 to disable retries.\n\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\n\nconst { embedding } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  maxRetries: 0, // Disable retries\n});\nAbort Signals and Timeouts\n\nBoth embed and embedMany accept an optional abortSignal parameter of type AbortSignal\n that you can use to abort the embedding process or set a timeout.\n\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\n\nconst { embedding } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\nCustom Headers\n\nBoth embed and embedMany accept an optional headers parameter of type Record<string, string> that you can use to add custom headers to the embedding request.\n\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\n\nconst { embedding } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\nResponse Information\n\nBoth embed and embedMany return response information that includes the raw provider response:\n\nimport { openai } from '@ai-sdk/openai';\nimport { embed } from 'ai';\n\n\nconst { embedding, response } = await embed({\n  model: openai.textEmbeddingModel('text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\n\n\nconsole.log(response); // Raw provider response\nEmbedding Providers & Models\n\nSeveral providers offer embedding models:\n\nProvider\tModel\tEmbedding Dimensions\nOpenAI\ttext-embedding-3-large\t3072\nOpenAI\ttext-embedding-3-small\t1536\nOpenAI\ttext-embedding-ada-002\t1536\nGoogle Generative AI\tgemini-embedding-001\t3072\nGoogle Generative AI\ttext-embedding-004\t768\nMistral\tmistral-embed\t1024\nCohere\tembed-english-v3.0\t1024\nCohere\tembed-multilingual-v3.0\t1024\nCohere\tembed-english-light-v3.0\t384\nCohere\tembed-multilingual-light-v3.0\t384\nCohere\tembed-english-v2.0\t4096\nCohere\tembed-english-light-v2.0\t1024\nCohere\tembed-multilingual-v2.0\t768\nAmazon Bedrock\tamazon.titan-embed-text-v1\t1536\nAmazon Bedrock\tamazon.titan-embed-text-v2:0\t1024\nPrevious\nSettings\nNext\nImage Generation"
  },
  {
    "title": "AI SDK Core: Image Generation",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation",
    "html": "AI SDK Core\nImage Generation\nCopy markdown\nImage Generation\nImage generation is an experimental feature.\n\nThe AI SDK provides the generateImage function to generate images based on a given prompt using an image model.\n\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n});\n\nYou can access the image data using the base64 or uint8Array properties:\n\nconst base64 = image.base64; // base64 image data\nconst uint8Array = image.uint8Array; // Uint8Array image data\nSettings\nSize and Aspect Ratio\n\nDepending on the model, you can either specify the size or the aspect ratio.\n\nSize\n\nThe size is specified as a string in the format {width}x{height}. Models only support a few sizes, and the supported sizes are different for each model and provider.\n\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  size: '1024x1024',\n});\nAspect Ratio\n\nThe aspect ratio is specified as a string in the format {width}:{height}. Models only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.\n\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { vertex } from '@ai-sdk/google-vertex';\n\n\nconst { image } = await generateImage({\n  model: vertex.image('imagen-3.0-generate-002'),\n  prompt: 'Santa Claus driving a Cadillac',\n  aspectRatio: '16:9',\n});\nGenerating Multiple Images\n\ngenerateImage also supports generating multiple images at once:\n\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst { images } = await generateImage({\n  model: openai.image('dall-e-2'),\n  prompt: 'Santa Claus driving a Cadillac',\n  n: 4, // number of images to generate\n});\n\ngenerateImage will automatically call the model as often as needed (in parallel) to generate the requested number of images.\n\nEach image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the n parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).\n\nIf needed, you can override this behavior using the maxImagesPerCall setting when generating your image. This is particularly useful when working with new or custom models where the default batch size might not be optimal:\n\nconst { images } = await generateImage({\n  model: openai.image('dall-e-2'),\n  prompt: 'Santa Claus driving a Cadillac',\n  maxImagesPerCall: 5, // Override the default batch size\n  n: 10, // Will make 2 calls of 5 images each\n});\nProviding a Seed\n\nYou can provide a seed to the generateImage function to control the output of the image generation process. If supported by the model, the same seed will always produce the same image.\n\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  seed: 1234567890,\n});\nProvider-specific Settings\n\nImage models often have provider- or even model-specific settings. You can pass such settings to the generateImage function using the providerOptions parameter. The options for the provider (openai in the example below) become request body properties.\n\nimport { experimental_generateImage as generateImage } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  size: '1024x1024',\n  providerOptions: {\n    openai: { style: 'vivid', quality: 'hd' },\n  },\n});\nAbort Signals and Timeouts\n\ngenerateImage accepts an optional abortSignal parameter of type AbortSignal\n that you can use to abort the image generation process or set a timeout.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\nCustom Headers\n\ngenerateImage accepts an optional headers parameter of type Record<string, string> that you can use to add custom headers to the image generation request.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateImage as generateImage } from 'ai';\n\n\nconst { image } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\nWarnings\n\nIf the model returns warnings, e.g. for unsupported parameters, they will be available in the warnings property of the response.\n\nconst { image, warnings } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt: 'Santa Claus driving a Cadillac',\n});\nAdditional provider-specific meta data\n\nSome providers expose additional meta data for the result overall or per image.\n\nconst prompt = 'Santa Claus driving a Cadillac';\n\n\nconst { image, providerMetadata } = await generateImage({\n  model: openai.image('dall-e-3'),\n  prompt,\n});\n\n\nconst revisedPrompt = providerMetadata.openai.images[0]?.revisedPrompt;\n\n\nconsole.log({\n  prompt,\n  revisedPrompt,\n});\n\nThe outer key of the returned providerMetadata is the provider name. The inner values are the metadata. An images key is always present in the metadata and is an array with the same length as the top level images key.\n\nError Handling\n\nWhen generateImage cannot generate a valid image, it throws a AI_NoImageGeneratedError.\n\nThis error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:\n\nThe model failed to generate a response\nThe model generated a response that could not be parsed\n\nThe error preserves the following information to help you log the issue:\n\nresponses: Metadata about the image model responses, including timestamp, model, and headers.\ncause: The cause of the error. You can use this for more detailed error handling\nimport { generateImage, NoImageGeneratedError } from 'ai';\n\n\ntry {\n  await generateImage({ model, prompt });\n} catch (error) {\n  if (NoImageGeneratedError.isInstance(error)) {\n    console.log('NoImageGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Responses:', error.responses);\n  }\n}\nGenerating Images with Language Models\n\nSome language models such as Google gemini-2.5-flash-image-preview support multi-modal outputs including images. With such models, you can access the generated images using the files property of the response.\n\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  model: google('gemini-2.5-flash-image-preview'),\n  prompt: 'Generate an image of a comic cat',\n});\n\n\nfor (const file of result.files) {\n  if (file.mediaType.startsWith('image/')) {\n    // The file object provides multiple data formats:\n    // Access images as base64 string, Uint8Array binary data, or check type\n    // - file.base64: string (data URL format)\n    // - file.uint8Array: Uint8Array (binary data)\n    // - file.mediaType: string (e.g. \"image/png\")\n  }\n}\nImage Models\nProvider\tModel\tSupport sizes (width x height) or aspect ratios (width : height)\nxAI Grok\tgrok-2-image\t1024x768 (default)\nOpenAI\tgpt-image-1\t1024x1024, 1536x1024, 1024x1536\nOpenAI\tdall-e-3\t1024x1024, 1792x1024, 1024x1792\nOpenAI\tdall-e-2\t256x256, 512x512, 1024x1024\nAmazon Bedrock\tamazon.nova-canvas-v1:0\t320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels\nFal\tfal-ai/flux/dev\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/flux-lora\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/fast-sdxl\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/flux-pro/v1.1-ultra\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/ideogram/v2\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/recraft-v3\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/stable-diffusion-3.5-large\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nFal\tfal-ai/hyper-sdxl\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nDeepInfra\tstabilityai/sd3.5\t1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21\nDeepInfra\tblack-forest-labs/FLUX-1.1-pro\t256-1440 (multiples of 32)\nDeepInfra\tblack-forest-labs/FLUX-1-schnell\t256-1440 (multiples of 32)\nDeepInfra\tblack-forest-labs/FLUX-1-dev\t256-1440 (multiples of 32)\nDeepInfra\tblack-forest-labs/FLUX-pro\t256-1440 (multiples of 32)\nDeepInfra\tstabilityai/sd3.5-medium\t1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21\nDeepInfra\tstabilityai/sdxl-turbo\t1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21\nReplicate\tblack-forest-labs/flux-schnell\t1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9\nReplicate\trecraft-ai/recraft-v3\t1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024\nGoogle\timagen-3.0-generate-002\t1:1, 3:4, 4:3, 9:16, 16:9\nGoogle Vertex\timagen-3.0-generate-002\t1:1, 3:4, 4:3, 9:16, 16:9\nGoogle Vertex\timagen-3.0-fast-generate-001\t1:1, 3:4, 4:3, 9:16, 16:9\nFireworks\taccounts/fireworks/models/flux-1-dev-fp8\t1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9\nFireworks\taccounts/fireworks/models/flux-1-schnell-fp8\t1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9\nFireworks\taccounts/fireworks/models/playground-v2-5-1024px-aesthetic\t640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640\nFireworks\taccounts/fireworks/models/japanese-stable-diffusion-xl\t640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640\nFireworks\taccounts/fireworks/models/playground-v2-1024px-aesthetic\t640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640\nFireworks\taccounts/fireworks/models/SSD-1B\t640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640\nFireworks\taccounts/fireworks/models/stable-diffusion-xl-1024-v1-0\t640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640\nLuma\tphoton-1\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nLuma\tphoton-flash-1\t1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9\nTogether.ai\tstabilityai/stable-diffusion-xl-base-1.0\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-dev\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-dev-lora\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-schnell\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-canny\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-depth\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-redux\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1.1-pro\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-pro\t512x512, 768x768, 1024x1024\nTogether.ai\tblack-forest-labs/FLUX.1-schnell-Free\t512x512, 768x768, 1024x1024\n\nAbove are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.\n\nPrevious\nEmbeddings\nNext\nTranscription"
  },
  {
    "title": "AI SDK Core: Transcription",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/transcription",
    "html": "AI SDK Core\nTranscription\nCopy markdown\nTranscription\nTranscription is an experimental feature.\n\nThe AI SDK provides the transcribe function to transcribe audio using a transcription model.\n\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n});\n\nThe audio property can be a Uint8Array, ArrayBuffer, Buffer, string (base64 encoded audio data), or a URL.\n\nTo access the generated transcript:\n\nconst text = transcript.text; // transcript text e.g. \"Hello, world!\"\nconst segments = transcript.segments; // array of segments with start and end times, if available\nconst language = transcript.language; // language of the transcript e.g. \"en\", if available\nconst durationInSeconds = transcript.durationInSeconds; // duration of the transcript in seconds, if available\nSettings\nProvider-Specific settings\n\nTranscription models often have provider or model-specific settings which you can set using the providerOptions parameter.\n\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  providerOptions: {\n    openai: {\n      timestampGranularities: ['word'],\n    },\n  },\n});\nAbort Signals and Timeouts\n\ntranscribe accepts an optional abortSignal parameter of type AbortSignal\n that you can use to abort the transcription process or set a timeout.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { readFile } from 'fs/promises';\n\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\nCustom Headers\n\ntranscribe accepts an optional headers parameter of type Record<string, string> that you can use to add custom headers to the transcription request.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { readFile } from 'fs/promises';\n\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\nWarnings\n\nWarnings (e.g. unsupported parameters) are available on the warnings property.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_transcribe as transcribe } from 'ai';\nimport { readFile } from 'fs/promises';\n\n\nconst transcript = await transcribe({\n  model: openai.transcription('whisper-1'),\n  audio: await readFile('audio.mp3'),\n});\n\n\nconst warnings = transcript.warnings;\nError Handling\n\nWhen transcribe cannot generate a valid transcript, it throws a AI_NoTranscriptGeneratedError.\n\nThis error can arise for any the following reasons:\n\nThe model failed to generate a response\nThe model generated a response that could not be parsed\n\nThe error preserves the following information to help you log the issue:\n\nresponses: Metadata about the transcription model responses, including timestamp, model, and headers.\ncause: The cause of the error. You can use this for more detailed error handling.\nimport {\n  experimental_transcribe as transcribe,\n  NoTranscriptGeneratedError,\n} from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { readFile } from 'fs/promises';\n\n\ntry {\n  await transcribe({\n    model: openai.transcription('whisper-1'),\n    audio: await readFile('audio.mp3'),\n  });\n} catch (error) {\n  if (NoTranscriptGeneratedError.isInstance(error)) {\n    console.log('NoTranscriptGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Responses:', error.responses);\n  }\n}\nTranscription Models\nProvider\tModel\nOpenAI\twhisper-1\nOpenAI\tgpt-4o-transcribe\nOpenAI\tgpt-4o-mini-transcribe\nElevenLabs\tscribe_v1\nElevenLabs\tscribe_v1_experimental\nGroq\twhisper-large-v3-turbo\nGroq\tdistil-whisper-large-v3-en\nGroq\twhisper-large-v3\nAzure OpenAI\twhisper-1\nAzure OpenAI\tgpt-4o-transcribe\nAzure OpenAI\tgpt-4o-mini-transcribe\nRev.ai\tmachine\nRev.ai\tlow_cost\nRev.ai\tfusion\nDeepgram\tbase (+ variants)\nDeepgram\tenhanced (+ variants)\nDeepgram\tnova (+ variants)\nDeepgram\tnova-2 (+ variants)\nDeepgram\tnova-3 (+ variants)\nGladia\tdefault\nAssemblyAI\tbest\nAssemblyAI\tnano\nFal\twhisper\nFal\twizper\n\nAbove are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.\n\nPrevious\nImage Generation\nNext\nSpeech"
  },
  {
    "title": "AI SDK Core: Speech",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/speech",
    "html": "AI SDK Core\nSpeech\nCopy markdown\nSpeech\nSpeech is an experimental feature.\n\nThe AI SDK provides the generateSpeech function to generate speech from text using a speech model.\n\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  voice: 'alloy',\n});\nLanguage Setting\n\nYou can specify the language for speech generation (provider support varies):\n\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { lmnt } from '@ai-sdk/lmnt';\n\n\nconst audio = await generateSpeech({\n  model: lmnt.speech('aurora'),\n  text: 'Hola, mundo!',\n  language: 'es', // Spanish\n});\n\nTo access the generated audio:\n\nconst audio = audio.audioData; // audio data e.g. Uint8Array\nSettings\nProvider-Specific settings\n\nYou can set model-specific settings with the providerOptions parameter.\n\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  providerOptions: {\n    openai: {\n      // ...\n    },\n  },\n});\nAbort Signals and Timeouts\n\ngenerateSpeech accepts an optional abortSignal parameter of type AbortSignal\n that you can use to abort the speech generation process or set a timeout.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\n\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n});\nCustom Headers\n\ngenerateSpeech accepts an optional headers parameter of type Record<string, string> that you can use to add custom headers to the speech generation request.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\n\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n  headers: { 'X-Custom-Header': 'custom-value' },\n});\nWarnings\n\nWarnings (e.g. unsupported parameters) are available on the warnings property.\n\nimport { openai } from '@ai-sdk/openai';\nimport { experimental_generateSpeech as generateSpeech } from 'ai';\n\n\nconst audio = await generateSpeech({\n  model: openai.speech('tts-1'),\n  text: 'Hello, world!',\n});\n\n\nconst warnings = audio.warnings;\nError Handling\n\nWhen generateSpeech cannot generate a valid audio, it throws a AI_NoSpeechGeneratedError.\n\nThis error can arise for any the following reasons:\n\nThe model failed to generate a response\nThe model generated a response that could not be parsed\n\nThe error preserves the following information to help you log the issue:\n\nresponses: Metadata about the speech model responses, including timestamp, model, and headers.\ncause: The cause of the error. You can use this for more detailed error handling.\nimport {\n  experimental_generateSpeech as generateSpeech,\n  NoSpeechGeneratedError,\n} from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\ntry {\n  await generateSpeech({\n    model: openai.speech('tts-1'),\n    text: 'Hello, world!',\n  });\n} catch (error) {\n  if (NoSpeechGeneratedError.isInstance(error)) {\n    console.log('AI_NoSpeechGeneratedError');\n    console.log('Cause:', error.cause);\n    console.log('Responses:', error.responses);\n  }\n}\nSpeech Models\nProvider\tModel\nOpenAI\ttts-1\nOpenAI\ttts-1-hd\nOpenAI\tgpt-4o-mini-tts\nElevenLabs\televen_v3\nElevenLabs\televen_multilingual_v2\nElevenLabs\televen_flash_v2_5\nElevenLabs\televen_flash_v2\nElevenLabs\televen_turbo_v2_5\nElevenLabs\televen_turbo_v2\nLMNT\taurora\nLMNT\tblizzard\nHume\tdefault\n\nAbove are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.\n\nPrevious\nTranscription\nNext\nLanguage Model Middleware"
  },
  {
    "title": "AI SDK Core: Provider & Model Management",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management",
    "html": "AI SDK Core\nProvider & Model Management\nCopy markdown\nProvider & Model Management\n\nWhen you work with multiple providers and models, it is often desirable to manage them in a central place and access the models through simple string ids.\n\nThe AI SDK offers custom providers and a provider registry for this purpose:\n\nWith custom providers, you can pre-configure model settings, provide model name aliases, and limit the available models.\nThe provider registry lets you mix multiple providers and access them through simple string ids.\n\nYou can mix and match custom providers, the provider registry, and middleware in your application.\n\nCustom Providers\n\nYou can create a custom provider using customProvider.\n\nExample: custom model settings\n\nYou might want to override the default model settings for a provider or provide model name aliases with pre-configured settings.\n\nimport { openai as originalOpenAI } from '@ai-sdk/openai';\nimport {\n  customProvider,\n  defaultSettingsMiddleware,\n  wrapLanguageModel,\n} from 'ai';\n\n\n// custom provider with different provider options:\nexport const openai = customProvider({\n  languageModels: {\n    // replacement model with custom provider options:\n    'gpt-4o': wrapLanguageModel({\n      model: originalOpenAI('gpt-4o'),\n      middleware: defaultSettingsMiddleware({\n        settings: {\n          providerOptions: {\n            openai: {\n              reasoningEffort: 'high',\n            },\n          },\n        },\n      }),\n    }),\n    // alias model with custom provider options:\n    'gpt-4o-mini-high-reasoning': wrapLanguageModel({\n      model: originalOpenAI('gpt-4o-mini'),\n      middleware: defaultSettingsMiddleware({\n        settings: {\n          providerOptions: {\n            openai: {\n              reasoningEffort: 'high',\n            },\n          },\n        },\n      }),\n    }),\n  },\n  fallbackProvider: originalOpenAI,\n});\nExample: model name alias\n\nYou can also provide model name aliases, so you can update the model version in one place in the future:\n\nimport { anthropic as originalAnthropic } from '@ai-sdk/anthropic';\nimport { customProvider } from 'ai';\n\n\n// custom provider with alias names:\nexport const anthropic = customProvider({\n  languageModels: {\n    opus: originalAnthropic('claude-3-opus-20240229'),\n    sonnet: originalAnthropic('claude-3-5-sonnet-20240620'),\n    haiku: originalAnthropic('claude-3-haiku-20240307'),\n  },\n  fallbackProvider: originalAnthropic,\n});\nExample: limit available models\n\nYou can limit the available models in the system, even if you have multiple providers.\n\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { openai } from '@ai-sdk/openai';\nimport {\n  customProvider,\n  defaultSettingsMiddleware,\n  wrapLanguageModel,\n} from 'ai';\n\n\nexport const myProvider = customProvider({\n  languageModels: {\n    'text-medium': anthropic('claude-3-5-sonnet-20240620'),\n    'text-small': openai('gpt-4o-mini'),\n    'reasoning-medium': wrapLanguageModel({\n      model: openai('gpt-4o'),\n      middleware: defaultSettingsMiddleware({\n        settings: {\n          providerOptions: {\n            openai: {\n              reasoningEffort: 'high',\n            },\n          },\n        },\n      }),\n    }),\n    'reasoning-fast': wrapLanguageModel({\n      model: openai('gpt-4o-mini'),\n      middleware: defaultSettingsMiddleware({\n        settings: {\n          providerOptions: {\n            openai: {\n              reasoningEffort: 'high',\n            },\n          },\n        },\n      }),\n    }),\n  },\n  embeddingModels: {\n    embedding: openai.textEmbeddingModel('text-embedding-3-small'),\n  },\n  // no fallback provider\n});\nProvider Registry\n\nYou can create a provider registry with multiple providers and models using createProviderRegistry.\n\nSetup\nregistry.ts\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { createProviderRegistry } from 'ai';\n\n\nexport const registry = createProviderRegistry({\n  // register provider with prefix and default setup:\n  anthropic,\n\n\n  // register provider with prefix and custom setup:\n  openai: createOpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n  }),\n});\nSetup with Custom Separator\n\nBy default, the registry uses : as the separator between provider and model IDs. You can customize this separator:\n\nregistry.ts\nimport { createProviderRegistry } from 'ai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { openai } from '@ai-sdk/openai';\n\n\nexport const customSeparatorRegistry = createProviderRegistry(\n  {\n    anthropic,\n    openai,\n  },\n  { separator: ' > ' },\n);\nExample: Use language models\n\nYou can access language models by using the languageModel method on the registry. The provider id will become the prefix of the model id: providerId:modelId.\n\nimport { generateText } from 'ai';\nimport { registry } from './registry';\n\n\nconst { text } = await generateText({\n  model: registry.languageModel('openai:gpt-4.1'), // default separator\n  // or with custom separator:\n  // model: customSeparatorRegistry.languageModel('openai > gpt-4.1'),\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\nExample: Use text embedding models\n\nYou can access text embedding models by using the textEmbeddingModel method on the registry. The provider id will become the prefix of the model id: providerId:modelId.\n\nimport { embed } from 'ai';\nimport { registry } from './registry';\n\n\nconst { embedding } = await embed({\n  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),\n  value: 'sunny day at the beach',\n});\nExample: Use image models\n\nYou can access image models by using the imageModel method on the registry. The provider id will become the prefix of the model id: providerId:modelId.\n\nimport { generateImage } from 'ai';\nimport { registry } from './registry';\n\n\nconst { image } = await generateImage({\n  model: registry.imageModel('openai:dall-e-3'),\n  prompt: 'A beautiful sunset over a calm ocean',\n});\nCombining Custom Providers, Provider Registry, and Middleware\n\nThe central idea of provider management is to set up a file that contains all the providers and models you want to use. You may want to pre-configure model settings, provide model name aliases, limit the available models, and more.\n\nHere is an example that implements the following concepts:\n\npass through a full provider with a namespace prefix (here: xai > *)\nsetup an OpenAI-compatible provider with custom api key and base URL (here: custom > *)\nsetup model name aliases (here: anthropic > fast, anthropic > writing, anthropic > reasoning)\npre-configure model settings (here: anthropic > reasoning)\nvalidate the provider-specific options (here: AnthropicProviderOptions)\nuse a fallback provider (here: anthropic > *)\nlimit a provider to certain models without a fallback (here: groq > gemma2-9b-it, groq > qwen-qwq-32b)\ndefine a custom separator for the provider registry (here: >)\nimport { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';\nimport { createOpenAICompatible } from '@ai-sdk/openai-compatible';\nimport { xai } from '@ai-sdk/xai';\nimport { groq } from '@ai-sdk/groq';\nimport {\n  createProviderRegistry,\n  customProvider,\n  defaultSettingsMiddleware,\n  wrapLanguageModel,\n} from 'ai';\n\n\nexport const registry = createProviderRegistry(\n  {\n    // pass through a full provider with a namespace prefix\n    xai,\n\n\n    // access an OpenAI-compatible provider with custom setup\n    custom: createOpenAICompatible({\n      name: 'provider-name',\n      apiKey: process.env.CUSTOM_API_KEY,\n      baseURL: 'https://api.custom.com/v1',\n    }),\n\n\n    // setup model name aliases\n    anthropic: customProvider({\n      languageModels: {\n        fast: anthropic('claude-3-haiku-20240307'),\n\n\n        // simple model\n        writing: anthropic('claude-3-7-sonnet-20250219'),\n\n\n        // extended reasoning model configuration:\n        reasoning: wrapLanguageModel({\n          model: anthropic('claude-3-7-sonnet-20250219'),\n          middleware: defaultSettingsMiddleware({\n            settings: {\n              maxOutputTokens: 100000, // example default setting\n              providerOptions: {\n                anthropic: {\n                  thinking: {\n                    type: 'enabled',\n                    budgetTokens: 32000,\n                  },\n                } satisfies AnthropicProviderOptions,\n              },\n            },\n          }),\n        }),\n      },\n      fallbackProvider: anthropic,\n    }),\n\n\n    // limit a provider to certain models without a fallback\n    groq: customProvider({\n      languageModels: {\n        'gemma2-9b-it': groq('gemma2-9b-it'),\n        'qwen-qwq-32b': groq('qwen-qwq-32b'),\n      },\n    }),\n  },\n  { separator: ' > ' },\n);\n\n\n// usage:\nconst model = registry.languageModel('anthropic > reasoning');\nGlobal Provider Configuration\n\nThe AI SDK 5 includes a global provider feature that allows you to specify a model using just a plain model ID string:\n\nimport { streamText } from 'ai';\n\n\nconst result = await streamText({\n  model: 'openai/gpt-4o', // Uses the global provider (defaults to AI Gateway)\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nBy default, the global provider is set to the Vercel AI Gateway.\n\nCustomizing the Global Provider\n\nYou can set your own preferred global provider:\n\nsetup.ts\nimport { openai } from '@ai-sdk/openai';\n\n\n// Initialize once during startup:\nglobalThis.AI_SDK_DEFAULT_PROVIDER = openai;\napp.ts\nimport { streamText } from 'ai';\n\n\nconst result = await streamText({\n  model: 'gpt-4o', // Uses OpenAI provider without prefix\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nThis simplifies provider usage and makes it easier to switch between providers without changing your model references throughout your codebase.\n\nPrevious\nLanguage Model Middleware\nNext\nError Handling"
  },
  {
    "title": "AI SDK Core: Language Model Middleware",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/middleware",
    "html": "AI SDK Core\nLanguage Model Middleware\nCopy markdown\nLanguage Model Middleware\n\nLanguage model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model.\n\nIt can be used to add features like guardrails, RAG, caching, and logging in a language model agnostic way. Such middleware can be developed and distributed independently from the language models that they are applied to.\n\nUsing Language Model Middleware\n\nYou can use language model middleware with the wrapLanguageModel function. It takes a language model and a language model middleware and returns a new language model that incorporates the middleware.\n\nimport { wrapLanguageModel } from 'ai';\n\n\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: yourLanguageModelMiddleware,\n});\n\nThe wrapped language model can be used just like any other language model, e.g. in streamText:\n\nconst result = streamText({\n  model: wrappedLanguageModel,\n  prompt: 'What cities are in the United States?',\n});\nMultiple middlewares\n\nYou can provide multiple middlewares to the wrapLanguageModel function. The middlewares will be applied in the order they are provided.\n\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: [firstMiddleware, secondMiddleware],\n});\n\n\n// applied as: firstMiddleware(secondMiddleware(yourModel))\nBuilt-in Middleware\n\nThe AI SDK comes with several built-in middlewares that you can use to configure language models:\n\nextractReasoningMiddleware: Extracts reasoning information from the generated text and exposes it as a reasoning property on the result.\nsimulateStreamingMiddleware: Simulates streaming behavior with responses from non-streaming language models.\ndefaultSettingsMiddleware: Applies default settings to a language model.\nExtract Reasoning\n\nSome providers and models expose reasoning information in the generated text using special tags, e.g. <think> and </think>.\n\nThe extractReasoningMiddleware function can be used to extract this reasoning information and expose it as a reasoning property on the result.\n\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n\nYou can then use that enhanced model in functions like generateText and streamText.\n\nThe extractReasoningMiddleware function also includes a startWithReasoning option. When set to true, the reasoning tag will be prepended to the generated text. This is useful for models that do not include the reasoning tag at the beginning of the response. For more details, see the DeepSeek R1 guide.\n\nSimulate Streaming\n\nThe simulateStreamingMiddleware function can be used to simulate streaming behavior with responses from non-streaming language models. This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\n\nimport { wrapLanguageModel, simulateStreamingMiddleware } from 'ai';\n\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: simulateStreamingMiddleware(),\n});\nDefault Settings\n\nThe defaultSettingsMiddleware function can be used to apply default settings to a language model.\n\nimport { wrapLanguageModel, defaultSettingsMiddleware } from 'ai';\n\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: defaultSettingsMiddleware({\n    settings: {\n      temperature: 0.5,\n      maxOutputTokens: 800,\n      providerOptions: { openai: { store: false } },\n    },\n  }),\n});\nCommunity Middleware\n\nThe AI SDK provides a Language Model Middleware specification. Community members can develop middleware that adheres to this specification, making it compatible with the AI SDK ecosystem.\n\nHere are some community middlewares that you can explore:\n\nCustom tool call parser\n\nThe Custom tool call parser\n middleware extends tool call capabilities to models that don't natively support the OpenAI-style tools parameter. This includes many self-hosted and third-party models that lack native function calling features.\n\nUsing this middleware on models that support native function calls may result in unintended performance degradation, so check whether your model supports native function calls before deciding to use it.\n\nThis middleware enables function calling capabilities by converting function schemas into prompt instructions and parsing the model's responses into structured function calls. It works by transforming the JSON function definitions into natural language instructions the model can understand, then analyzing the generated text to extract function call attempts. This approach allows developers to use the same function calling API across different model providers, even with models that don't natively support the OpenAI-style function calling format, providing a consistent function calling experience regardless of the underlying model implementation.\n\nThe @ai-sdk-tool/parser package offers three middleware variants:\n\ncreateToolMiddleware: A flexible function for creating custom tool call middleware tailored to specific models\nhermesToolMiddleware: Ready-to-use middleware for Hermes & Qwen format function calls\ngemmaToolMiddleware: Pre-configured middleware for Gemma 3 model series function call format\n\nHere's how you can enable function calls with Gemma models that don't support them natively:\n\nimport { wrapLanguageModel } from 'ai';\nimport { gemmaToolMiddleware } from '@ai-sdk-tool/parser';\n\n\nconst model = wrapLanguageModel({\n  model: openrouter('google/gemma-3-27b-it'),\n  middleware: gemmaToolMiddleware,\n});\n\nFind more examples at this link\n.\n\nImplementing Language Model Middleware\n\nImplementing language model middleware is advanced functionality and requires a solid understanding of the language model specification\n.\n\nYou can implement any of the following three function to modify the behavior of the language model:\n\ntransformParams: Transforms the parameters before they are passed to the language model, for both doGenerate and doStream.\nwrapGenerate: Wraps the doGenerate method of the language model\n. You can modify the parameters, call the language model, and modify the result.\nwrapStream: Wraps the doStream method of the language model\n. You can modify the parameters, call the language model, and modify the result.\n\nHere are some examples of how to implement language model middleware:\n\nExamples\n\nThese examples are not meant to be used in production. They are just to show how you can use middleware to enhance the behavior of language models.\n\nLogging\n\nThis example shows how to log the parameters and generated text of a language model call.\n\nimport type {\n  LanguageModelV2Middleware,\n  LanguageModelV2StreamPart,\n} from '@ai-sdk/provider';\n\n\nexport const yourLogMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log('doGenerate called');\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n\n    const result = await doGenerate();\n\n\n    console.log('doGenerate finished');\n    console.log(`generated text: ${result.text}`);\n\n\n    return result;\n  },\n\n\n  wrapStream: async ({ doStream, params }) => {\n    console.log('doStream called');\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n\n    const { stream, ...rest } = await doStream();\n\n\n    let generatedText = '';\n    const textBlocks = new Map<string, string>();\n\n\n    const transformStream = new TransformStream<\n      LanguageModelV2StreamPart,\n      LanguageModelV2StreamPart\n    >({\n      transform(chunk, controller) {\n        switch (chunk.type) {\n          case 'text-start': {\n            textBlocks.set(chunk.id, '');\n            break;\n          }\n          case 'text-delta': {\n            const existing = textBlocks.get(chunk.id) || '';\n            textBlocks.set(chunk.id, existing + chunk.delta);\n            generatedText += chunk.delta;\n            break;\n          }\n          case 'text-end': {\n            console.log(\n              `Text block ${chunk.id} completed:`,\n              textBlocks.get(chunk.id),\n            );\n            break;\n          }\n        }\n\n\n        controller.enqueue(chunk);\n      },\n\n\n      flush() {\n        console.log('doStream finished');\n        console.log(`generated text: ${generatedText}`);\n      },\n    });\n\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\nCaching\n\nThis example shows how to build a simple cache for the generated text of a language model call.\n\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\n\nconst cache = new Map<string, any>();\n\n\nexport const yourCacheMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n\n    if (cache.has(cacheKey)) {\n      return cache.get(cacheKey);\n    }\n\n\n    const result = await doGenerate();\n\n\n    cache.set(cacheKey, result);\n\n\n    return result;\n  },\n\n\n  // here you would implement the caching logic for streaming\n};\nRetrieval Augmented Generation (RAG)\n\nThis example shows how to use RAG as middleware.\n\nHelper functions like getLastUserMessageText and findSources are not part of the AI SDK. They are just used in this example to illustrate the concept of RAG.\n\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\n\nexport const yourRagMiddleware: LanguageModelV2Middleware = {\n  transformParams: async ({ params }) => {\n    const lastUserMessageText = getLastUserMessageText({\n      prompt: params.prompt,\n    });\n\n\n    if (lastUserMessageText == null) {\n      return params; // do not use RAG (send unmodified parameters)\n    }\n\n\n    const instruction =\n      'Use the following information to answer the question:\\n' +\n      findSources({ text: lastUserMessageText })\n        .map(chunk => JSON.stringify(chunk))\n        .join('\\n');\n\n\n    return addToLastUserMessage({ params, text: instruction });\n  },\n};\nGuardrails\n\nGuard rails are a way to ensure that the generated text of a language model call is safe and appropriate. This example shows how to use guardrails as middleware.\n\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\n\nexport const yourGuardrailMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate }) => {\n    const { text, ...rest } = await doGenerate();\n\n\n    // filtering approach, e.g. for PII or other sensitive information:\n    const cleanedText = text?.replace(/badword/g, '<REDACTED>');\n\n\n    return { text: cleanedText, ...rest };\n  },\n\n\n  // here you would implement the guardrail logic for streaming\n  // Note: streaming guardrails are difficult to implement, because\n  // you do not know the full content of the stream until it's finished.\n};\nConfiguring Per Request Custom Metadata\n\nTo send and access custom metadata in Middleware, you can use providerOptions. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.\n\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, wrapLanguageModel } from 'ai';\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\n\nexport const yourLogMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log('METADATA', params?.providerMetadata?.yourLogMiddleware);\n    const result = await doGenerate();\n    return result;\n  },\n};\n\n\nconst { text } = await generateText({\n  model: wrapLanguageModel({\n    model: openai('gpt-4o'),\n    middleware: yourLogMiddleware,\n  }),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  providerOptions: {\n    yourLogMiddleware: {\n      hello: 'world',\n    },\n  },\n});\n\n\nconsole.log(text);\nPrevious\nSpeech\nNext\nProvider & Model Management"
  },
  {
    "title": "AI SDK Core: Error Handling",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling",
    "html": "AI SDK Core\nError Handling\nCopy markdown\nError Handling\nHandling regular errors\n\nRegular errors are thrown and can be handled using the try/catch block.\n\nimport { generateText } from 'ai';\n\n\ntry {\n  const { text } = await generateText({\n    model: 'openai/gpt-4.1',\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n} catch (error) {\n  // handle error\n}\n\nSee Error Types for more information on the different types of errors that may be thrown.\n\nHandling streaming errors (simple streams)\n\nWhen errors occur during streams that do not support error chunks, the error is thrown as a regular error. You can handle these errors using the try/catch block.\n\nimport { generateText } from 'ai';\n\n\ntry {\n  const { textStream } = streamText({\n    model: 'openai/gpt-4.1',\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n\n\n  for await (const textPart of textStream) {\n    process.stdout.write(textPart);\n  }\n} catch (error) {\n  // handle error\n}\nHandling streaming errors (streaming with error support)\n\nFull streams support error parts. You can handle those parts similar to other parts. It is recommended to also add a try-catch block for errors that happen outside of the streaming.\n\nimport { generateText } from 'ai';\n\n\ntry {\n  const { fullStream } = streamText({\n    model: 'openai/gpt-4.1',\n    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  });\n\n\n  for await (const part of fullStream) {\n    switch (part.type) {\n      // ... handle other part types\n\n\n      case 'error': {\n        const error = part.error;\n        // handle error\n        break;\n      }\n\n\n      case 'abort': {\n        // handle stream abort\n        break;\n      }\n\n\n      case 'tool-error': {\n        const error = part.error;\n        // handle error\n        break;\n      }\n    }\n  }\n} catch (error) {\n  // handle error\n}\nHandling stream aborts\n\nWhen streams are aborted (e.g., via chat stop button), you may want to perform cleanup operations like updating stored messages in your UI. Use the onAbort callback to handle these cases.\n\nThe onAbort callback is called when a stream is aborted via AbortSignal, but onFinish is not called. This ensures you can still update your UI state appropriately.\n\nimport { streamText } from 'ai';\n\n\nconst { textStream } = streamText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n  onAbort: ({ steps }) => {\n    // Update stored messages or perform cleanup\n    console.log('Stream aborted after', steps.length, 'steps');\n  },\n  onFinish: ({ steps, totalUsage }) => {\n    // This is called on normal completion\n    console.log('Stream completed normally');\n  },\n});\n\n\nfor await (const textPart of textStream) {\n  process.stdout.write(textPart);\n}\n\nThe onAbort callback receives:\n\nsteps: An array of all completed steps before the abort\n\nYou can also handle abort events directly in the stream:\n\nimport { streamText } from 'ai';\n\n\nconst { fullStream } = streamText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n});\n\n\nfor await (const chunk of fullStream) {\n  switch (chunk.type) {\n    case 'abort': {\n      // Handle abort directly in stream\n      console.log('Stream was aborted');\n      break;\n    }\n    // ... handle other part types\n  }\n}\nPrevious\nProvider & Model Management\nNext\nTesting"
  },
  {
    "title": "AI SDK Core: Testing",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/testing",
    "html": "AI SDK Core\nTesting\nCopy markdown\nTesting\n\nTesting language models can be challenging, because they are non-deterministic and calling them is slow and expensive.\n\nTo enable you to unit test your code that uses the AI SDK, the AI SDK Core includes mock providers and test helpers. You can import the following helpers from ai/test:\n\nMockEmbeddingModelV2: A mock embedding model using the embedding model v2 specification\n.\nMockLanguageModelV2: A mock language model using the language model v2 specification\n.\nmockId: Provides an incrementing integer ID.\nmockValues: Iterates over an array of values with each call. Returns the last value when the array is exhausted.\nsimulateReadableStream: Simulates a readable stream with delays.\n\nWith mock providers and test helpers, you can control the output of the AI SDK and test your code in a repeatable and deterministic way without actually calling a language model provider.\n\nExamples\n\nYou can use the test helpers with the AI Core functions in your unit tests:\n\ngenerateText\nimport { generateText } from 'ai';\nimport { MockLanguageModelV2 } from 'ai/test';\n\n\nconst result = await generateText({\n  model: new MockLanguageModelV2({\n    doGenerate: async () => ({\n      finishReason: 'stop',\n      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\n      content: [{ type: 'text', text: `Hello, world!` }],\n      warnings: [],\n    }),\n  }),\n  prompt: 'Hello, test!',\n});\nstreamText\nimport { streamText, simulateReadableStream } from 'ai';\nimport { MockLanguageModelV2 } from 'ai/test';\n\n\nconst result = streamText({\n  model: new MockLanguageModelV2({\n    doStream: async () => ({\n      stream: simulateReadableStream({\n        chunks: [\n          { type: 'text-start', id: 'text-1' },\n          { type: 'text-delta', id: 'text-1', delta: 'Hello' },\n          { type: 'text-delta', id: 'text-1', delta: ', ' },\n          { type: 'text-delta', id: 'text-1', delta: 'world!' },\n          { type: 'text-end', id: 'text-1' },\n          {\n            type: 'finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\n          },\n        ],\n      }),\n    }),\n  }),\n  prompt: 'Hello, test!',\n});\ngenerateObject\nimport { generateObject } from 'ai';\nimport { MockLanguageModelV2 } from 'ai/test';\nimport { z } from 'zod';\n\n\nconst result = await generateObject({\n  model: new MockLanguageModelV2({\n    doGenerate: async () => ({\n      finishReason: 'stop',\n      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\n      content: [{ type: 'text', text: `{\"content\":\"Hello, world!\"}` }],\n      warnings: [],\n    }),\n  }),\n  schema: z.object({ content: z.string() }),\n  prompt: 'Hello, test!',\n});\nstreamObject\nimport { streamObject, simulateReadableStream } from 'ai';\nimport { MockLanguageModelV2 } from 'ai/test';\nimport { z } from 'zod';\n\n\nconst result = streamObject({\n  model: new MockLanguageModelV2({\n    doStream: async () => ({\n      stream: simulateReadableStream({\n        chunks: [\n          { type: 'text-start', id: 'text-1' },\n          { type: 'text-delta', id: 'text-1', delta: '{ ' },\n          { type: 'text-delta', id: 'text-1', delta: '\"content\": ' },\n          { type: 'text-delta', id: 'text-1', delta: `\"Hello, ` },\n          { type: 'text-delta', id: 'text-1', delta: `world` },\n          { type: 'text-delta', id: 'text-1', delta: `!\"` },\n          { type: 'text-delta', id: 'text-1', delta: ' }' },\n          { type: 'text-end', id: 'text-1' },\n          {\n            type: 'finish',\n            finishReason: 'stop',\n            logprobs: undefined,\n            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\n          },\n        ],\n      }),\n    }),\n  }),\n  schema: z.object({ content: z.string() }),\n  prompt: 'Hello, test!',\n});\nSimulate UI Message Stream Responses\n\nYou can also simulate UI Message Stream responses for testing, debugging, or demonstration purposes.\n\nHere is a Next example:\n\nroute.ts\nimport { simulateReadableStream } from 'ai';\n\n\nexport async function POST(req: Request) {\n  return new Response(\n    simulateReadableStream({\n      initialDelayInMs: 1000, // Delay before the first chunk\n      chunkDelayInMs: 300, // Delay between chunks\n      chunks: [\n        `data: {\"type\":\"start\",\"messageId\":\"msg-123\"}\\n\\n`,\n        `data: {\"type\":\"text-start\",\"id\":\"text-1\"}\\n\\n`,\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\"This\"}\\n\\n`,\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" is an\"}\\n\\n`,\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" example.\"}\\n\\n`,\n        `data: {\"type\":\"text-end\",\"id\":\"text-1\"}\\n\\n`,\n        `data: {\"type\":\"finish\"}\\n\\n`,\n        `data: [DONE]\\n\\n`,\n      ],\n    }).pipeThrough(new TextEncoderStream()),\n    {\n      status: 200,\n      headers: {\n        'Content-Type': 'text/event-stream',\n        'Cache-Control': 'no-cache',\n        Connection: 'keep-alive',\n        'x-vercel-ai-ui-message-stream': 'v1',\n      },\n    },\n  );\n}\nPrevious\nError Handling\nNext\nTelemetry"
  },
  {
    "title": "AI SDK Core: Telemetry",
    "url": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry",
    "html": "AI SDK Core\nTelemetry\nCopy markdown\nTelemetry\n\nAI SDK Telemetry is experimental and may change in the future.\n\nThe AI SDK uses OpenTelemetry\n to collect telemetry data. OpenTelemetry is an open-source observability framework designed to provide standardized instrumentation for collecting telemetry data.\n\nCheck out the AI SDK Observability Integrations to see providers that offer monitoring and tracing for AI SDK applications.\n\nEnabling telemetry\n\nFor Next.js applications, please follow the Next.js OpenTelemetry guide\n to enable telemetry first.\n\nYou can then use the experimental_telemetry option to enable telemetry on specific function calls while the feature is experimental:\n\nconst result = await generateText({\n  model: openai('gpt-4.1'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: { isEnabled: true },\n});\n\nWhen telemetry is enabled, you can also control if you want to record the input values and the output values for the function. By default, both are enabled. You can disable them by setting the recordInputs and recordOutputs options to false.\n\nDisabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons. You might for example want to disable recording inputs if they contain sensitive information.\n\nTelemetry Metadata\n\nYou can provide a functionId to identify the function that the telemetry data is for, and metadata to include additional information in the telemetry data.\n\nconst result = await generateText({\n  model: openai('gpt-4.1'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: {\n    isEnabled: true,\n    functionId: 'my-awesome-function',\n    metadata: {\n      something: 'custom',\n      someOtherThing: 'other-value',\n    },\n  },\n});\nCustom Tracer\n\nYou may provide a tracer which must return an OpenTelemetry Tracer. This is useful in situations where you want your traces to use a TracerProvider other than the one provided by the @opentelemetry/api singleton.\n\nconst tracerProvider = new NodeTracerProvider();\nconst result = await generateText({\n  model: openai('gpt-4.1'),\n  prompt: 'Write a short story about a cat.',\n  experimental_telemetry: {\n    isEnabled: true,\n    tracer: tracerProvider.getTracer('ai'),\n  },\n});\nCollected Data\ngenerateText function\n\ngenerateText records 3 types of spans:\n\nai.generateText (span): the full length of the generateText call. It contains 1 or more ai.generateText.doGenerate spans. It contains the basic LLM span information and the following attributes:\n\noperation.name: ai.generateText and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.generateText\"\nai.prompt: the prompt that was used when calling generateText\nai.response.text: the text that was generated\nai.response.toolCalls: the tool calls that were made as part of the generation (stringified JSON)\nai.response.finishReason: the reason why the generation finished\nai.settings.maxOutputTokens: the maximum number of output tokens that were set\n\nai.generateText.doGenerate (span): a provider doGenerate call. It can contain ai.toolCall spans. It contains the call LLM span information and the following attributes:\n\noperation.name: ai.generateText.doGenerate and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.generateText.doGenerate\"\nai.prompt.messages: the messages that were passed into the provider\nai.prompt.tools: array of stringified tool definitions. The tools can be of type function or provider-defined-client. Function tools have a name, description (optional), and inputSchema (JSON schema). Provider-defined-client tools have a name, id, and input (Record).\nai.prompt.toolChoice: the stringified tool choice setting (JSON). It has a type property (auto, none, required, tool), and if the type is tool, a toolName property with the specific tool.\nai.response.text: the text that was generated\nai.response.toolCalls: the tool calls that were made as part of the generation (stringified JSON)\nai.response.finishReason: the reason why the generation finished\n\nai.toolCall (span): a tool call that is made as part of the generateText call. See Tool call spans for more details.\n\nstreamText function\n\nstreamText records 3 types of spans and 2 types of events:\n\nai.streamText (span): the full length of the streamText call. It contains a ai.streamText.doStream span. It contains the basic LLM span information and the following attributes:\n\noperation.name: ai.streamText and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.streamText\"\nai.prompt: the prompt that was used when calling streamText\nai.response.text: the text that was generated\nai.response.toolCalls: the tool calls that were made as part of the generation (stringified JSON)\nai.response.finishReason: the reason why the generation finished\nai.settings.maxOutputTokens: the maximum number of output tokens that were set\n\nai.streamText.doStream (span): a provider doStream call. This span contains an ai.stream.firstChunk event and ai.toolCall spans. It contains the call LLM span information and the following attributes:\n\noperation.name: ai.streamText.doStream and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.streamText.doStream\"\nai.prompt.messages: the messages that were passed into the provider\nai.prompt.tools: array of stringified tool definitions. The tools can be of type function or provider-defined-client. Function tools have a name, description (optional), and inputSchema (JSON schema). Provider-defined-client tools have a name, id, and input (Record).\nai.prompt.toolChoice: the stringified tool choice setting (JSON). It has a type property (auto, none, required, tool), and if the type is tool, a toolName property with the specific tool.\nai.response.text: the text that was generated\nai.response.toolCalls: the tool calls that were made as part of the generation (stringified JSON)\nai.response.msToFirstChunk: the time it took to receive the first chunk in milliseconds\nai.response.msToFinish: the time it took to receive the finish part of the LLM stream in milliseconds\nai.response.avgCompletionTokensPerSecond: the average number of completion tokens per second\nai.response.finishReason: the reason why the generation finished\n\nai.toolCall (span): a tool call that is made as part of the generateText call. See Tool call spans for more details.\n\nai.stream.firstChunk (event): an event that is emitted when the first chunk of the stream is received.\n\nai.response.msToFirstChunk: the time it took to receive the first chunk\n\nai.stream.finish (event): an event that is emitted when the finish part of the LLM stream is received.\n\nIt also records a ai.stream.firstChunk event when the first chunk of the stream is received.\n\ngenerateObject function\n\ngenerateObject records 2 types of spans:\n\nai.generateObject (span): the full length of the generateObject call. It contains 1 or more ai.generateObject.doGenerate spans. It contains the basic LLM span information and the following attributes:\n\noperation.name: ai.generateObject and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.generateObject\"\nai.prompt: the prompt that was used when calling generateObject\nai.schema: Stringified JSON schema version of the schema that was passed into the generateObject function\nai.schema.name: the name of the schema that was passed into the generateObject function\nai.schema.description: the description of the schema that was passed into the generateObject function\nai.response.object: the object that was generated (stringified JSON)\nai.settings.output: the output type that was used, e.g. object or no-schema\n\nai.generateObject.doGenerate (span): a provider doGenerate call. It contains the call LLM span information and the following attributes:\n\noperation.name: ai.generateObject.doGenerate and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.generateObject.doGenerate\"\nai.prompt.messages: the messages that were passed into the provider\nai.response.object: the object that was generated (stringified JSON)\nai.response.finishReason: the reason why the generation finished\nstreamObject function\n\nstreamObject records 2 types of spans and 1 type of event:\n\nai.streamObject (span): the full length of the streamObject call. It contains 1 or more ai.streamObject.doStream spans. It contains the basic LLM span information and the following attributes:\n\noperation.name: ai.streamObject and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.streamObject\"\nai.prompt: the prompt that was used when calling streamObject\nai.schema: Stringified JSON schema version of the schema that was passed into the streamObject function\nai.schema.name: the name of the schema that was passed into the streamObject function\nai.schema.description: the description of the schema that was passed into the streamObject function\nai.response.object: the object that was generated (stringified JSON)\nai.settings.output: the output type that was used, e.g. object or no-schema\n\nai.streamObject.doStream (span): a provider doStream call. This span contains an ai.stream.firstChunk event. It contains the call LLM span information and the following attributes:\n\noperation.name: ai.streamObject.doStream and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.streamObject.doStream\"\nai.prompt.messages: the messages that were passed into the provider\nai.response.object: the object that was generated (stringified JSON)\nai.response.msToFirstChunk: the time it took to receive the first chunk\nai.response.finishReason: the reason why the generation finished\n\nai.stream.firstChunk (event): an event that is emitted when the first chunk of the stream is received.\n\nai.response.msToFirstChunk: the time it took to receive the first chunk\nembed function\n\nembed records 2 types of spans:\n\nai.embed (span): the full length of the embed call. It contains 1 ai.embed.doEmbed spans. It contains the basic embedding span information and the following attributes:\n\noperation.name: ai.embed and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.embed\"\nai.value: the value that was passed into the embed function\nai.embedding: a JSON-stringified embedding\n\nai.embed.doEmbed (span): a provider doEmbed call. It contains the basic embedding span information and the following attributes:\n\noperation.name: ai.embed.doEmbed and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.embed.doEmbed\"\nai.values: the values that were passed into the provider (array)\nai.embeddings: an array of JSON-stringified embeddings\nembedMany function\n\nembedMany records 2 types of spans:\n\nai.embedMany (span): the full length of the embedMany call. It contains 1 or more ai.embedMany.doEmbed spans. It contains the basic embedding span information and the following attributes:\n\noperation.name: ai.embedMany and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.embedMany\"\nai.values: the values that were passed into the embedMany function\nai.embeddings: an array of JSON-stringified embedding\n\nai.embedMany.doEmbed (span): a provider doEmbed call. It contains the basic embedding span information and the following attributes:\n\noperation.name: ai.embedMany.doEmbed and the functionId that was set through telemetry.functionId\nai.operationId: \"ai.embedMany.doEmbed\"\nai.values: the values that were sent to the provider\nai.embeddings: an array of JSON-stringified embeddings for each value\nSpan Details\nBasic LLM span information\n\nMany spans that use LLMs (ai.generateText, ai.generateText.doGenerate, ai.streamText, ai.streamText.doStream, ai.generateObject, ai.generateObject.doGenerate, ai.streamObject, ai.streamObject.doStream) contain the following attributes:\n\nresource.name: the functionId that was set through telemetry.functionId\nai.model.id: the id of the model\nai.model.provider: the provider of the model\nai.request.headers.*: the request headers that were passed in through headers\nai.response.providerMetadata: provider specific metadata returned with the generation response\nai.settings.maxRetries: the maximum number of retries that were set\nai.telemetry.functionId: the functionId that was set through telemetry.functionId\nai.telemetry.metadata.*: the metadata that was passed in through telemetry.metadata\nai.usage.completionTokens: the number of completion tokens that were used\nai.usage.promptTokens: the number of prompt tokens that were used\nCall LLM span information\n\nSpans that correspond to individual LLM calls (ai.generateText.doGenerate, ai.streamText.doStream, ai.generateObject.doGenerate, ai.streamObject.doStream) contain basic LLM span information and the following attributes:\n\nai.response.model: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\nai.response.id: the id of the response. Uses the ID from the provider when available.\nai.response.timestamp: the timestamp of the response. Uses the timestamp from the provider when available.\nSemantic Conventions for GenAI operations\ngen_ai.system: the provider that was used\ngen_ai.request.model: the model that was requested\ngen_ai.request.temperature: the temperature that was set\ngen_ai.request.max_tokens: the maximum number of tokens that were set\ngen_ai.request.frequency_penalty: the frequency penalty that was set\ngen_ai.request.presence_penalty: the presence penalty that was set\ngen_ai.request.top_k: the topK parameter value that was set\ngen_ai.request.top_p: the topP parameter value that was set\ngen_ai.request.stop_sequences: the stop sequences\ngen_ai.response.finish_reasons: the finish reasons that were returned by the provider\ngen_ai.response.model: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\ngen_ai.response.id: the id of the response. Uses the ID from the provider when available.\ngen_ai.usage.input_tokens: the number of prompt tokens that were used\ngen_ai.usage.output_tokens: the number of completion tokens that were used\nBasic embedding span information\n\nMany spans that use embedding models (ai.embed, ai.embed.doEmbed, ai.embedMany, ai.embedMany.doEmbed) contain the following attributes:\n\nai.model.id: the id of the model\nai.model.provider: the provider of the model\nai.request.headers.*: the request headers that were passed in through headers\nai.settings.maxRetries: the maximum number of retries that were set\nai.telemetry.functionId: the functionId that was set through telemetry.functionId\nai.telemetry.metadata.*: the metadata that was passed in through telemetry.metadata\nai.usage.tokens: the number of tokens that were used\nresource.name: the functionId that was set through telemetry.functionId\nTool call spans\n\nTool call spans (ai.toolCall) contain the following attributes:\n\noperation.name: \"ai.toolCall\"\nai.operationId: \"ai.toolCall\"\nai.toolCall.name: the name of the tool\nai.toolCall.id: the id of the tool call\nai.toolCall.args: the input parameters of the tool call\nai.toolCall.result: the output result of the tool call. Only available if the tool call is successful and the result is serializable.\nPrevious\nTesting\nNext\nAI SDK UI"
  },
  {
    "title": "AI SDK UI: Overview",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
    "html": "AI SDK UI\nOverview\nCopy markdown\nAI SDK UI\n\nAI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a framework-agnostic toolkit, streamlining the integration of advanced AI functionalities into your applications.\n\nAI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With three main hooks — useChat, useCompletion, and useObject — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.\n\nuseChat offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.\nuseCompletion enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.\nuseObject is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.\n\nThese hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.\n\nUI Framework Support\n\nAI SDK UI supports the following frameworks: React\n, Svelte\n, Vue.js\n, and Angular\n. Here is a comparison of the supported functions across these frameworks:\n\nFunction\tReact\tSvelte\tVue.js\tAngular\nuseChat\t\n\t\n Chat\t\n\t\n Chat\nuseCompletion\t\n\t\n Completion\t\n\t\n Completion\nuseObject\t\n\t\n StructuredObject\t\n\t\n StructuredObject\n\nContributions\n are welcome to implement missing features for non-React frameworks.\n\nFramework Examples\n\nExplore these example implementations for different frameworks:\n\nNext.js\nNuxt\nSvelteKit\nAngular\nAPI Reference\n\nPlease check out the AI SDK UI API Reference for more details on each function.\n\nPrevious\nAI SDK UI\nNext\nChatbot"
  },
  {
    "title": "AI SDK UI: Chatbot Message Persistence",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
    "html": "AI SDK UI\nChatbot Message Persistence\nCopy markdown\nChatbot Message Persistence\n\nBeing able to store and load chat messages is crucial for most AI chatbots. In this guide, we'll show how to implement message persistence with useChat and streamText.\n\nThis guide does not cover authorization, error handling, or other real-world considerations. It is intended to be a simple example of how to implement message persistence.\n\nStarting a new chat\n\nWhen the user navigates to the chat page without providing a chat ID, we need to create a new chat and redirect to the chat page with the new chat ID.\n\napp/chat/page.tsx\nimport { redirect } from 'next/navigation';\nimport { createChat } from '@util/chat-store';\n\n\nexport default async function Page() {\n  const id = await createChat(); // create a new chat\n  redirect(`/chat/${id}`); // redirect to chat page, see below\n}\n\nOur example chat store implementation uses files to store the chat messages. In a real-world application, you would use a database or a cloud storage service, and get the chat ID from the database. That being said, the function interfaces are designed to be easily replaced with other implementations.\n\nutil/chat-store.ts\nimport { generateId } from 'ai';\nimport { existsSync, mkdirSync } from 'fs';\nimport { writeFile } from 'fs/promises';\nimport path from 'path';\n\n\nexport async function createChat(): Promise<string> {\n  const id = generateId(); // generate a unique chat ID\n  await writeFile(getChatFile(id), '[]'); // create an empty chat file\n  return id;\n}\n\n\nfunction getChatFile(id: string): string {\n  const chatDir = path.join(process.cwd(), '.chats');\n  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });\n  return path.join(chatDir, `${id}.json`);\n}\nLoading an existing chat\n\nWhen the user navigates to the chat page with a chat ID, we need to load the chat messages from storage.\n\nThe loadChat function in our file-based chat store is implemented as follows:\n\nutil/chat-store.ts\nimport { UIMessage } from 'ai';\nimport { readFile } from 'fs/promises';\n\n\nexport async function loadChat(id: string): Promise<UIMessage[]> {\n  return JSON.parse(await readFile(getChatFile(id), 'utf8'));\n}\n\n\n// ... rest of the file\nValidating messages on the server\n\nWhen processing messages on the server that contain tool calls, custom metadata, or data parts, you should validate them using validateUIMessages before sending them to the model.\n\nValidation with tools\n\nWhen your messages include tool calls, validate them against your tool definitions:\n\napp/api/chat/route.ts\nimport {\n  convertToModelMessages,\n  streamText,\n  UIMessage,\n  validateUIMessages,\n  tool,\n} from 'ai';\nimport { z } from 'zod';\nimport { loadChat, saveChat } from '@util/chat-store';\nimport { openai } from '@ai-sdk/openai';\nimport { dataPartsSchema, metadataSchema } from '@util/schemas';\n\n\n// Define your tools\nconst tools = {\n  weather: tool({\n    description: 'Get weather information',\n    parameters: z.object({\n      location: z.string(),\n      units: z.enum(['celsius', 'fahrenheit']),\n    }),\n    execute: async ({ location, units }) => {\n      /* tool implementation */\n    },\n  }),\n  // other tools\n};\n\n\nexport async function POST(req: Request) {\n  const { message, id } = await req.json();\n\n\n  // Load previous messages from database\n  const previousMessages = await loadChat(id);\n\n\n  // Append new message to previousMessages messages\n  const messages = [...previousMessages, message];\n\n\n  // Validate loaded messages against\n  // tools, data parts schema, and metadata schema\n  const validatedMessages = await validateUIMessages({\n    messages,\n    tools, // Ensures tool calls in messages match current schemas\n    dataPartsSchema,\n    metadataSchema,\n  });\n\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages: convertToModelMessages(validatedMessages),\n    tools,\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId: id, messages });\n    },\n  });\n}\nHandling validation errors\n\nHandle validation errors gracefully when messages from the database don't match current schemas:\n\napp/api/chat/route.ts\nimport {\n  convertToModelMessages,\n  streamText,\n  validateUIMessages,\n  TypeValidationError,\n} from 'ai';\nimport { type MyUIMessage } from '@/types';\n\n\nexport async function POST(req: Request) {\n  const { message, id } = await req.json();\n\n\n  // Load and validate messages from database\n  let validatedMessages: MyUIMessage[];\n\n\n  try {\n    const previousMessages = await loadMessagesFromDB(id);\n    validatedMessages = await validateUIMessages({\n      // append the new message to the previous messages:\n      messages: [...previousMessages, message],\n      tools,\n      metadataSchema,\n    });\n  } catch (error) {\n    if (error instanceof TypeValidationError) {\n      // Log validation error for monitoring\n      console.error('Database messages validation failed:', error);\n      // Could implement message migration or filtering here\n      // For now, start with empty history\n      validatedMessages = [];\n    } else {\n      throw error;\n    }\n  }\n\n\n  // Continue with validated messages...\n}\nDisplaying the chat\n\nOnce messages are loaded from storage, you can display them in your chat UI. Here's how to set up the page component and the chat display:\n\napp/chat/[id]/page.tsx\nimport { loadChat } from '@util/chat-store';\nimport Chat from '@ui/chat';\n\n\nexport default async function Page(props: { params: Promise<{ id: string }> }) {\n  const { id } = await props.params;\n  const messages = await loadChat(id);\n  return <Chat id={id} initialMessages={messages} />;\n}\n\nThe chat component uses the useChat hook to manage the conversation:\n\nui/chat.tsx\n'use client';\n\n\nimport { UIMessage, useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat({\n  id,\n  initialMessages,\n}: { id?: string | undefined; initialMessages?: UIMessage[] } = {}) {\n  const [input, setInput] = useState('');\n  const { sendMessage, messages } = useChat({\n    id, // use the provided chat ID\n    messages: initialMessages, // load initial messages\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    if (input.trim()) {\n      sendMessage({ text: input });\n      setInput('');\n    }\n  };\n\n\n  // simplified rendering code, extend as needed:\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.parts\n            .map(part => (part.type === 'text' ? part.text : ''))\n            .join('')}\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\nStoring messages\n\nuseChat sends the chat id and the messages to the backend.\n\nThe useChat message format is different from the ModelMessage format. The useChat message format is designed for frontend display, and contains additional fields such as id and createdAt. We recommend storing the messages in the useChat message format.\n\nWhen loading messages from storage that contain tools, metadata, or custom data parts, validate them using validateUIMessages before processing (see the validation section above).\n\nStoring messages is done in the onFinish callback of the toUIMessageStreamResponse function. onFinish receives the complete messages including the new AI response as UIMessage[].\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { saveChat } from '@util/chat-store';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\n    await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId, messages });\n    },\n  });\n}\n\nThe actual storage of the messages is done in the saveChat function, which in our file-based chat store is implemented as follows:\n\nutil/chat-store.ts\nimport { UIMessage } from 'ai';\nimport { writeFile } from 'fs/promises';\n\n\nexport async function saveChat({\n  chatId,\n  messages,\n}: {\n  chatId: string;\n  messages: UIMessage[];\n}): Promise<void> {\n  const content = JSON.stringify(messages, null, 2);\n  await writeFile(getChatFile(chatId), content);\n}\n\n\n// ... rest of the file\nMessage IDs\n\nIn addition to a chat ID, each message has an ID. You can use this message ID to e.g. manipulate individual messages.\n\nClient-side vs Server-side ID Generation\n\nBy default, message IDs are generated client-side:\n\nUser message IDs are generated by the useChat hook on the client\nAI response message IDs are generated by streamText on the server\n\nFor applications without persistence, client-side ID generation works perfectly. However, for persistence, you need server-side generated IDs to ensure consistency across sessions and prevent ID conflicts when messages are stored and retrieved.\n\nSetting Up Server-side ID Generation\n\nWhen implementing persistence, you have two options for generating server-side IDs:\n\nUsing generateMessageId in toUIMessageStreamResponse\nSetting IDs in your start message part with createUIMessageStream\nOption 1: Using generateMessageId in toUIMessageStreamResponse\n\nYou can control the ID format by providing ID generators using createIdGenerator():\n\napp/api/chat/route.ts\nimport { createIdGenerator, streamText } from 'ai';\n\n\nexport async function POST(req: Request) {\n  // ...\n  const result = streamText({\n    // ...\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    // Generate consistent server-side IDs for persistence:\n    generateMessageId: createIdGenerator({\n      prefix: 'msg',\n      size: 16,\n    }),\n    onFinish: ({ messages }) => {\n      saveChat({ chatId, messages });\n    },\n  });\n}\nOption 2: Setting IDs with createUIMessageStream\n\nAlternatively, you can use createUIMessageStream to control the message ID by writing a start message part:\n\napp/api/chat/route.ts\nimport {\n  generateId,\n  streamText,\n  createUIMessageStream,\n  createUIMessageStreamResponse,\n} from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages, chatId } = await req.json();\n\n\n  const stream = createUIMessageStream({\n    execute: ({ writer }) => {\n      // Write start message part with custom ID\n      writer.write({\n        type: 'start',\n        messageId: generateId(), // Generate server-side ID for persistence\n      });\n\n\n      const result = streamText({\n        model: openai('gpt-4o-mini'),\n        messages: convertToModelMessages(messages),\n      });\n\n\n      writer.merge(result.toUIMessageStream({ sendStart: false })); // omit start message part\n    },\n    originalMessages: messages,\n    onFinish: ({ responseMessage }) => {\n      // save your chat here\n    },\n  });\n\n\n  return createUIMessageStreamResponse({ stream });\n}\n\nFor client-side applications that don't require persistence, you can still customize client-side ID generation:\n\nui/chat.tsx\nimport { createIdGenerator } from 'ai';\nimport { useChat } from '@ai-sdk/react';\n\n\nconst { ... } = useChat({\n  generateId: createIdGenerator({\n    prefix: 'msgc',\n    size: 16,\n  }),\n  // ...\n});\nSending only the last message\n\nOnce you have implemented message persistence, you might want to send only the last message to the server. This reduces the amount of data sent to the server on each request and can improve performance.\n\nTo achieve this, you can provide a prepareSendMessagesRequest function to the transport. This function receives the messages and the chat ID, and returns the request body to be sent to the server.\n\nui/chat.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst {\n  // ...\n} = useChat({\n  // ...\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n    // only send the last message to the server:\n    prepareSendMessagesRequest({ messages, id }) {\n      return { body: { message: messages[messages.length - 1], id } };\n    },\n  }),\n});\n\nOn the server, you can then load the previous messages and append the new message to the previous messages. If your messages contain tools, metadata, or custom data parts, you should validate them:\n\napp/api/chat/route.ts\nimport { convertToModelMessages, UIMessage, validateUIMessages } from 'ai';\n// import your tools and schemas\n\n\nexport async function POST(req: Request) {\n  // get the last message from the client:\n  const { message, id } = await req.json();\n\n\n  // load the previous messages from the server:\n  const previousMessages = await loadChat(id);\n\n\n  // validate messages if they contain tools, metadata, or data parts:\n  const validatedMessages = await validateUIMessages({\n    // append the new message to the previous messages:\n    messages: [...previousMessages, message],\n    tools, // if using tools\n    metadataSchema, // if using custom metadata\n    dataSchemas, // if using custom data parts\n  });\n\n\n  const result = streamText({\n    // ...\n    messages: convertToModelMessages(validatedMessages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: validatedMessages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId: id, messages });\n    },\n  });\n}\nHandling client disconnects\n\nBy default, the AI SDK streamText function uses backpressure to the language model provider to prevent the consumption of tokens that are not yet requested.\n\nHowever, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue, the stream from the LLM will be aborted and the conversation may end up in a broken state.\n\nAssuming that you have a storage solution in place, you can use the consumeStream method to consume the stream on the backend, and then save the result as usual. consumeStream effectively removes the backpressure, meaning that the result is stored even when the client has already disconnected.\n\napp/api/chat/route.ts\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\nimport { saveChat } from '@util/chat-store';\n\n\nexport async function POST(req: Request) {\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\n    await req.json();\n\n\n  const result = streamText({\n    model,\n    messages: convertToModelMessages(messages),\n  });\n\n\n  // consume the stream to ensure it runs to completion & triggers onFinish\n  // even when the client response is aborted:\n  result.consumeStream(); // no await\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId, messages });\n    },\n  });\n}\n\nWhen the client reloads the page after a disconnect, the chat will be restored from the storage solution.\n\nIn production applications, you would also track the state of the request (in progress, complete) in your stored messages and use it on the client to cover the case where the client reloads the page after a disconnection, but the streaming is not yet complete.\n\nFor more robust handling of disconnects, you may want to add resumability on disconnects. Check out the Chatbot Resume Streams documentation to learn more.\n\nPrevious\nChatbot\nNext\nChatbot Resume Streams"
  },
  {
    "title": "AI SDK UI: Chatbot",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
    "html": "AI SDK UI\nChatbot\nCopy markdown\nChatbot\n\nThe useChat hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.\n\nTo summarize, the useChat hook provides the following features:\n\nMessage Streaming: All the messages from the AI provider are streamed to the chat UI in real-time.\nManaged States: The hook manages the states for input, messages, status, error and more for you.\nSeamless Integration: Easily integrate your chat AI into any design or layout with minimal effort.\n\nIn this guide, you will learn how to use the useChat hook to create a chatbot application with real-time message streaming. Check out our chatbot with tools guide to learn how to use tools in your chatbot. Let's start with the following example first.\n\nExample\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n          placeholder=\"Say something...\"\n        />\n        <button type=\"submit\" disabled={status !== 'ready'}>\n          Submit\n        </button>\n      </form>\n    </>\n  );\n}\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    system: 'You are a helpful assistant.',\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nThe UI messages have a new parts property that contains the message parts. We recommend rendering the messages using the parts property instead of the content property. The parts property supports different message types, including text, tool invocation, and tool result, and allows for more flexible and complex chat UIs.\n\nIn the Page component, the useChat hook will request to your AI provider endpoint whenever the user sends a message using sendMessage. The messages are then streamed back in real-time and displayed in the chat UI.\n\nThis enables a seamless chat experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\n\nCustomized UI\n\nuseChat also provides ways to manage the chat message states via code, show status, and update messages without being triggered by user interactions.\n\nStatus\n\nThe useChat hook returns a status. It has the following possible values:\n\nsubmitted: The message has been sent to the API and we're awaiting the start of the response stream.\nstreaming: The response is actively streaming in from the API, receiving chunks of data.\nready: The full response has been received and processed; a new user message can be submitted.\nerror: An error occurred during the API request, preventing successful completion.\n\nYou can use status for e.g. the following purposes:\n\nTo show a loading spinner while the chatbot is processing the user's message.\nTo show a \"Stop\" button to abort the current message.\nTo disable the submit button.\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status, stop } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      {(status === 'submitted' || status === 'streaming') && (\n        <div>\n          {status === 'submitted' && <Spinner />}\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n          placeholder=\"Say something...\"\n        />\n        <button type=\"submit\" disabled={status !== 'ready'}>\n          Submit\n        </button>\n      </form>\n    </>\n  );\n}\nError State\n\nSimilarly, the error state reflects the error object thrown during the fetch request. It can be used to display an error message, disable the submit button, or show a retry button:\n\nWe recommend showing a generic error message to the user, such as \"Something went wrong.\" This is a good practice to avoid leaking information from the server.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, error, reload } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      {error && (\n        <>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => reload()}>\n            Retry\n          </button>\n        </>\n      )}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={error != null}\n        />\n      </form>\n    </div>\n  );\n}\n\nPlease also see the error handling guide for more information.\n\nModify messages\n\nSometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.\n\nThe setMessages function can help you achieve these tasks:\n\nconst { messages, setMessages } = useChat()\n\n\nconst handleDelete = (id) => {\n  setMessages(messages.filter(message => message.id !== id))\n}\n\n\nreturn <>\n  {messages.map(message => (\n    <div key={message.id}>\n      {message.role === 'user' ? 'User: ' : 'AI: '}\n      {message.parts.map((part, index) => (\n        part.type === 'text' ? (\n          <span key={index}>{part.text}</span>\n        ) : null\n      ))}\n      <button onClick={() => handleDelete(message.id)}>Delete</button>\n    </div>\n  ))}\n  ...\n\nYou can think of messages and setMessages as a pair of state and setState in React.\n\nCancellation and regeneration\n\nIt's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the stop function returned by the useChat hook.\n\nconst { stop, status } = useChat()\n\n\nreturn <>\n  <button onClick={stop} disabled={!(status === 'streaming' || status === 'submitted')}>Stop</button>\n  ...\n\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.\n\nSimilarly, you can also request the AI provider to reprocess the last message by calling the regenerate function returned by the useChat hook:\n\nconst { regenerate, status } = useChat();\n\n\nreturn (\n  <>\n    <button\n      onClick={regenerate}\n      disabled={!(status === 'ready' || status === 'error')}\n    >\n      Regenerate\n    </button>\n    ...\n  </>\n);\n\nWhen the user clicks the \"Regenerate\" button, the AI provider will regenerate the last message and replace the current one correspondingly.\n\nThrottling UI Updates\nThis feature is currently only available for React.\n\nBy default, the useChat hook will trigger a render every time a new chunk is received. You can throttle the UI updates with the experimental_throttle option.\n\npage.tsx\nconst { messages, ... } = useChat({\n  // Throttle the messages and data updates to 50ms:\n  experimental_throttle: 50\n})\nEvent Callbacks\n\nuseChat provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:\n\nonFinish: Called when the assistant response is completed. The event includes the response message, all messages, and flags for abort, disconnect, and errors.\nonError: Called when an error occurs during the fetch request.\nonData: Called whenever a data part is received.\n\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\nimport { UIMessage } from 'ai';\n\n\nconst {\n  /* ... */\n} = useChat({\n  onFinish: ({ message, messages, isAbort, isDisconnect, isError }) => {\n    // use information to e.g. update other UI states\n  },\n  onError: error => {\n    console.error('An error occurred:', error);\n  },\n  onData: data => {\n    console.log('Received data part from server:', data);\n  },\n});\n\nIt's worth noting that you can abort the processing by throwing an error in the onData callback. This will trigger the onError callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\n\nRequest Configuration\nCustom headers, body, and credentials\n\nBy default, the useChat hook sends a HTTP POST request to the /api/chat endpoint with the message list as the request body. You can customize the request in two ways:\n\nHook-Level Configuration (Applied to all requests)\n\nYou can configure transport-level options that will be applied to all requests made by the hook:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/custom-chat',\n    headers: {\n      Authorization: 'your_token',\n    },\n    body: {\n      user_id: '123',\n    },\n    credentials: 'same-origin',\n  }),\n});\nDynamic Hook-Level Configuration\n\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/custom-chat',\n    headers: () => ({\n      Authorization: `Bearer ${getAuthToken()}`,\n      'X-User-ID': getCurrentUserId(),\n    }),\n    body: () => ({\n      sessionId: getCurrentSessionId(),\n      preferences: getUserPreferences(),\n    }),\n    credentials: () => 'include',\n  }),\n});\n\nFor component state that changes over time, use useRef to store the current value and reference ref.current in your configuration function, or prefer request-level options (see next section) for better reliability.\n\nRequest-Level Configuration (Recommended)\n\nRecommended: Use request-level options for better flexibility and control. Request-level options take precedence over hook-level options and allow you to customize each request individually.\n\n// Pass options as the second parameter to sendMessage\nsendMessage(\n  { text: input },\n  {\n    headers: {\n      Authorization: 'Bearer token123',\n      'X-Custom-Header': 'custom-value',\n    },\n    body: {\n      temperature: 0.7,\n      max_tokens: 100,\n      user_id: '123',\n    },\n    metadata: {\n      userId: 'user123',\n      sessionId: 'session456',\n    },\n  },\n);\n\nThe request-level options are merged with hook-level options, with request-level options taking precedence. On your server side, you can handle the request with this additional information.\n\nSetting custom body fields per request\n\nYou can configure custom body fields on a per-request basis using the second parameter of the sendMessage function. This is useful if you want to pass in additional information to your backend that is not part of the message list.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage } = useChat();\n  const [input, setInput] = useState('');\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={event => {\n          event.preventDefault();\n          if (input.trim()) {\n            sendMessage(\n              { text: input },\n              {\n                body: {\n                  customKey: 'customValue',\n                },\n              },\n            );\n            setInput('');\n          }\n        }}\n      >\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </div>\n  );\n}\n\nYou can retrieve these custom fields on your server side by destructuring the request body:\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  // Extract additional information (\"customKey\") from the body of the request:\n  const { messages, customKey }: { messages: UIMessage[]; customKey: string } =\n    await req.json();\n  //...\n}\nMessage Metadata\n\nYou can attach custom metadata to messages for tracking information like timestamps, model details, and token usage.\n\n// Server: Send metadata about the message\nreturn result.toUIMessageStreamResponse({\n  messageMetadata: ({ part }) => {\n    if (part.type === 'start') {\n      return {\n        createdAt: Date.now(),\n        model: 'gpt-4o',\n      };\n    }\n\n\n    if (part.type === 'finish') {\n      return {\n        totalTokens: part.totalUsage.totalTokens,\n      };\n    }\n  },\n});\n// Client: Access metadata via message.metadata\n{\n  messages.map(message => (\n    <div key={message.id}>\n      {message.role}:{' '}\n      {message.metadata?.createdAt &&\n        new Date(message.metadata.createdAt).toLocaleTimeString()}\n      {/* Render message content */}\n      {message.parts.map((part, index) =>\n        part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n      )}\n      {/* Show token count if available */}\n      {message.metadata?.totalTokens && (\n        <span>{message.metadata.totalTokens} tokens</span>\n      )}\n    </div>\n  ));\n}\n\nFor complete examples with type safety and advanced use cases, see the Message Metadata documentation.\n\nTransport Configuration\n\nYou can configure custom transport behavior using the transport option to customize how messages are sent to your API:\n\napp/page.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  const { messages, sendMessage } = useChat({\n    id: 'my-chat',\n    transport: new DefaultChatTransport({\n      prepareSendMessagesRequest: ({ id, messages }) => {\n        return {\n          body: {\n            id,\n            message: messages[messages.length - 1],\n          },\n        };\n      },\n    }),\n  });\n\n\n  // ... rest of your component\n}\n\nThe corresponding API route receives the custom request format:\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { id, message } = await req.json();\n\n\n  // Load existing messages and add the new one\n  const messages = await loadMessages(id);\n  messages.push(message);\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nAdvanced: Trigger-based routing\n\nFor more complex scenarios like message regeneration, you can use trigger-based routing:\n\napp/page.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, regenerate } = useChat({\n    id: 'my-chat',\n    transport: new DefaultChatTransport({\n      prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\n        if (trigger === 'submit-user-message') {\n          return {\n            body: {\n              trigger: 'submit-user-message',\n              id,\n              message: messages[messages.length - 1],\n              messageId,\n            },\n          };\n        } else if (trigger === 'regenerate-assistant-message') {\n          return {\n            body: {\n              trigger: 'regenerate-assistant-message',\n              id,\n              messageId,\n            },\n          };\n        }\n        throw new Error(`Unsupported trigger: ${trigger}`);\n      },\n    }),\n  });\n\n\n  // ... rest of your component\n}\n\nThe corresponding API route would handle different triggers:\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { trigger, id, message, messageId } = await req.json();\n\n\n  const chat = await readChat(id);\n  let messages = chat.messages;\n\n\n  if (trigger === 'submit-user-message') {\n    // Handle new user message\n    messages = [...messages, message];\n  } else if (trigger === 'regenerate-assistant-message') {\n    // Handle message regeneration - remove messages after messageId\n    const messageIndex = messages.findIndex(m => m.id === messageId);\n    if (messageIndex !== -1) {\n      messages = messages.slice(0, messageIndex);\n    }\n  }\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nTo learn more about building custom transports, refer to the Transport API documentation.\n\nControlling the response stream\n\nWith streamText, you can control how error messages and usage information are sent back to the client.\n\nError Messages\n\nBy default, the error message is masked for security reasons. The default error message is \"An error occurred.\" You can forward error messages or send your own error message by providing a getErrorMessage function:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    onError: error => {\n      if (error == null) {\n        return 'unknown error';\n      }\n\n\n      if (typeof error === 'string') {\n        return error;\n      }\n\n\n      if (error instanceof Error) {\n        return error.message;\n      }\n\n\n      return JSON.stringify(error);\n    },\n  });\n}\nUsage Information\n\nTrack token consumption and resource usage with message metadata:\n\nDefine a custom metadata type with usage fields (optional, for type safety)\nAttach usage data using messageMetadata in your response\nDisplay usage metrics in your UI components\n\nUsage data is attached as metadata to messages and becomes available once the model completes its response generation.\n\nimport { openai } from '@ai-sdk/openai';\nimport {\n  convertToModelMessages,\n  streamText,\n  UIMessage,\n  type LanguageModelUsage,\n} from 'ai';\n\n\n// Create a new metadata type (optional for type-safety)\ntype MyMetadata = {\n  totalUsage: LanguageModelUsage;\n};\n\n\n// Create a new custom message type with your own metadata\nexport type MyUIMessage = UIMessage<MyMetadata>;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    messageMetadata: ({ part }) => {\n      // Send total usage when generation is finished\n      if (part.type === 'finish') {\n        return { totalUsage: part.totalUsage };\n      }\n    },\n  });\n}\n\nThen, on the client, you can access the message-level metadata.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport type { MyUIMessage } from './api/chat/route';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  // Use custom message type defined on the server (optional for type-safety)\n  const { messages } = useChat<MyUIMessage>({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(m => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.parts.map(part => {\n            if (part.type === 'text') {\n              return part.text;\n            }\n          })}\n          {/* Render usage via metadata */}\n          {m.metadata?.totalUsage && (\n            <div>Total usage: {m.metadata?.totalUsage.totalTokens} tokens</div>\n          )}\n        </div>\n      ))}\n    </div>\n  );\n}\n\nYou can also access your metadata from the onFinish callback of useChat:\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport type { MyUIMessage } from './api/chat/route';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  // Use custom message type defined on the server (optional for type-safety)\n  const { messages } = useChat<MyUIMessage>({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n    onFinish: ({ message }) => {\n      // Access message metadata via onFinish callback\n      console.log(message.metadata?.totalUsage);\n    },\n  });\n}\nText Streams\n\nuseChat can handle plain text streams by setting the streamProtocol option to text:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { TextStreamChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  const { messages } = useChat({\n    transport: new TextStreamChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  return <>...</>;\n}\n\nThis configuration also works with other backend servers that stream plain text. Check out the stream protocol guide for more information.\n\nWhen using TextStreamChatTransport, tool calls, usage information and finish reasons are not available.\n\nReasoning\n\nSome models such as as DeepSeek deepseek-reasoner and Anthropic claude-3-7-sonnet-20250219 support reasoning tokens. These tokens are typically sent before the message content. You can forward them to the client with the sendReasoning option:\n\napp/api/chat/route.ts\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: deepseek('deepseek-reasoner'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    sendReasoning: true,\n  });\n}\n\nOn the client side, you can access the reasoning parts of the message object.\n\nReasoning parts have a text property that contains the reasoning content.\n\napp/page.tsx\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts.map((part, index) => {\n      // text parts:\n      if (part.type === 'text') {\n        return <div key={index}>{part.text}</div>;\n      }\n\n\n      // reasoning parts:\n      if (part.type === 'reasoning') {\n        return <pre key={index}>{part.text}</pre>;\n      }\n    })}\n  </div>\n));\nSources\n\nSome providers such as Perplexity and Google Generative AI include sources in the response.\n\nCurrently sources are limited to web pages that ground the response. You can forward them to the client with the sendSources option:\n\napp/api/chat/route.ts\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: perplexity('sonar-pro'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    sendSources: true,\n  });\n}\n\nOn the client side, you can access source parts of the message object. There are two types of sources: source-url for web pages and source-document for documents. Here is an example that renders both types of sources:\n\napp/page.tsx\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n\n\n    {/* Render URL sources */}\n    {message.parts\n      .filter(part => part.type === 'source-url')\n      .map(part => (\n        <span key={`source-${part.id}`}>\n          [\n          <a href={part.url} target=\"_blank\">\n            {part.title ?? new URL(part.url).hostname}\n          </a>\n          ]\n        </span>\n      ))}\n\n\n    {/* Render document sources */}\n    {message.parts\n      .filter(part => part.type === 'source-document')\n      .map(part => (\n        <span key={`source-${part.id}`}>\n          [<span>{part.title ?? `Document ${part.id}`}</span>]\n        </span>\n      ))}\n  </div>\n));\nImage Generation\n\nSome models such as Google gemini-2.5-flash-image-preview support image generation. When images are generated, they are exposed as files to the client. On the client side, you can access file parts of the message object and render them as images.\n\napp/page.tsx\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts.map((part, index) => {\n      if (part.type === 'text') {\n        return <div key={index}>{part.text}</div>;\n      } else if (part.type === 'file' && part.mediaType.startsWith('image/')) {\n        return <img key={index} src={part.url} alt=\"Generated image\" />;\n      }\n    })}\n  </div>\n));\nAttachments\n\nThe useChat hook supports sending file attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.\n\nThere are two ways to send files with a message: using a FileList object from file inputs or using an array of file objects.\n\nFileList\n\nBy using FileList, you can send multiple files as attachments along with a message using the file input element. The useChat hook will automatically convert them into data URLs and send them to the AI provider.\n\nCurrently, only image/* and text/* content types get automatically converted into multi-modal content parts. You will need to handle other content types manually.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useRef, useState } from 'react';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status } = useChat();\n\n\n  const [input, setInput] = useState('');\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\n  const fileInputRef = useRef<HTMLInputElement>(null);\n\n\n  return (\n    <div>\n      <div>\n        {messages.map(message => (\n          <div key={message.id}>\n            <div>{`${message.role}: `}</div>\n\n\n            <div>\n              {message.parts.map((part, index) => {\n                if (part.type === 'text') {\n                  return <span key={index}>{part.text}</span>;\n                }\n\n\n                if (\n                  part.type === 'file' &&\n                  part.mediaType?.startsWith('image/')\n                ) {\n                  return <img key={index} src={part.url} alt={part.filename} />;\n                }\n\n\n                return null;\n              })}\n            </div>\n          </div>\n        ))}\n      </div>\n\n\n      <form\n        onSubmit={event => {\n          event.preventDefault();\n          if (input.trim()) {\n            sendMessage({\n              text: input,\n              files,\n            });\n            setInput('');\n            setFiles(undefined);\n\n\n            if (fileInputRef.current) {\n              fileInputRef.current.value = '';\n            }\n          }\n        }}\n      >\n        <input\n          type=\"file\"\n          onChange={event => {\n            if (event.target.files) {\n              setFiles(event.target.files);\n            }\n          }}\n          multiple\n          ref={fileInputRef}\n        />\n        <input\n          value={input}\n          placeholder=\"Send message...\"\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n        />\n      </form>\n    </div>\n  );\n}\nFile Objects\n\nYou can also send files as objects along with a message. This can be useful for sending pre-uploaded files or data URLs.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport { FileUIPart } from 'ai';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status } = useChat();\n\n\n  const [input, setInput] = useState('');\n  const [files] = useState<FileUIPart[]>([\n    {\n      type: 'file',\n      filename: 'earth.png',\n      mediaType: 'image/png',\n      url: 'https://example.com/earth.png',\n    },\n    {\n      type: 'file',\n      filename: 'moon.png',\n      mediaType: 'image/png',\n      url: 'data:image/png;base64,iVBORw0KGgo...',\n    },\n  ]);\n\n\n  return (\n    <div>\n      <div>\n        {messages.map(message => (\n          <div key={message.id}>\n            <div>{`${message.role}: `}</div>\n\n\n            <div>\n              {message.parts.map((part, index) => {\n                if (part.type === 'text') {\n                  return <span key={index}>{part.text}</span>;\n                }\n\n\n                if (\n                  part.type === 'file' &&\n                  part.mediaType?.startsWith('image/')\n                ) {\n                  return <img key={index} src={part.url} alt={part.filename} />;\n                }\n\n\n                return null;\n              })}\n            </div>\n          </div>\n        ))}\n      </div>\n\n\n      <form\n        onSubmit={event => {\n          event.preventDefault();\n          if (input.trim()) {\n            sendMessage({\n              text: input,\n              files,\n            });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          placeholder=\"Send message...\"\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n        />\n      </form>\n    </div>\n  );\n}\nType Inference for Tools\n\nWhen working with tools in TypeScript, AI SDK UI provides type inference helpers to ensure type safety for your tool inputs and outputs.\n\nInferUITool\n\nThe InferUITool type helper infers the input and output types of a single tool for use in UI messages:\n\nimport { InferUITool } from 'ai';\nimport { z } from 'zod';\n\n\nconst weatherTool = {\n  description: 'Get the current weather',\n  inputSchema: z.object({\n    location: z.string().describe('The city and state'),\n  }),\n  execute: async ({ location }) => {\n    return `The weather in ${location} is sunny.`;\n  },\n};\n\n\n// Infer the types from the tool\ntype WeatherUITool = InferUITool<typeof weatherTool>;\n// This creates a type with:\n// {\n//   input: { location: string };\n//   output: string;\n// }\nInferUITools\n\nThe InferUITools type helper infers the input and output types of a ToolSet:\n\nimport { InferUITools, ToolSet } from 'ai';\nimport { z } from 'zod';\n\n\nconst tools = {\n  weather: {\n    description: 'Get the current weather',\n    inputSchema: z.object({\n      location: z.string().describe('The city and state'),\n    }),\n    execute: async ({ location }) => {\n      return `The weather in ${location} is sunny.`;\n    },\n  },\n  calculator: {\n    description: 'Perform basic arithmetic',\n    inputSchema: z.object({\n      operation: z.enum(['add', 'subtract', 'multiply', 'divide']),\n      a: z.number(),\n      b: z.number(),\n    }),\n    execute: async ({ operation, a, b }) => {\n      switch (operation) {\n        case 'add':\n          return a + b;\n        case 'subtract':\n          return a - b;\n        case 'multiply':\n          return a * b;\n        case 'divide':\n          return a / b;\n      }\n    },\n  },\n} satisfies ToolSet;\n\n\n// Infer the types from the tool set\ntype MyUITools = InferUITools<typeof tools>;\n// This creates a type with:\n// {\n//   weather: { input: { location: string }; output: string };\n//   calculator: { input: { operation: 'add' | 'subtract' | 'multiply' | 'divide'; a: number; b: number }; output: number };\n// }\nUsing Inferred Types\n\nYou can use these inferred types to create a custom UIMessage type and pass it to various AI SDK UI functions:\n\nimport { InferUITools, UIMessage, UIDataTypes } from 'ai';\n\n\ntype MyUITools = InferUITools<typeof tools>;\ntype MyUIMessage = UIMessage<never, UIDataTypes, MyUITools>;\n\nPass the custom type to useChat or createUIMessageStream:\n\nimport { useChat } from '@ai-sdk/react';\nimport { createUIMessageStream } from 'ai';\nimport type { MyUIMessage } from './types';\n\n\n// With useChat\nconst { messages } = useChat<MyUIMessage>();\n\n\n// With createUIMessageStream\nconst stream = createUIMessageStream<MyUIMessage>(/* ... */);\n\nThis provides full type safety for tool inputs and outputs on the client and server.\n\nPrevious\nOverview\nNext\nChatbot Message Persistence"
  },
  {
    "title": "AI SDK UI: Chatbot Resume Streams",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams",
    "html": "AI SDK UI\nChatbot Resume Streams\nCopy markdown\nChatbot Resume Streams\n\nuseChat supports resuming ongoing streams after page reloads. Use this feature to build applications with long-running generations.\n\nStream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use resume: true if you need abort functionality in your application. See troubleshooting for more details.\n\nHow stream resumption works\n\nStream resumption requires persistence for messages and active streams in your application. The AI SDK provides tools to connect to storage, but you need to set up the storage yourself.\n\nThe AI SDK provides:\n\nA resume option in useChat that automatically reconnects to active streams\nAccess to the outgoing stream through the consumeSseStream callback\nAutomatic HTTP requests to your resume endpoints\n\nYou build:\n\nStorage to track which stream belongs to each chat\nRedis to store the UIMessage stream\nTwo API endpoints: POST to create streams, GET to resume them\nIntegration with resumable-stream\n to manage Redis storage\nPrerequisites\n\nTo implement resumable streams in your chat application, you need:\n\nThe resumable-stream package - Handles the publisher/subscriber mechanism for streams\nA Redis instance - Stores stream data (e.g. Redis through Vercel\n)\nA persistence layer - Tracks which stream ID is active for each chat (e.g. database)\nImplementation\n1. Client-side: Enable stream resumption\n\nUse the resume option in the useChat hook to enable stream resumption. When resume is true, the hook automatically attempts to reconnect to any active stream for the chat on mount:\n\napp/chat/[chatId]/chat.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport, type UIMessage } from 'ai';\n\n\nexport function Chat({\n  chatData,\n  resume = false,\n}: {\n  chatData: { id: string; messages: UIMessage[] };\n  resume?: boolean;\n}) {\n  const { messages, sendMessage, status } = useChat({\n    id: chatData.id,\n    messages: chatData.messages,\n    resume, // Enable automatic stream resumption\n    transport: new DefaultChatTransport({\n      // You must send the id of the chat\n      prepareSendMessagesRequest: ({ id, messages }) => {\n        return {\n          body: {\n            id,\n            message: messages[messages.length - 1],\n          },\n        };\n      },\n    }),\n  });\n\n\n  return <div>{/* Your chat UI */}</div>;\n}\n\nYou must send the chat ID with each request (see prepareSendMessagesRequest).\n\nWhen you enable resume, the useChat hook makes a GET request to /api/chat/[id]/stream on mount to check for and resume any active streams.\n\nLet's start by creating the POST handler to create the resumable stream.\n\n2. Create the POST handler\n\nThe POST handler creates resumable streams using the consumeSseStream callback:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { readChat, saveChat } from '@util/chat-store';\nimport {\n  convertToModelMessages,\n  generateId,\n  streamText,\n  type UIMessage,\n} from 'ai';\nimport { after } from 'next/server';\nimport { createResumableStreamContext } from 'resumable-stream';\n\n\nexport async function POST(req: Request) {\n  const {\n    message,\n    id,\n  }: {\n    message: UIMessage | undefined;\n    id: string;\n  } = await req.json();\n\n\n  const chat = await readChat(id);\n  let messages = chat.messages;\n\n\n  messages = [...messages, message!];\n\n\n  // Clear any previous active stream and save the user message\n  saveChat({ id, messages, activeStreamId: null });\n\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    generateMessageId: generateId,\n    onFinish: ({ messages }) => {\n      // Clear the active stream when finished\n      saveChat({ id, messages, activeStreamId: null });\n    },\n    async consumeSseStream({ stream }) {\n      const streamId = generateId();\n\n\n      // Create a resumable stream from the SSE stream\n      const streamContext = createResumableStreamContext({ waitUntil: after });\n      await streamContext.createNewResumableStream(streamId, () => stream);\n\n\n      // Update the chat with the active stream ID\n      saveChat({ id, activeStreamId: streamId });\n    },\n  });\n}\n3. Implement the GET handler\n\nCreate a GET handler at /api/chat/[id]/stream that:\n\nReads the chat ID from the route params\nLoads the chat data to check for an active stream\nReturns 204 (No Content) if no stream is active\nResumes the existing stream if one is found\napp/api/chat/[id]/stream/route.ts\nimport { readChat } from '@util/chat-store';\nimport { UI_MESSAGE_STREAM_HEADERS } from 'ai';\nimport { after } from 'next/server';\nimport { createResumableStreamContext } from 'resumable-stream';\n\n\nexport async function GET(\n  _: Request,\n  { params }: { params: Promise<{ id: string }> },\n) {\n  const { id } = await params;\n\n\n  const chat = await readChat(id);\n\n\n  if (chat.activeStreamId == null) {\n    // no content response when there is no active stream\n    return new Response(null, { status: 204 });\n  }\n\n\n  const streamContext = createResumableStreamContext({\n    waitUntil: after,\n  });\n\n\n  return new Response(\n    await streamContext.resumeExistingStream(chat.activeStreamId),\n    { headers: UI_MESSAGE_STREAM_HEADERS },\n  );\n}\n\nThe after function from Next.js allows work to continue after the response has been sent. This ensures that the resumable stream persists in Redis even after the initial response is returned to the client, enabling reconnection later.\n\nHow it works\nRequest lifecycle\n\nThe diagram above shows the complete lifecycle of a resumable stream:\n\nStream creation: When you send a new message, the POST handler uses streamText to generate the response. The consumeSseStream callback creates a resumable stream with a unique ID and stores it in Redis through the resumable-stream package\nStream tracking: Your persistence layer saves the activeStreamId in the chat data\nClient reconnection: When the client reconnects (page reload), the resume option triggers a GET request to /api/chat/[id]/stream\nStream recovery: The GET handler checks for an activeStreamId and uses resumeExistingStream to reconnect. If no active stream exists, it returns a 204 (No Content) response\nCompletion cleanup: When the stream finishes, the onFinish callback clears the activeStreamId by setting it to null\nCustomize the resume endpoint\n\nBy default, the useChat hook makes a GET request to /api/chat/[id]/stream when resuming. Customize this endpoint, credentials, and headers, using the prepareReconnectToStreamRequest option in DefaultChatTransport:\n\napp/chat/[chatId]/chat.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport function Chat({ chatData, resume }) {\n  const { messages, sendMessage } = useChat({\n    id: chatData.id,\n    messages: chatData.messages,\n    resume,\n    transport: new DefaultChatTransport({\n      // Customize reconnect settings (optional)\n      prepareReconnectToStreamRequest: ({ id }) => {\n        return {\n          api: `/api/chat/${id}/stream`, // Default pattern\n          // Or use a different pattern:\n          // api: `/api/streams/${id}/resume`,\n          // api: `/api/resume-chat?id=${id}`,\n          credentials: 'include', // Include cookies/auth\n          headers: {\n            Authorization: 'Bearer token',\n            'X-Custom-Header': 'value',\n          },\n        };\n      },\n    }),\n  });\n\n\n  return <div>{/* Your chat UI */}</div>;\n}\n\nThis lets you:\n\nMatch your existing API route structure\nAdd query parameters or custom paths\nIntegrate with different backend architectures\nImportant considerations\nIncompatibility with abort: Stream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use resume: true if you need abort functionality in your application\nStream expiration: Streams in Redis expire after a set time (configurable in the resumable-stream package)\nMultiple clients: Multiple clients can connect to the same stream simultaneously\nError handling: When no active stream exists, the GET handler returns a 204 (No Content) status code\nSecurity: Ensure proper authentication and authorization for both creating and resuming streams\nRace conditions: Clear the activeStreamId when starting a new stream to prevent resuming outdated streams\n\n\nView Example on GitHub\nPrevious\nChatbot Message Persistence\nNext\nChatbot Tool Usage"
  },
  {
    "title": "AI SDK UI: Chatbot Tool Usage",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
    "html": "AI SDK UI\nChatbot Tool Usage\nCopy markdown\nChatbot Tool Usage\n\nWith useChat and streamText, you can use tools in your chatbot application. The AI SDK supports three types of tools in this context:\n\nAutomatically executed server-side tools\nAutomatically executed client-side tools\nTools that require user interaction, such as confirmation dialogs\n\nThe flow is as follows:\n\nThe user enters a message in the chat UI.\nThe message is sent to the API route.\nIn your server side route, the language model generates tool calls during the streamText call.\nAll tool calls are forwarded to the client.\nServer-side tools are executed using their execute method and their results are forwarded to the client.\nClient-side tools that should be automatically executed are handled with the onToolCall callback. You must call addToolResult to provide the tool result.\nClient-side tool that require user interactions can be displayed in the UI. The tool calls and results are available as tool invocation parts in the parts property of the last assistant message.\nWhen the user interaction is done, addToolResult can be used to add the tool result to the chat.\nThe chat can be configured to automatically submit when all tool results are available using sendAutomaticallyWhen. This triggers another iteration of this flow.\n\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts. A tool part is at first a tool call, and then it becomes a tool result when the tool is executed. The tool result contains all information about the tool call as well as the result of the tool execution.\n\nTool result submission can be configured using the sendAutomaticallyWhen option. You can use the lastAssistantMessageIsCompleteWithToolCalls helper to automatically submit when all tool results are available. This simplifies the client-side code while still allowing full control when needed.\n\nExample\n\nIn this example, we'll use three tools:\n\ngetWeatherInformation: An automatically executed server-side tool that returns the weather in a given city.\naskForConfirmation: A user-interaction client-side tool that asks the user for confirmation.\ngetLocation: An automatically executed client-side tool that returns a random city.\nAPI route\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\nimport { z } from 'zod';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      // server-side tool with execute function:\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n      // client-side tool that starts user interaction:\n      askForConfirmation: {\n        description: 'Ask the user for confirmation.',\n        inputSchema: z.object({\n          message: z.string().describe('The message to ask for confirmation.'),\n        }),\n      },\n      // client-side tool that is automatically executed on the client:\n      getLocation: {\n        description:\n          'Get the user location. Always ask for confirmation before using this tool.',\n        inputSchema: z.object({}),\n      },\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nClient-side page\n\nThe client-side page uses the useChat hook to create a chatbot application with real-time message streaming. Tool calls are displayed in the chat UI as typed tool parts. Please make sure to render the messages using the parts property of the message.\n\nThere are three things worth mentioning:\n\nThe onToolCall callback is used to handle client-side tools that should be automatically executed. In this example, the getLocation tool is a client-side tool that returns a random city. You call addToolResult to provide the result (without await to avoid potential deadlocks).\n\nAlways check if (toolCall.dynamic) first in your onToolCall handler. Without this check, TypeScript will throw an error like: Type 'string' is not assignable to type '\"toolName1\" | \"toolName2\"' when you try to use toolCall.toolName in addToolResult.\n\nThe sendAutomaticallyWhen option with lastAssistantMessageIsCompleteWithToolCalls helper automatically submits when all tool results are available.\n\nThe parts array of assistant messages contains tool parts with typed names like tool-askForConfirmation. The client-side tool askForConfirmation is displayed in the UI. It asks the user for confirmation and displays the result once the user confirms or denies the execution. The result is added to the chat using addToolResult with the tool parameter for type safety.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getLocation') {\n        const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];\n\n\n        // No await - avoids potential deadlocks\n        addToolResult({\n          tool: 'getLocation',\n          toolCallId: toolCall.toolCallId,\n          output: cities[Math.floor(Math.random() * cities.length)],\n        });\n      }\n    },\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map(part => {\n            switch (part.type) {\n              // render text parts as simple text:\n              case 'text':\n                return part.text;\n\n\n              // for tool parts, use the typed tool part names:\n              case 'tool-askForConfirmation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Loading confirmation request...</div>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        {part.input.message}\n                        <div>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'Yes, confirmed.',\n                              })\n                            }\n                          >\n                            Yes\n                          </button>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'No, denied',\n                              })\n                            }\n                          >\n                            No\n                          </button>\n                        </div>\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Location access allowed: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={callId}>Error: {part.errorText}</div>;\n                }\n                break;\n              }\n\n\n              case 'tool-getLocation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Preparing location request...</div>\n                    );\n                  case 'input-available':\n                    return <div key={callId}>Getting location...</div>;\n                  case 'output-available':\n                    return <div key={callId}>Location: {part.output}</div>;\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting location: {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n\n\n              case 'tool-getWeatherInformation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  // example of pre-rendering streaming tool inputs:\n                  case 'input-streaming':\n                    return (\n                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        Getting weather information for {part.input.city}...\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Weather in {part.input.city}: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting weather for {part.input.city}:{' '}\n                        {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n            }\n          })}\n          <br />\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </>\n  );\n}\nError handling\n\nSometimes an error may occur during client-side tool execution. Use the addToolResult method with a state of output-error and errorText value instead of output record the error.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getWeatherInformation') {\n        try {\n          const weather = await getWeatherInformation(toolCall.input);\n\n\n          // No await - avoids potential deadlocks\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            output: weather,\n          });\n        } catch (err) {\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            state: 'output-error',\n            errorText: 'Unable to get the weather information',\n          });\n        }\n      }\n    },\n  });\n}\nDynamic Tools\n\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic dynamic-tool type instead of specific tool types:\n\napp/page.tsx\n{\n  message.parts.map((part, index) => {\n    switch (part.type) {\n      // Static tools with specific (`tool-${toolName}`) types\n      case 'tool-getWeatherInformation':\n        return <WeatherDisplay part={part} />;\n\n\n      // Dynamic tools use generic `dynamic-tool` type\n      case 'dynamic-tool':\n        return (\n          <div key={index}>\n            <h4>Tool: {part.toolName}</h4>\n            {part.state === 'input-streaming' && (\n              <pre>{JSON.stringify(part.input, null, 2)}</pre>\n            )}\n            {part.state === 'output-available' && (\n              <pre>{JSON.stringify(part.output, null, 2)}</pre>\n            )}\n            {part.state === 'output-error' && (\n              <div>Error: {part.errorText}</div>\n            )}\n          </div>\n        );\n    }\n  });\n}\n\nDynamic tools are useful when integrating with:\n\nMCP (Model Context Protocol) tools without schemas\nUser-defined functions loaded at runtime\nExternal tool providers\nTool call streaming\n\nTool call streaming is enabled by default in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    // toolCallStreaming is enabled by default in v5\n    // ...\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream. They are available through the useChat hook. The typed tool parts of assistant messages will also contain partial tool calls. You can use the state property of the tool part to render the correct UI.\n\napp/page.tsx\nexport default function Chat() {\n  // ...\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.parts.map(part => {\n            switch (part.type) {\n              case 'tool-askForConfirmation':\n              case 'tool-getLocation':\n              case 'tool-getWeatherInformation':\n                switch (part.state) {\n                  case 'input-streaming':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'input-available':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'output-available':\n                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\n                  case 'output-error':\n                    return <div>Error: {part.errorText}</div>;\n                }\n            }\n          })}\n        </div>\n      ))}\n    </>\n  );\n}\nStep start parts\n\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages. If you want to display boundaries between tool calls, you can use the step-start parts as follows:\n\napp/page.tsx\n// ...\n// where you render the message parts:\nmessage.parts.map((part, index) => {\n  switch (part.type) {\n    case 'step-start':\n      // show step boundaries as horizontal lines:\n      return index > 0 ? (\n        <div key={index} className=\"text-gray-500\">\n          <hr className=\"my-2 border-gray-300\" />\n        </div>\n      ) : null;\n    case 'text':\n    // ...\n    case 'tool-askForConfirmation':\n    case 'tool-getLocation':\n    case 'tool-getWeatherInformation':\n    // ...\n  }\n});\n// ...\nServer-side Multi-Step Calls\n\nYou can also use multi-step calls on the server-side with streamText. This works when all invoked tools have an execute function on the server side.\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        // tool has execute function:\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n    },\n    stopWhen: stepCountIs(5),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nErrors\n\nLanguage models can make errors when calling tools. By default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\n\nTo surface the errors, you can use the onError function when calling toUIMessageResponse.\n\nexport function errorHandler(error: unknown) {\n  if (error == null) {\n    return 'unknown error';\n  }\n\n\n  if (typeof error === 'string') {\n    return error;\n  }\n\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n\n  return JSON.stringify(error);\n}\nconst result = streamText({\n  // ...\n});\n\n\nreturn result.toUIMessageStreamResponse({\n  onError: errorHandler,\n});\n\nIn case you are using createUIMessageResponse, you can use the onError function when calling toUIMessageResponse:\n\nconst response = createUIMessageResponse({\n  // ...\n  async execute(dataStream) {\n    // ...\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\nPrevious\nChatbot Resume Streams\nNext\nGenerative User Interfaces"
  },
  {
    "title": "AI SDK UI: Generative User Interfaces",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
    "html": "AI SDK UI\nGenerative User Interfaces\nCopy markdown\nGenerative User Interfaces\n\nGenerative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and \"generate UI\". This creates a more engaging and AI-native experience for users.\n\nWhat is the weather in SF?\ngetWeather(\"San Francisco\")\nThursday, March 7\n47°\nSunny\n7am\n48°\n8am\n50°\n9am\n52°\n10am\n54°\n11am\n56°\n12pm\n58°\n1pm\n60°\nThanks!\n\nAt the core of generative UI are tools , which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.\n\nGenerative UI is the process of connecting the results of a tool call to a React component. Here's how it works:\n\nYou provide the model with a prompt or conversation history, along with a set of tools.\nBased on the context, the model may decide to call a tool.\nIf a tool is called, it will execute and return data.\nThis data can then be passed to a React component for rendering.\n\nBy passing the tool results to React components, you can create a generative UI experience that's more engaging and adaptive to your needs.\n\nBuild a Generative UI Chat Interface\n\nLet's create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.\n\nBasic Chat Implementation\n\nStart with a basic chat implementation using the useChat hook:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n          <div>\n            {message.parts.map((part, index) => {\n              if (part.type === 'text') {\n                return <span key={index}>{part.text}</span>;\n              }\n              return null;\n            })}\n          </div>\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n\nTo handle the chat requests and model responses, set up an API route:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';\n\n\nexport async function POST(request: Request) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a friendly assistant!',\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nThis API route uses the streamText function to process chat messages and stream the model's responses back to the client.\n\nCreate a Tool\n\nBefore enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.\n\nCreate a new file called ai/tools.ts with the following content:\n\nai/tools.ts\nimport { tool as createTool } from 'ai';\nimport { z } from 'zod';\n\n\nexport const weatherTool = createTool({\n  description: 'Display the weather for a location',\n  inputSchema: z.object({\n    location: z.string().describe('The location to get the weather for'),\n  }),\n  execute: async function ({ location }) {\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    return { weather: 'Sunny', temperature: 75, location };\n  },\n});\n\n\nexport const tools = {\n  displayWeather: weatherTool,\n};\n\nIn this file, you've created a tool called weatherTool. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.\n\nUpdate the API Route\n\nUpdate the API route to include the tool you've defined:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';\nimport { tools } from '@/ai/tools';\n\n\nexport async function POST(request: Request) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a friendly assistant!',\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools,\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nNow that you've defined the tool and added it to your streamText call, let's build a React component to display the weather information it returns.\n\nCreate UI Components\n\nCreate a new file called components/weather.tsx:\n\ncomponents/weather.tsx\ntype WeatherProps = {\n  temperature: number;\n  weather: string;\n  location: string;\n};\n\n\nexport const Weather = ({ temperature, weather, location }: WeatherProps) => {\n  return (\n    <div>\n      <h2>Current Weather for {location}</h2>\n      <p>Condition: {weather}</p>\n      <p>Temperature: {temperature}°C</p>\n    </div>\n  );\n};\n\nThis component will display the weather information for a given location. It takes three props: temperature, weather, and location (exactly what the weatherTool returns).\n\nRender the Weather Component\n\nNow that you have your tool and corresponding React component, let's integrate them into your chat interface. You'll render the Weather component when the model calls the weather tool.\n\nTo check if the model has called a tool, you can check the parts array of the UIMessage object for tool-specific parts. In AI SDK 5.0, tool parts use typed naming: tool-${toolName} instead of generic types.\n\nUpdate your page.tsx file:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport { Weather } from '@/components/weather';\n\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n          <div>\n            {message.parts.map((part, index) => {\n              if (part.type === 'text') {\n                return <span key={index}>{part.text}</span>;\n              }\n\n\n              if (part.type === 'tool-displayWeather') {\n                switch (part.state) {\n                  case 'input-available':\n                    return <div key={index}>Loading weather...</div>;\n                  case 'output-available':\n                    return (\n                      <div key={index}>\n                        <Weather {...part.output} />\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={index}>Error: {part.errorText}</div>;\n                  default:\n                    return null;\n                }\n              }\n\n\n              return null;\n            })}\n          </div>\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n\nIn this updated code snippet, you:\n\nUse manual input state management with useState instead of the built-in input and handleInputChange.\nUse sendMessage instead of handleSubmit to send messages.\nCheck the parts array of each message for different content types.\nHandle tool parts with type tool-displayWeather and their different states (input-available, output-available, output-error).\n\nThis approach allows you to dynamically render UI components based on the model's responses, creating a more interactive and context-aware chat experience.\n\nExpanding Your Generative UI Application\n\nYou can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here's how you can expand your application:\n\nAdding More Tools\n\nTo add more tools, simply define them in your ai/tools.ts file:\n\n// Add a new stock tool\nexport const stockTool = createTool({\n  description: 'Get price for a stock',\n  inputSchema: z.object({\n    symbol: z.string().describe('The stock symbol to get the price for'),\n  }),\n  execute: async function ({ symbol }) {\n    // Simulated API call\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    return { symbol, price: 100 };\n  },\n});\n\n\n// Update the tools object\nexport const tools = {\n  displayWeather: weatherTool,\n  getStockPrice: stockTool,\n};\n\nNow, create a new file called components/stock.tsx:\n\ntype StockProps = {\n  price: number;\n  symbol: string;\n};\n\n\nexport const Stock = ({ price, symbol }: StockProps) => {\n  return (\n    <div>\n      <h2>Stock Information</h2>\n      <p>Symbol: {symbol}</p>\n      <p>Price: ${price}</p>\n    </div>\n  );\n};\n\nFinally, update your page.tsx file to include the new Stock component:\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport { Weather } from '@/components/weather';\nimport { Stock } from '@/components/stock';\n\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role}</div>\n          <div>\n            {message.parts.map((part, index) => {\n              if (part.type === 'text') {\n                return <span key={index}>{part.text}</span>;\n              }\n\n\n              if (part.type === 'tool-displayWeather') {\n                switch (part.state) {\n                  case 'input-available':\n                    return <div key={index}>Loading weather...</div>;\n                  case 'output-available':\n                    return (\n                      <div key={index}>\n                        <Weather {...part.output} />\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={index}>Error: {part.errorText}</div>;\n                  default:\n                    return null;\n                }\n              }\n\n\n              if (part.type === 'tool-getStockPrice') {\n                switch (part.state) {\n                  case 'input-available':\n                    return <div key={index}>Loading stock price...</div>;\n                  case 'output-available':\n                    return (\n                      <div key={index}>\n                        <Stock {...part.output} />\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={index}>Error: {part.errorText}</div>;\n                  default:\n                    return null;\n                }\n              }\n\n\n              return null;\n            })}\n          </div>\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={e => setInput(e.target.value)}\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n\nBy following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.\n\nPrevious\nChatbot Tool Usage\nNext\nCompletion"
  },
  {
    "title": "AI SDK UI: Completion",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
    "html": "AI SDK UI\nCompletion\nCopy markdown\nCompletion\n\nThe useCompletion hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\n\nThe useCompletion hook is now part of the @ai-sdk/react package.\n\nIn this guide, you will learn how to use the useCompletion hook in your application to generate text completions and stream them in real-time to your users.\n\nExample\napp/page.tsx\n'use client';\n\n\nimport { useCompletion } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\n    api: '/api/completion',\n  });\n\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input\n        name=\"prompt\"\n        value={input}\n        onChange={handleInputChange}\n        id=\"input\"\n      />\n      <button type=\"submit\">Submit</button>\n      <div>{completion}</div>\n    </form>\n  );\n}\napp/api/completion/route.ts\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-3.5-turbo'),\n    prompt,\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIn the Page component, the useCompletion hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.\n\nThis enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\n\nCustomized UI\n\nuseCompletion also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.\n\nLoading and error states\n\nTo show a loading spinner while the chatbot is processing the user's message, you can use the isLoading state returned by the useCompletion hook:\n\nconst { isLoading, ... } = useCompletion()\n\n\nreturn(\n  <>\n    {isLoading ? <Spinner /> : null}\n  </>\n)\n\nSimilarly, the error state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:\n\nconst { error, ... } = useCompletion()\n\n\nuseEffect(() => {\n  if (error) {\n    toast.error(error.message)\n  }\n}, [error])\n\n\n// Or display the error message in the UI:\nreturn (\n  <>\n    {error ? <div>{error.message}</div> : null}\n  </>\n)\nControlled input\n\nIn the initial example, we have handleSubmit and handleInputChange callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.\n\nThe following example demonstrates how to use more granular APIs like setInput with your custom input and submit button components:\n\nconst { input, setInput } = useCompletion();\n\n\nreturn (\n  <>\n    <MyCustomInput value={input} onChange={value => setInput(value)} />\n  </>\n);\nCancelation\n\nIt's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the stop function returned by the useCompletion hook.\n\nconst { stop, isLoading, ... } = useCompletion()\n\n\nreturn (\n  <>\n    <button onClick={stop} disabled={!isLoading}>Stop</button>\n  </>\n)\n\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.\n\nThrottling UI Updates\nThis feature is currently only available for React.\n\nBy default, the useCompletion hook will trigger a render every time a new chunk is received. You can throttle the UI updates with the experimental_throttle option.\n\npage.tsx\nconst { completion, ... } = useCompletion({\n  // Throttle the completion and data updates to 50ms:\n  experimental_throttle: 50\n})\nEvent Callbacks\n\nuseCompletion also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\nconst { ... } = useCompletion({\n  onResponse: (response: Response) => {\n    console.log('Received response from server:', response)\n  },\n  onFinish: (prompt: string, completion: string) => {\n    console.log('Finished streaming completion:', completion)\n  },\n  onError: (error: Error) => {\n    console.error('An error occurred:', error)\n  },\n})\n\nIt's worth noting that you can abort the processing by throwing an error in the onResponse callback. This will trigger the onError callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\n\nConfigure Request Options\n\nBy default, the useCompletion hook sends a HTTP POST request to the /api/completion endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the useCompletion hook:\n\nconst { messages, input, handleInputChange, handleSubmit } = useCompletion({\n  api: '/api/custom-completion',\n  headers: {\n    Authorization: 'your_token',\n  },\n  body: {\n    user_id: '123',\n  },\n  credentials: 'same-origin',\n});\n\nIn this example, the useCompletion hook sends a POST request to the /api/completion endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.\n\nPrevious\nGenerative User Interfaces\nNext\nObject Generation"
  },
  {
    "title": "AI SDK UI: Object Generation",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
    "html": "AI SDK UI\nObject Generation\nCopy markdown\nObject Generation\n\nuseObject is an experimental feature and only available in React, Svelte, and Vue.\n\nThe useObject hook allows you to create interfaces that represent a structured JSON object that is being streamed.\n\nIn this guide, you will learn how to use the useObject hook in your application to generate UIs for structured data on the fly.\n\nExample\n\nThe example shows a small notifications demo app that generates fake notifications in real-time.\n\nSchema\n\nIt is helpful to set up the schema in a separate file that is imported on both the client and server.\n\napp/api/notifications/schema.ts\nimport { z } from 'zod';\n\n\n// define a schema for the notifications\nexport const notificationSchema = z.object({\n  notifications: z.array(\n    z.object({\n      name: z.string().describe('Name of a fictional person.'),\n      message: z.string().describe('Message. Do not use emojis or links.'),\n    }),\n  ),\n});\nClient\n\nThe client uses useObject to stream the object generation process.\n\nThe results are partial and are displayed as they are received. Please note the code for handling undefined values in the JSX.\n\napp/page.tsx\n'use client';\n\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/notifications/schema';\n\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nServer\n\nOn the server, we use streamObject to stream the object generation process.\n\napp/api/notifications/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { notificationSchema } from './schema';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n\n  const result = streamObject({\n    model: openai('gpt-4.1'),\n    schema: notificationSchema,\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n\n  return result.toTextStreamResponse();\n}\nEnum Output Mode\n\nWhen you need to classify or categorize input into predefined options, you can use the enum output mode with useObject. This requires a specific schema structure where the object has enum as a key with z.enum containing your possible values.\n\nExample: Text Classification\n\nThis example shows how to build a simple text classifier that categorizes statements as true or false.\n\nClient\n\nWhen using useObject with enum output mode, your schema must be an object with enum as the key:\n\napp/classify/page.tsx\n'use client';\n\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { z } from 'zod';\n\n\nexport default function ClassifyPage() {\n  const { object, submit, isLoading } = useObject({\n    api: '/api/classify',\n    schema: z.object({ enum: z.enum(['true', 'false']) }),\n  });\n\n\n  return (\n    <>\n      <button onClick={() => submit('The earth is flat')} disabled={isLoading}>\n        Classify statement\n      </button>\n\n\n      {object && <div>Classification: {object.enum}</div>}\n    </>\n  );\n}\nServer\n\nOn the server, use streamObject with output: 'enum' to stream the classification result:\n\napp/api/classify/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n\n  const result = streamObject({\n    model: openai('gpt-4.1'),\n    output: 'enum',\n    enum: ['true', 'false'],\n    prompt: `Classify this statement as true or false: ${context}`,\n  });\n\n\n  return result.toTextStreamResponse();\n}\nCustomized UI\n\nuseObject also provides ways to show loading and error states:\n\nLoading State\n\nThe isLoading state returned by the useObject hook can be used for several purposes:\n\nTo show a loading spinner while the object is generated.\nTo disable the submit button.\napp/page.tsx\n'use client';\n\n\nimport { useObject } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { isLoading, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      {isLoading && <Spinner />}\n\n\n      <button\n        onClick={() => submit('Messages during finals week.')}\n        disabled={isLoading}\n      >\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nStop Handler\n\nThe stop function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.\n\napp/page.tsx\n'use client';\n\n\nimport { useObject } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { isLoading, stop, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      {isLoading && (\n        <button type=\"button\" onClick={() => stop()}>\n          Stop\n        </button>\n      )}\n\n\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nError State\n\nSimilarly, the error state reflects the error object thrown during the fetch request. It can be used to display an error message, or to disable the submit button:\n\nWe recommend showing a generic error message to the user, such as \"Something went wrong.\" This is a good practice to avoid leaking information from the server.\n\n'use client';\n\n\nimport { useObject } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { error, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      {error && <div>An error occurred.</div>}\n\n\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nEvent Callbacks\n\nuseObject provides optional event callbacks that you can use to handle life-cycle events.\n\nonFinish: Called when the object generation is completed.\nonError: Called when an error occurs during the fetch request.\n\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\napp/page.tsx\n'use client';\n\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/notifications/schema';\n\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n    onFinish({ object, error }) {\n      // typed object, undefined if schema validation fails:\n      console.log('Object generation completed:', object);\n\n\n      // error, undefined if schema validation succeeds:\n      console.log('Schema validation error:', error);\n    },\n    onError(error) {\n      // error during fetch request:\n      console.error('An error occurred:', error);\n    },\n  });\n\n\n  return (\n    <div>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\nConfigure Request Options\n\nYou can configure the API endpoint, optional headers and credentials using the api, headers and credentials settings.\n\nconst { submit, object } = useObject({\n  api: '/api/use-object',\n  headers: {\n    'X-Custom-Header': 'CustomValue',\n  },\n  credentials: 'include',\n  schema: yourSchema,\n});\nPrevious\nCompletion\nNext\nStreaming Custom Data"
  },
  {
    "title": "AI SDK UI: Streaming Custom Data",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
    "html": "AI SDK UI\nStreaming Custom Data\nCopy markdown\nStreaming Custom Data\n\nIt is often useful to send additional data alongside the model's response. For example, you may want to send status information, the message ids after storing them, or references to content that the language model is referring to.\n\nThe AI SDK provides several helpers that allows you to stream additional data to the client and attach it to the UIMessage parts array:\n\ncreateUIMessageStream: creates a data stream\ncreateUIMessageStreamResponse: creates a response object that streams data\npipeUIMessageStreamToResponse: pipes a data stream to a server response object\n\nThe data is streamed as part of the response stream using Server-Sent Events.\n\nSetting Up Type-Safe Data Streaming\n\nFirst, define your custom message type with data part schemas for type safety:\n\nai/types.ts\nimport { UIMessage } from 'ai';\n\n\n// Define your custom message type with data part schemas\nexport type MyUIMessage = UIMessage<\n  never, // metadata type\n  {\n    weather: {\n      city: string;\n      weather?: string;\n      status: 'loading' | 'success';\n    };\n    notification: {\n      message: string;\n      level: 'info' | 'warning' | 'error';\n    };\n  } // data parts type\n>;\nStreaming Data from the Server\n\nIn your server-side route handler, you can create a UIMessageStream and then pass it to createUIMessageStreamResponse:\n\nroute.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  createUIMessageStream,\n  createUIMessageStreamResponse,\n  streamText,\n  convertToModelMessages,\n} from 'ai';\nimport type { MyUIMessage } from '@/ai/types';\n\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n\n  const stream = createUIMessageStream<MyUIMessage>({\n    execute: ({ writer }) => {\n      // 1. Send initial status (transient - won't be added to message history)\n      writer.write({\n        type: 'data-notification',\n        data: { message: 'Processing your request...', level: 'info' },\n        transient: true, // This part won't be added to message history\n      });\n\n\n      // 2. Send sources (useful for RAG use cases)\n      writer.write({\n        type: 'source',\n        value: {\n          type: 'source',\n          sourceType: 'url',\n          id: 'source-1',\n          url: 'https://weather.com',\n          title: 'Weather Data Source',\n        },\n      });\n\n\n      // 3. Send data parts with loading state\n      writer.write({\n        type: 'data-weather',\n        id: 'weather-1',\n        data: { city: 'San Francisco', status: 'loading' },\n      });\n\n\n      const result = streamText({\n        model: openai('gpt-4.1'),\n        messages: convertToModelMessages(messages),\n        onFinish() {\n          // 4. Update the same data part (reconciliation)\n          writer.write({\n            type: 'data-weather',\n            id: 'weather-1', // Same ID = update existing part\n            data: {\n              city: 'San Francisco',\n              weather: 'sunny',\n              status: 'success',\n            },\n          });\n\n\n          // 5. Send completion notification (transient)\n          writer.write({\n            type: 'data-notification',\n            data: { message: 'Request completed', level: 'info' },\n            transient: true, // Won't be added to message history\n          });\n        },\n      });\n\n\n      writer.merge(result.toUIMessageStream());\n    },\n  });\n\n\n  return createUIMessageStreamResponse({ stream });\n}\n\nYou can also send stream data from custom backends, e.g. Python / FastAPI, using the UI Message Stream Protocol.\n\nTypes of Streamable Data\nData Parts (Persistent)\n\nRegular data parts are added to the message history and appear in message.parts:\n\nwriter.write({\n  type: 'data-weather',\n  id: 'weather-1', // Optional: enables reconciliation\n  data: { city: 'San Francisco', status: 'loading' },\n});\nSources\n\nSources are useful for RAG implementations where you want to show which documents or URLs were referenced:\n\nwriter.write({\n  type: 'source',\n  value: {\n    type: 'source',\n    sourceType: 'url',\n    id: 'source-1',\n    url: 'https://example.com',\n    title: 'Example Source',\n  },\n});\nTransient Data Parts (Ephemeral)\n\nTransient parts are sent to the client but not added to the message history. They are only accessible via the onData useChat handler:\n\n// server\nwriter.write({\n  type: 'data-notification',\n  data: { message: 'Processing...', level: 'info' },\n  transient: true, // Won't be added to message history\n});\n\n\n// client\nconst [notification, setNotification] = useState();\n\n\nconst { messages } = useChat({\n  onData: ({ data, type }) => {\n    if (type === 'data-notification') {\n      setNotification({ message: data.message, level: data.level });\n    }\n  },\n});\nData Part Reconciliation\n\nWhen you write to a data part with the same ID, the client automatically reconciles and updates that part. This enables powerful dynamic experiences like:\n\nCollaborative artifacts - Update code, documents, or designs in real-time\nProgressive data loading - Show loading states that transform into final results\nLive status updates - Update progress bars, counters, or status indicators\nInteractive components - Build UI elements that evolve based on user interaction\n\nThe reconciliation happens automatically - simply use the same id when writing to the stream.\n\nProcessing Data on the Client\nUsing the onData Callback\n\nThe onData callback is essential for handling streaming data, especially transient parts:\n\npage.tsx\nimport { useChat } from '@ai-sdk/react';\nimport type { MyUIMessage } from '@/ai/types';\n\n\nconst { messages } = useChat<MyUIMessage>({\n  api: '/api/chat',\n  onData: dataPart => {\n    // Handle all data parts as they arrive (including transient parts)\n    console.log('Received data part:', dataPart);\n\n\n    // Handle different data part types\n    if (dataPart.type === 'data-weather') {\n      console.log('Weather update:', dataPart.data);\n    }\n\n\n    // Handle transient notifications (ONLY available here, not in message.parts)\n    if (dataPart.type === 'data-notification') {\n      showToast(dataPart.data.message, dataPart.data.level);\n    }\n  },\n});\n\nImportant: Transient data parts are only available through the onData callback. They will not appear in the message.parts array since they're not added to message history.\n\nRendering Persistent Data Parts\n\nYou can filter and render data parts from the message parts array:\n\npage.tsx\nconst result = (\n  <>\n    {messages?.map(message => (\n      <div key={message.id}>\n        {/* Render weather data parts */}\n        {message.parts\n          .filter(part => part.type === 'data-weather')\n          .map((part, index) => (\n            <div key={index} className=\"weather-widget\">\n              {part.data.status === 'loading' ? (\n                <>Getting weather for {part.data.city}...</>\n              ) : (\n                <>\n                  Weather in {part.data.city}: {part.data.weather}\n                </>\n              )}\n            </div>\n          ))}\n\n\n        {/* Render text content */}\n        {message.parts\n          .filter(part => part.type === 'text')\n          .map((part, index) => (\n            <div key={index}>{part.text}</div>\n          ))}\n\n\n        {/* Render sources */}\n        {message.parts\n          .filter(part => part.type === 'source')\n          .map((part, index) => (\n            <div key={index} className=\"source\">\n              Source: <a href={part.url}>{part.title}</a>\n            </div>\n          ))}\n      </div>\n    ))}\n  </>\n);\nComplete Example\npage.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport type { MyUIMessage } from '@/ai/types';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n\n\n  const { messages, sendMessage } = useChat<MyUIMessage>({\n    api: '/api/chat',\n    onData: dataPart => {\n      // Handle transient notifications\n      if (dataPart.type === 'data-notification') {\n        console.log('Notification:', dataPart.data.message);\n      }\n    },\n  });\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n\n          {/* Render weather data */}\n          {message.parts\n            .filter(part => part.type === 'data-weather')\n            .map((part, index) => (\n              <span key={index} className=\"weather-update\">\n                {part.data.status === 'loading' ? (\n                  <>Getting weather for {part.data.city}...</>\n                ) : (\n                  <>\n                    Weather in {part.data.city}: {part.data.weather}\n                  </>\n                )}\n              </span>\n            ))}\n\n\n          {/* Render text content */}\n          {message.parts\n            .filter(part => part.type === 'text')\n            .map((part, index) => (\n              <div key={index}>{part.text}</div>\n            ))}\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Ask about the weather...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </>\n  );\n}\nUse Cases\nRAG Applications - Stream sources and retrieved documents\nReal-time Status - Show loading states and progress updates\nCollaborative Tools - Stream live updates to shared artifacts\nAnalytics - Send usage data without cluttering message history\nNotifications - Display temporary alerts and status messages\nMessage Metadata vs Data Parts\n\nBoth message metadata and data parts allow you to send additional information alongside messages, but they serve different purposes:\n\nMessage Metadata\n\nMessage metadata is best for message-level information that describes the message as a whole:\n\nAttached at the message level via message.metadata\nSent using the messageMetadata callback in toUIMessageStreamResponse\nIdeal for: timestamps, model info, token usage, user context\nType-safe with custom metadata types\n// Server: Send metadata about the message\nreturn result.toUIMessageStreamResponse({\n  messageMetadata: ({ part }) => {\n    if (part.type === 'finish') {\n      return {\n        model: part.response.modelId,\n        totalTokens: part.totalUsage.totalTokens,\n        createdAt: Date.now(),\n      };\n    }\n  },\n});\nData Parts\n\nData parts are best for streaming dynamic arbitrary data:\n\nAdded to the message parts array via message.parts\nStreamed using createUIMessageStream and writer.write()\nCan be reconciled/updated using the same ID\nSupport transient parts that don't persist\nIdeal for: dynamic content, loading states, interactive components\n// Server: Stream data as part of message content\nwriter.write({\n  type: 'data-weather',\n  id: 'weather-1',\n  data: { city: 'San Francisco', status: 'loading' },\n});\n\nFor more details on message metadata, see the Message Metadata documentation.\n\nPrevious\nObject Generation\nNext\nError Handling"
  },
  {
    "title": "AI SDK UI: Error Handling",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
    "html": "AI SDK UI\nError Handling\nCopy markdown\nError Handling and warnings\nWarnings\n\nThe AI SDK shows warnings when something might not work as expected. These warnings help you fix problems before they cause errors.\n\nWhen Warnings Appear\n\nWarnings are shown in the browser console when:\n\nUnsupported settings: You use a setting that the AI model doesn't support\nUnsupported tools: You use a tool that the AI model can't use\nOther issues: The AI model reports other problems\nWarning Messages\n\nAll warnings start with \"AI SDK Warning:\" so you can easily find them. For example:\n\nAI SDK Warning: The \"temperature\" setting is not supported by this model\nAI SDK Warning: The tool \"calculator\" is not supported by this model\nTurning Off Warnings\n\nBy default, warnings are shown in the console. You can control this behavior:\n\nTurn Off All Warnings\n\nSet a global variable to turn off warnings completely:\n\nglobalThis.AI_SDK_LOG_WARNINGS = false;\nCustom Warning Handler\n\nYou can also provide your own function to handle warnings:\n\nglobalThis.AI_SDK_LOG_WARNINGS = warnings => {\n  // Handle warnings your own way\n  warnings.forEach(warning => {\n    // Your custom logic here\n    console.log('Custom warning:', warning);\n  });\n};\n\nCustom warning functions are experimental and can change in patch releases without notice.\n\nError Handling\nError Helper Object\n\nEach AI SDK UI hook also returns an error object that you can use to render the error in your UI. You can use the error object to show an error message, disable the submit button, or show a retry button.\n\nWe recommend showing a generic error message to the user, such as \"Something went wrong.\" This is a good practice to avoid leaking information from the server.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage, error, regenerate } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts\n            .filter(part => part.type === 'text')\n            .map(part => part.text)\n            .join('')}\n        </div>\n      ))}\n\n\n      {error && (\n        <>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => regenerate()}>\n            Retry\n          </button>\n        </>\n      )}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={error != null}\n        />\n      </form>\n    </div>\n  );\n}\nAlternative: replace last message\n\nAlternatively you can write a custom submit handler that replaces the last message when an error is present.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { sendMessage, error, messages, setMessages } = useChat();\n\n\n  function customSubmit(event: React.FormEvent<HTMLFormElement>) {\n    event.preventDefault();\n\n\n    if (error != null) {\n      setMessages(messages.slice(0, -1)); // remove last message\n    }\n\n\n    sendMessage({ text: input });\n    setInput('');\n  }\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts\n            .filter(part => part.type === 'text')\n            .map(part => part.text)\n            .join('')}\n        </div>\n      ))}\n\n\n      {error && <div>An error occurred.</div>}\n\n\n      <form onSubmit={customSubmit}>\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </div>\n  );\n}\nError Handling Callback\n\nErrors can be processed by passing an onError callback function as an option to the useChat or useCompletion hooks. The callback function receives an error object as an argument.\n\nimport { useChat } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const {\n    /* ... */\n  } = useChat({\n    // handle error:\n    onError: error => {\n      console.error(error);\n    },\n  });\n}\nInjecting Errors for Testing\n\nYou might want to create errors for testing. You can easily do so by throwing an error in your route handler:\n\nexport async function POST(req: Request) {\n  throw new Error('This is a test error');\n}\nPrevious\nStreaming Custom Data\nNext\nTransport"
  },
  {
    "title": "AI SDK UI: Transport",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/transport",
    "html": "AI SDK UI\nTransport\nCopy markdown\nTransport\n\nThe useChat transport system provides fine-grained control over how messages are sent to your API endpoints and how responses are processed. This is particularly useful for alternative communication protocols like WebSockets, custom authentication patterns, or specialized backend integrations.\n\nDefault Transport\n\nBy default, useChat uses HTTP POST requests to send messages to /api/chat:\n\nimport { useChat } from '@ai-sdk/react';\n\n\n// Uses default HTTP transport\nconst { messages, sendMessage } = useChat();\n\nThis is equivalent to:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n  }),\n});\nCustom Transport Configuration\n\nConfigure the default transport with custom options:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/custom-chat',\n    headers: {\n      Authorization: 'Bearer your-token',\n      'X-API-Version': '2024-01',\n    },\n    credentials: 'include',\n  }),\n});\nDynamic Configuration\n\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n    headers: () => ({\n      Authorization: `Bearer ${getAuthToken()}`,\n      'X-User-ID': getCurrentUserId(),\n    }),\n    body: () => ({\n      sessionId: getCurrentSessionId(),\n      preferences: getUserPreferences(),\n    }),\n    credentials: () => 'include',\n  }),\n});\nRequest Transformation\n\nTransform requests before sending to your API:\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n    prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\n      return {\n        headers: {\n          'X-Session-ID': id,\n        },\n        body: {\n          messages: messages.slice(-10), // Only send last 10 messages\n          trigger,\n          messageId,\n        },\n      };\n    },\n  }),\n});\nBuilding Custom Transports\n\nTo understand how to build your own transport, refer to the source code of the default implementation:\n\nDefaultChatTransport\n - The complete default HTTP transport implementation\nHttpChatTransport\n - Base HTTP transport with request handling\nChatTransport Interface\n - The transport interface you need to implement\n\nThese implementations show you exactly how to:\n\nHandle the sendMessages method\nProcess UI message streams\nTransform requests and responses\nHandle errors and connection management\n\nThe transport system gives you complete control over how your chat application communicates, enabling integration with any backend protocol or service.\n\nPrevious\nError Handling\nNext\nReading UIMessage Streams"
  },
  {
    "title": "AI SDK UI: Reading UIMessage Streams",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams",
    "html": "AI SDK UI\nReading UIMessage Streams\nCopy markdown\nReading UI Message Streams\n\nUIMessage streams are useful outside of traditional chat use cases. You can consume them for terminal UIs, custom stream processing on the client, or React Server Components (RSC).\n\nThe readUIMessageStream helper transforms a stream of UIMessageChunk objects into an AsyncIterableStream of UIMessage objects, allowing you to process messages as they're being constructed.\n\nBasic Usage\nimport { openai } from '@ai-sdk/openai';\nimport { readUIMessageStream, streamText } from 'ai';\n\n\nasync function main() {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Write a short story about a robot.',\n  });\n\n\n  for await (const uiMessage of readUIMessageStream({\n    stream: result.toUIMessageStream(),\n  })) {\n    console.log('Current message state:', uiMessage);\n  }\n}\nTool Calls Integration\n\nHandle streaming responses that include tool calls:\n\nimport { openai } from '@ai-sdk/openai';\nimport { readUIMessageStream, streamText, tool } from 'ai';\nimport { z } from 'zod';\n\n\nasync function handleToolCalls() {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: ({ location }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      }),\n    },\n    prompt: 'What is the weather in Tokyo?',\n  });\n\n\n  for await (const uiMessage of readUIMessageStream({\n    stream: result.toUIMessageStream(),\n  })) {\n    // Handle different part types\n    uiMessage.parts.forEach(part => {\n      switch (part.type) {\n        case 'text':\n          console.log('Text:', part.text);\n          break;\n        case 'tool-call':\n          console.log('Tool called:', part.toolName, 'with args:', part.args);\n          break;\n        case 'tool-result':\n          console.log('Tool result:', part.result);\n          break;\n      }\n    });\n  }\n}\nResuming Conversations\n\nResume streaming from a previous message state:\n\nimport { readUIMessageStream, streamText } from 'ai';\n\n\nasync function resumeConversation(lastMessage: UIMessage) {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: [\n      { role: 'user', content: 'Continue our previous conversation.' },\n    ],\n  });\n\n\n  // Resume from the last message\n  for await (const uiMessage of readUIMessageStream({\n    stream: result.toUIMessageStream(),\n    message: lastMessage, // Resume from this message\n  })) {\n    console.log('Resumed message:', uiMessage);\n  }\n}\nPrevious\nTransport\nNext\nMessage Metadata"
  },
  {
    "title": "AI SDK UI: Message Metadata",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata",
    "html": "AI SDK UI\nMessage Metadata\nCopy markdown\nMessage Metadata\n\nMessage metadata allows you to attach custom information to messages at the message level. This is useful for tracking timestamps, model information, token usage, user context, and other message-level data.\n\nOverview\n\nMessage metadata differs from data parts in that it's attached at the message level rather than being part of the message content. While data parts are ideal for dynamic content that forms part of the message, metadata is perfect for information about the message itself.\n\nGetting Started\n\nHere's a simple example of using message metadata to track timestamps and model information:\n\nDefining Metadata Types\n\nFirst, define your metadata type for type safety:\n\napp/types.ts\nimport { UIMessage } from 'ai';\nimport { z } from 'zod';\n\n\n// Define your metadata schema\nexport const messageMetadataSchema = z.object({\n  createdAt: z.number().optional(),\n  model: z.string().optional(),\n  totalTokens: z.number().optional(),\n});\n\n\nexport type MessageMetadata = z.infer<typeof messageMetadataSchema>;\n\n\n// Create a typed UIMessage\nexport type MyUIMessage = UIMessage<MessageMetadata>;\nSending Metadata from the Server\n\nUse the messageMetadata callback in toUIMessageStreamResponse to send metadata at different streaming stages:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText } from 'ai';\nimport type { MyUIMessage } from '@/types';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages, // pass this in for type-safe return objects\n    messageMetadata: ({ part }) => {\n      // Send metadata when streaming starts\n      if (part.type === 'start') {\n        return {\n          createdAt: Date.now(),\n          model: 'gpt-4o',\n        };\n      }\n\n\n      // Send additional metadata when streaming completes\n      if (part.type === 'finish') {\n        return {\n          totalTokens: part.totalUsage.totalTokens,\n        };\n      }\n    },\n  });\n}\n\nTo enable type-safe metadata return object in messageMetadata, pass in the originalMessages parameter typed to your UIMessage type.\n\nAccessing Metadata on the Client\n\nAccess metadata through the message.metadata property:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport type { MyUIMessage } from '@/types';\n\n\nexport default function Chat() {\n  const { messages } = useChat<MyUIMessage>({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>\n            {message.role === 'user' ? 'User: ' : 'AI: '}\n            {message.metadata?.createdAt && (\n              <span className=\"text-sm text-gray-500\">\n                {new Date(message.metadata.createdAt).toLocaleTimeString()}\n              </span>\n            )}\n          </div>\n\n\n          {/* Render message content */}\n          {message.parts.map((part, index) =>\n            part.type === 'text' ? <div key={index}>{part.text}</div> : null,\n          )}\n\n\n          {/* Display additional metadata */}\n          {message.metadata?.totalTokens && (\n            <div className=\"text-xs text-gray-400\">\n              {message.metadata.totalTokens} tokens\n            </div>\n          )}\n        </div>\n      ))}\n    </div>\n  );\n}\n\nFor streaming arbitrary data that changes during generation, consider using data parts instead.\n\nCommon Use Cases\n\nMessage metadata is ideal for:\n\nTimestamps: When messages were created or completed\nModel Information: Which AI model was used\nToken Usage: Track costs and usage limits\nUser Context: User IDs, session information\nPerformance Metrics: Generation time, time to first token\nQuality Indicators: Finish reason, confidence scores\nSee Also\nChatbot Guide - Message metadata in the context of building chatbots\nStreaming Data - Comparison with data parts\nUIMessage Reference - Complete UIMessage type reference\nPrevious\nReading UIMessage Streams\nNext\nStream Protocols"
  },
  {
    "title": "AI SDK UI: Stream Protocols",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
    "html": "AI SDK UI\nStream Protocols\nCopy markdown\nStream Protocols\n\nAI SDK UI functions such as useChat and useCompletion support both text streams and data streams. The stream protocol defines how the data is streamed to the frontend on top of the HTTP protocol.\n\nThis page describes both protocols and how to use them in the backend and frontend.\n\nYou can use this information to develop custom backends and frontends for your use case, e.g., to provide compatible API endpoints that are implemented in a different language such as Python.\n\nFor instance, here's an example using FastAPI\n as a backend.\n\nText Stream Protocol\n\nA text stream contains chunks in plain text, that are streamed to the frontend. Each chunk is then appended together to form a full text response.\n\nText streams are supported by useChat, useCompletion, and useObject. When you use useChat or useCompletion, you need to enable text streaming by setting the streamProtocol options to text.\n\nYou can generate text streams with streamText in the backend. When you call toTextStreamResponse() on the result object, a streaming HTTP response is returned.\n\nText streams only support basic text data. If you need to stream other types of data such as tool calls, use data streams.\n\nText Stream Example\n\nHere is a Next.js example that uses the text stream protocol:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { TextStreamChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat({\n    transport: new TextStreamChatTransport({ api: '/api/chat' }),\n  });\n\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\napp/api/chat/route.ts\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toTextStreamResponse();\n}\nData Stream Protocol\n\nA data stream follows a special protocol that the AI SDK provides to send information to the frontend.\n\nThe data stream protocol uses Server-Sent Events (SSE) format for improved standardization, keep-alive through ping, reconnect capabilities, and better cache handling.\n\nWhen you provide data streams from a custom backend, you need to set the x-vercel-ai-ui-message-stream header to v1.\n\nThe following stream parts are currently supported:\n\nMessage Start Part\n\nIndicates the beginning of a new message with metadata.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"start\",\"messageId\":\"...\"}\nText Parts\n\nText content is streamed using a start/delta/end pattern with unique IDs for each text block.\n\nText Start Part\n\nIndicates the beginning of a text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"text-start\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\nText Delta Part\n\nContains incremental text content for the text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"text-delta\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\",\"delta\":\"Hello\"}\nText End Part\n\nIndicates the completion of a text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"text-end\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\nReasoning Parts\n\nReasoning content is streamed using a start/delta/end pattern with unique IDs for each reasoning block.\n\nReasoning Start Part\n\nIndicates the beginning of a reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"reasoning-start\",\"id\":\"reasoning_123\"}\nReasoning Delta Part\n\nContains incremental reasoning content for the reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"reasoning-delta\",\"id\":\"reasoning_123\",\"delta\":\"This is some reasoning\"}\nReasoning End Part\n\nIndicates the completion of a reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"reasoning-end\",\"id\":\"reasoning_123\"}\nSource Parts\n\nSource parts provide references to external content sources.\n\nSource URL Part\n\nReferences to external URLs.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"source-url\",\"sourceId\":\"https://example.com\",\"url\":\"https://example.com\"}\nSource Document Part\n\nReferences to documents or files.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"source-document\",\"sourceId\":\"https://example.com\",\"mediaType\":\"file\",\"title\":\"Title\"}\nFile Part\n\nThe file parts contain references to files with their media type.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"file\",\"url\":\"https://example.com/file.png\",\"mediaType\":\"image/png\"}\nData Parts\n\nCustom data parts allow streaming of arbitrary structured data with type-specific handling.\n\nFormat: Server-Sent Event with JSON object where the type includes a custom suffix\n\nExample:\n\ndata: {\"type\":\"data-weather\",\"data\":{\"location\":\"SF\",\"temperature\":100}}\n\nThe data-* type pattern allows you to define custom data types that your frontend can handle specifically.\n\nError Part\n\nThe error parts are appended to the message as they are received.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"error\",\"errorText\":\"error message\"}\nTool Input Start Part\n\nIndicates the beginning of tool input streaming.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-input-start\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\"}\nTool Input Delta Part\n\nIncremental chunks of tool input as it's being generated.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-input-delta\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"inputTextDelta\":\"San Francisco\"}\nTool Input Available Part\n\nIndicates that tool input is complete and ready for execution.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-input-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\",\"input\":{\"city\":\"San Francisco\"}}\nTool Output Available Part\n\nContains the result of tool execution.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-output-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"output\":{\"city\":\"San Francisco\",\"weather\":\"sunny\"}}\nStart Step Part\n\nA part indicating the start of a step.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"start-step\"}\nFinish Step Part\n\nA part indicating that a step (i.e., one LLM API call in the backend) has been completed.\n\nThis part is necessary to correctly process multiple stitched assistant calls, e.g. when calling tools in the backend, and using steps in useChat at the same time.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"finish-step\"}\nFinish Message Part\n\nA part indicating the completion of a message.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"finish\"}\nStream Termination\n\nThe stream ends with a special [DONE] marker.\n\nFormat: Server-Sent Event with literal [DONE]\n\nExample:\n\ndata: [DONE]\n\nThe data stream protocol is supported by useChat and useCompletion on the frontend and used by default. useCompletion only supports the text and data stream parts.\n\nOn the backend, you can use toUIMessageStreamResponse() from the streamText result object to return a streaming HTTP response.\n\nUI Message Stream Example\n\nHere is a Next.js example that uses the UI message stream protocol:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nPrevious\nMessage Metadata\nNext\nAI SDK RSC"
  },
  {
    "title": "Agents: Agents",
    "url": "https://ai-sdk.dev/docs/agents/overview",
    "html": "Agents\nOverview\nCopy markdown\nAgents\n\nAgents are large language models (LLMs) that use tools in a loop to accomplish tasks.\n\nThese components work together:\n\nLLMs process input and decide the next action\nTools extend capabilities beyond text generation (reading files, calling APIs, writing to databases)\nLoop orchestrates execution through:\nContext management - Maintaining conversation history and deciding what the model sees (input) at each step\nStopping conditions - Determining when the loop (task) is complete\nAgent Class\n\nThe Agent class handles these three components. Here's an agent that uses multiple tools in a loop to accomplish a task:\n\nimport { Experimental_Agent as Agent, stepCountIs, tool } from 'ai';\nimport { z } from 'zod';\n\n\nconst weatherAgent = new Agent({\n  model: 'openai/gpt-4o',\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location (in Fahrenheit)',\n      inputSchema: z.object({\n        location: z.string().describe('The location to get the weather for'),\n      }),\n      execute: async ({ location }) => ({\n        location,\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n      }),\n    }),\n    convertFahrenheitToCelsius: tool({\n      description: 'Convert temperature from Fahrenheit to Celsius',\n      inputSchema: z.object({\n        temperature: z.number().describe('Temperature in Fahrenheit'),\n      }),\n      execute: async ({ temperature }) => {\n        const celsius = Math.round((temperature - 32) * (5 / 9));\n        return { celsius };\n      },\n    }),\n  },\n  stopWhen: stepCountIs(20),\n});\n\n\nconst result = await weatherAgent.generate({\n  prompt: 'What is the weather in San Francisco in celsius?',\n});\n\n\nconsole.log(result.text); // agent's final answer\nconsole.log(result.steps); // steps taken by the agent\n\nThe agent automatically:\n\nCalls the weather tool to get the temperature in Fahrenheit\nCalls convertFahrenheitToCelsius to convert it\nGenerates a final text response with the result\n\nThe Agent class handles the loop, context management, and stopping conditions.\n\nWhy Use the Agent Class?\n\nThe Agent class is the recommended approach for building agents with the AI SDK because it:\n\nReduces boilerplate - Manages loops and message arrays\nImproves reusability - Define once, use throughout your application\nSimplifies maintenance - Single place to update agent configuration\n\nFor most use cases, start with the Agent class. Use core functions (generateText, streamText) when you need explicit control over each step for complex structured workflows.\n\nStructured Workflows\n\nAgents are flexible and powerful, but non-deterministic. When you need reliable, repeatable outcomes with explicit control flow, use core functions with structured workflow patterns combining:\n\nConditional statements for explicit branching\nStandard functions for reusable logic\nError handling for robustness\nExplicit control flow for predictability\n\nExplore workflow patterns to learn more about building structured, reliable systems.\n\nNext Steps\nBuilding Agents - Guide to creating agents with the Agent class\nWorkflow Patterns - Structured patterns using core functions for complex workflows\nLoop Control - Execution control with stopWhen and prepareStep\nPrevious\nAgents\nNext\nBuilding Agents"
  },
  {
    "title": "Advanced: Prompt Engineering",
    "url": "https://ai-sdk.dev/docs/advanced/prompt-engineering",
    "html": "Advanced\nPrompt Engineering\nCopy markdown\nPrompt Engineering\nWhat is a Large Language Model (LLM)?\n\nA Large Language Model is essentially a prediction engine that takes a sequence of words as input and aims to predict the most likely sequence to follow. It does this by assigning probabilities to potential next sequences and then selecting one. The model continues to generate sequences until it meets a specified stopping criterion.\n\nThese models learn by training on massive text corpuses, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well. However, it's crucial to understand that the generated sequences, while often seeming plausible, can sometimes be random and not grounded in reality. As these models become more accurate, many surprising abilities and applications emerge.\n\nWhat is a prompt?\n\nPrompts are the starting points for LLMs. They are the inputs that trigger the model to generate text. The scope of prompt engineering involves not just crafting these prompts but also understanding related concepts such as hidden prompts, tokens, token limits, and the potential for prompt hacking, which includes phenomena like jailbreaks and leaks.\n\nWhy is prompt engineering needed?\n\nPrompt engineering currently plays a pivotal role in shaping the responses of LLMs. It allows us to tweak the model to respond more effectively to a broader range of queries. This includes the use of techniques like semantic search, command grammars, and the ReActive model architecture. The performance, context window, and cost of LLMs varies between models and model providers which adds further constraints to the mix. For example, the GPT-4 model is more expensive than GPT-3.5-turbo and significantly slower, but it can also be more effective at certain tasks. And so, like many things in software engineering, there is a trade-offs between cost and performance.\n\nTo assist with comparing and tweaking LLMs, we've built an AI playground that allows you to compare the performance of different models side-by-side online. When you're ready, you can even generate code with the AI SDK to quickly use your prompt and your selected model into your own applications.\n\nExample: Build a Slogan Generator\nStart with an instruction\n\nImagine you want to build a slogan generator for marketing campaigns. Creating catchy slogans isn't always straightforward!\n\nFirst, you'll need a prompt that makes it clear what you want. Let's start with an instruction. Submit this prompt to generate your first completion.\n\nGenerate\n...\n\nNot bad! Now, try making your instruction more specific.\n\nGenerate\n...\n\nIntroducing a single descriptive term to our prompt influences the completion. Essentially, crafting your prompt is the means by which you \"instruct\" or \"program\" the model.\n\nInclude examples\n\nClear instructions are key for quality outcomes, but that might not always be enough. Let's try to enhance your instruction further.\n\nGenerate\n...\n\nThese slogans are fine, but could be even better. It appears the model overlooked the 'live' part in our prompt. Let's change it slightly to generate more appropriate suggestions.\n\nOften, it's beneficial to both demonstrate and tell the model your requirements. Incorporating examples in your prompt can aid in conveying patterns or subtleties. Test this prompt that carries a few examples.\n\nGenerate\n...\n\nGreat! Incorporating examples of expected output for a certain input prompted the model to generate the kind of names we aimed for.\n\nTweak your settings\n\nApart from designing prompts, you can influence completions by tweaking model settings. A crucial setting is the temperature.\n\nYou might have seen that the same prompt, when repeated, yielded the same or nearly the same completions. This happens when your temperature is at 0.\n\nAttempt to re-submit the identical prompt a few times with temperature set to 1.\n\nTemperature\nGenerate\n...\n\nNotice the difference? With a temperature above 0, the same prompt delivers varied completions each time.\n\nKeep in mind that the model forecasts the text most likely to follow the preceding text. Temperature, a value from 0 to 1, essentially governs the model's confidence level in making these predictions. A lower temperature implies lesser risks, leading to more precise and deterministic completions. A higher temperature yields a broader range of completions.\n\nFor your slogan generator, you might want a large pool of name suggestions. A moderate temperature of 0.6 should serve well.\n\nRecommended Resources\n\nPrompt Engineering is evolving rapidly, with new methods and research papers surfacing every week. Here are some resources that we've found useful for learning about and experimenting with prompt engineering:\n\nThe Vercel AI Playground\nBrex Prompt Engineering\nPrompt Engineering Guide by Dair AI\nPrevious\nAdvanced\nNext\nStopping Streams"
  },
  {
    "title": "Foundations: Streaming",
    "url": "https://ai-sdk.dev/docs/foundations/streaming",
    "html": "Foundations\nStreaming\nCopy markdown\nStreaming\n\nStreaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.\n\nLarge language models (LLMs) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you're likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by displaying parts of the response as they become available.\n\nBlocking UI\n\nBlocking responses wait until the full response is available before displaying it.\n\nStreaming UI\n\nStreaming responses can transmit parts of the response as they become available.\n\nReal-world Examples\n\nHere are 2 examples that illustrate how streaming UIs can improve user experiences in a real-world setting – the first uses a blocking UI, while the second uses a streaming UI.\n\nBlocking UI\nGenerate\n...\nStreaming UI\nGenerate\n...\n\nAs you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.\n\nWhile streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren't always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.\n\nHowever, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible. In the example below, we stream text generation from OpenAI's gpt-4.1 in under 10 lines of code using the SDK's streamText function:\n\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n\nconst { textStream } = streamText({\n  model: openai('gpt-4.1'),\n  prompt: 'Write a poem about embedding models.',\n});\n\n\nfor await (const textPart of textStream) {\n  console.log(textPart);\n}\n\nFor an introduction to streaming UIs and the AI SDK, check out our Getting Started guides.\n\nPrevious\nTools\nNext\nGetting Started"
  },
  {
    "title": "Foundations: Prompts",
    "url": "https://ai-sdk.dev/docs/foundations/prompts",
    "html": "Foundations\nPrompts\nCopy markdown\nPrompts\n\nPrompts are instructions that you give a large language model (LLM) to tell it what to do. It's like when you ask someone for directions; the clearer your question, the better the directions you'll get.\n\nMany LLM providers offer complex interfaces for specifying prompts. They involve different roles and message types. While these interfaces are powerful, they can be hard to use and understand.\n\nIn order to simplify prompting, the AI SDK supports text, message, and system prompts.\n\nText Prompts\n\nText prompts are strings. They are ideal for simple generation use cases, e.g. repeatedly generating content for variants of the same prompt text.\n\nYou can set text prompts using the prompt property made available by AI SDK functions like streamText or generateObject. You can structure the text in any way and inject variables, e.g. using a template literal.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n\nYou can also use template literals to provide dynamic data to your prompt.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  prompt:\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n    `Please suggest the best tourist activities for me to do.`,\n});\nSystem Prompts\n\nSystem prompts are the initial set of instructions given to models that help guide and constrain the models' behaviors and responses. You can set system prompts using the system property. System prompts work with both the prompt and the messages properties.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  system:\n    `You help planning travel itineraries. ` +\n    `Respond to the users' request with a list ` +\n    `of the best stops to make in their destination.`,\n  prompt:\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n    `Please suggest the best tourist activities for me to do.`,\n});\n\nWhen you use a message prompt, you can also use system messages instead of a system prompt.\n\nMessage Prompts\n\nA message prompt is an array of user, assistant, and tool messages. They are great for chat interfaces and more complex, multi-modal prompts. You can use the messages property to set message prompts.\n\nEach message has a role and a content property. The content can either be text (for user and assistant messages), or an array of relevant parts (data) for that message type.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hello, how can I help?' },\n    { role: 'user', content: 'Where can I buy the best Currywurst in Berlin?' },\n  ],\n});\n\nInstead of sending a text in the content property, you can send an array of parts that includes a mix of text and other content parts.\n\nNot all language models support all message and content types. For example, some models might not be capable of handling multi-modal inputs or tool messages. Learn more about the capabilities of select models.\n\nProvider Options\n\nYou can pass through additional provider-specific metadata to enable provider-specific functionality at 3 levels.\n\nFunction Call Level\n\nFunctions like streamText or generateText accept a providerOptions property.\n\nAdding provider options at the function call level should be used when you do not need granular control over where the provider options are applied.\n\nconst { text } = await generateText({\n  model: azure('your-deployment-name'),\n  providerOptions: {\n    openai: {\n      reasoningEffort: 'low',\n    },\n  },\n});\nMessage Level\n\nFor granular control over applying provider options at the message level, you can pass providerOptions to the message object:\n\nimport { ModelMessage } from 'ai';\n\n\nconst messages: ModelMessage[] = [\n  {\n    role: 'system',\n    content: 'Cached system message',\n    providerOptions: {\n      // Sets a cache control breakpoint on the system message\n      anthropic: { cacheControl: { type: 'ephemeral' } },\n    },\n  },\n];\nMessage Part Level\n\nCertain provider-specific options require configuration at the message part level:\n\nimport { ModelMessage } from 'ai';\n\n\nconst messages: ModelMessage[] = [\n  {\n    role: 'user',\n    content: [\n      {\n        type: 'text',\n        text: 'Describe the image in detail.',\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n      {\n        type: 'image',\n        image:\n          'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n        // Sets image detail configuration for image part:\n        providerOptions: {\n          openai: { imageDetail: 'low' },\n        },\n      },\n    ],\n  },\n];\n\nAI SDK UI hooks like useChat return arrays of UIMessage objects, which do not support provider options. We recommend using the convertToModelMessages function to convert UIMessage objects to ModelMessage objects before applying or appending message(s) or message parts with providerOptions.\n\nUser Messages\nText Parts\n\nText content is the most common type of content. It is a string that is passed to the model.\n\nIf you only need to send text content in a message, the content property can be a string, but you can also use it to send multiple content parts.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'Where can I buy the best Currywurst in Berlin?',\n        },\n      ],\n    },\n  ],\n});\nImage Parts\n\nUser messages can include image parts. An image can be one of the following:\n\nbase64-encoded image:\nstring with base-64 encoded content\ndata URL string, e.g. data:image/png;base64,...\nbinary image:\nArrayBuffer\nUint8Array\nBuffer\nURL:\nhttp(s) URL string, e.g. https://example.com/image.png\nURL object, e.g. new URL('https://example.com/image.png')\nExample: Binary image (Buffer)\nconst result = await generateText({\n  model,\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image: fs.readFileSync('./data/comic-cat.png'),\n        },\n      ],\n    },\n  ],\n});\nExample: Base-64 encoded image (string)\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image: fs.readFileSync('./data/comic-cat.png').toString('base64'),\n        },\n      ],\n    },\n  ],\n});\nExample: Image URL (string)\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'Describe the image in detail.' },\n        {\n          type: 'image',\n          image:\n            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\n        },\n      ],\n    },\n  ],\n});\nFile Parts\n\nOnly a few providers and models currently support file parts: Google Generative AI, Google Vertex AI, OpenAI (for wav and mp3 audio with gpt-4o-audio-preview), Anthropic, OpenAI (for pdf).\n\nUser messages can include file parts. A file can be one of the following:\n\nbase64-encoded file:\nstring with base-64 encoded content\ndata URL string, e.g. data:image/png;base64,...\nbinary data:\nArrayBuffer\nUint8Array\nBuffer\nURL:\nhttp(s) URL string, e.g. https://example.com/some.pdf\nURL object, e.g. new URL('https://example.com/some.pdf')\n\nYou need to specify the MIME type of the file you are sending.\n\nExample: PDF file from Buffer\nimport { google } from '@ai-sdk/google';\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  model: google('gemini-1.5-flash'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the file about?' },\n        {\n          type: 'file',\n          mediaType: 'application/pdf',\n          data: fs.readFileSync('./data/example.pdf'),\n          filename: 'example.pdf', // optional, not used by all providers\n        },\n      ],\n    },\n  ],\n});\nExample: mp3 audio file from Buffer\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\n\nconst result = await generateText({\n  model: openai('gpt-4o-audio-preview'),\n  messages: [\n    {\n      role: 'user',\n      content: [\n        { type: 'text', text: 'What is the audio saying?' },\n        {\n          type: 'file',\n          mediaType: 'audio/mpeg',\n          data: fs.readFileSync('./data/galileo.mp3'),\n        },\n      ],\n    },\n  ],\n});\nCustom Download Function (Experimental)\n\nYou can use custom download functions to implement throttling, retries, authentication, caching, and more.\n\nThe default download implementation automatically downloads files in parallel when they are not supported by the model.\n\nCustom download function can be passed via the experimental_download property:\n\nconst result = await generateText({\n  model: openai('gpt-4o'),\n  experimental_download: async (\n    requestedDownloads: Array<{\n      url: URL;\n      isUrlSupportedByModel: boolean;\n    }>,\n  ): PromiseLike<\n    Array<{\n      data: Uint8Array;\n      mediaType: string | undefined;\n    } | null>\n  > => {\n    // ... download the files and return an array with similar order\n  },\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'file',\n          data: new URL('https://api.company.com/private/document.pdf'),\n          mediaType: 'application/pdf',\n        },\n      ],\n    },\n  ],\n});\n\nThe experimental_download option is experimental and may change in future releases.\n\nAssistant Messages\n\nAssistant messages are messages that have a role of assistant. They are typically previous responses from the assistant and can contain text, reasoning, and tool call parts.\n\nExample: Assistant message with text content\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hello, how can I help?' },\n  ],\n});\nExample: Assistant message with text content in array\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Hi!' },\n    {\n      role: 'assistant',\n      content: [{ type: 'text', text: 'Hello, how can I help?' }],\n    },\n  ],\n});\nExample: Assistant message with tool call content\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'How many calories are in this block of cheese?' },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'tool-call',\n          toolCallId: '12345',\n          toolName: 'get-nutrition-data',\n          input: { cheese: 'Roquefort' },\n        },\n      ],\n    },\n  ],\n});\nExample: Assistant message with file content\n\nThis content part is for model-generated files. Only a few models support this, and only for file types that they can generate.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'user', content: 'Generate an image of a roquefort cheese!' },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'file',\n          mediaType: 'image/png',\n          data: fs.readFileSync('./data/roquefort.jpg'),\n        },\n      ],\n    },\n  ],\n});\nTool messages\n\nTools (also known as function calling) are programs that you can provide an LLM to extend its built-in functionality. This can be anything from calling an external API to calling functions within your UI. Learn more about Tools in the next section.\n\nFor models that support tool calls, assistant messages can contain tool call parts, and tool messages can contain tool output parts. A single assistant message can call multiple tools, and a single tool message can contain multiple tool results.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    {\n      role: 'user',\n      content: [\n        {\n          type: 'text',\n          text: 'How many calories are in this block of cheese?',\n        },\n        { type: 'image', image: fs.readFileSync('./data/roquefort.jpg') },\n      ],\n    },\n    {\n      role: 'assistant',\n      content: [\n        {\n          type: 'tool-call',\n          toolCallId: '12345',\n          toolName: 'get-nutrition-data',\n          input: { cheese: 'Roquefort' },\n        },\n        // there could be more tool calls here (parallel calling)\n      ],\n    },\n    {\n      role: 'tool',\n      content: [\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          output: {\n            type: 'json',\n            value: {\n              name: 'Cheese, roquefort',\n              calories: 369,\n              fat: 31,\n              protein: 22,\n            },\n          },\n        },\n        // there could be more tool results here (parallel calling)\n      ],\n    },\n  ],\n});\nMulti-modal Tool Results\n\nMulti-part tool results are experimental and only supported by Anthropic.\n\nTool results can be multi-part and multi-modal, e.g. a text and an image. You can use the experimental_content property on tool parts to specify multi-part tool results.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    // ...\n    {\n      role: 'tool',\n      content: [\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          // for models that do not support multi-part tool results,\n          // you can include a regular output part:\n          output: {\n            type: 'json',\n            value: {\n              name: 'Cheese, roquefort',\n              calories: 369,\n              fat: 31,\n              protein: 22,\n            },\n          },\n        },\n        {\n          type: 'tool-result',\n          toolCallId: '12345', // needs to match the tool call id\n          toolName: 'get-nutrition-data',\n          // for models that support multi-part tool results,\n          // you can include a multi-part content part:\n          output: {\n            type: 'content',\n            value: [\n              {\n                type: 'text',\n                text: 'Here is an image of the nutrition data for the cheese:',\n              },\n              {\n                type: 'media',\n                data: fs\n                  .readFileSync('./data/roquefort-nutrition-data.png')\n                  .toString('base64'),\n                mediaType: 'image/png',\n              },\n            ],\n          },\n        },\n      ],\n    },\n  ],\n});\nSystem Messages\n\nSystem messages are messages that are sent to the model before the user messages to guide the assistant's behavior. You can alternatively use the system property.\n\nconst result = await generateText({\n  model: 'openai/gpt-4.1',\n  messages: [\n    { role: 'system', content: 'You help planning travel itineraries.' },\n    {\n      role: 'user',\n      content:\n        'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.',\n    },\n  ],\n});\nPrevious\nProviders and Models\nNext\nTools"
  },
  {
    "title": "AI SDK UI: Chatbot Tool Usage",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
    "html": "AI SDK UI\nChatbot Tool Usage\nCopy markdown\nChatbot Tool Usage\n\nWith useChat and streamText, you can use tools in your chatbot application. The AI SDK supports three types of tools in this context:\n\nAutomatically executed server-side tools\nAutomatically executed client-side tools\nTools that require user interaction, such as confirmation dialogs\n\nThe flow is as follows:\n\nThe user enters a message in the chat UI.\nThe message is sent to the API route.\nIn your server side route, the language model generates tool calls during the streamText call.\nAll tool calls are forwarded to the client.\nServer-side tools are executed using their execute method and their results are forwarded to the client.\nClient-side tools that should be automatically executed are handled with the onToolCall callback. You must call addToolResult to provide the tool result.\nClient-side tool that require user interactions can be displayed in the UI. The tool calls and results are available as tool invocation parts in the parts property of the last assistant message.\nWhen the user interaction is done, addToolResult can be used to add the tool result to the chat.\nThe chat can be configured to automatically submit when all tool results are available using sendAutomaticallyWhen. This triggers another iteration of this flow.\n\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts. A tool part is at first a tool call, and then it becomes a tool result when the tool is executed. The tool result contains all information about the tool call as well as the result of the tool execution.\n\nTool result submission can be configured using the sendAutomaticallyWhen option. You can use the lastAssistantMessageIsCompleteWithToolCalls helper to automatically submit when all tool results are available. This simplifies the client-side code while still allowing full control when needed.\n\nExample\n\nIn this example, we'll use three tools:\n\ngetWeatherInformation: An automatically executed server-side tool that returns the weather in a given city.\naskForConfirmation: A user-interaction client-side tool that asks the user for confirmation.\ngetLocation: An automatically executed client-side tool that returns a random city.\nAPI route\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\nimport { z } from 'zod';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      // server-side tool with execute function:\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n      // client-side tool that starts user interaction:\n      askForConfirmation: {\n        description: 'Ask the user for confirmation.',\n        inputSchema: z.object({\n          message: z.string().describe('The message to ask for confirmation.'),\n        }),\n      },\n      // client-side tool that is automatically executed on the client:\n      getLocation: {\n        description:\n          'Get the user location. Always ask for confirmation before using this tool.',\n        inputSchema: z.object({}),\n      },\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nClient-side page\n\nThe client-side page uses the useChat hook to create a chatbot application with real-time message streaming. Tool calls are displayed in the chat UI as typed tool parts. Please make sure to render the messages using the parts property of the message.\n\nThere are three things worth mentioning:\n\nThe onToolCall callback is used to handle client-side tools that should be automatically executed. In this example, the getLocation tool is a client-side tool that returns a random city. You call addToolResult to provide the result (without await to avoid potential deadlocks).\n\nAlways check if (toolCall.dynamic) first in your onToolCall handler. Without this check, TypeScript will throw an error like: Type 'string' is not assignable to type '\"toolName1\" | \"toolName2\"' when you try to use toolCall.toolName in addToolResult.\n\nThe sendAutomaticallyWhen option with lastAssistantMessageIsCompleteWithToolCalls helper automatically submits when all tool results are available.\n\nThe parts array of assistant messages contains tool parts with typed names like tool-askForConfirmation. The client-side tool askForConfirmation is displayed in the UI. It asks the user for confirmation and displays the result once the user confirms or denies the execution. The result is added to the chat using addToolResult with the tool parameter for type safety.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getLocation') {\n        const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];\n\n\n        // No await - avoids potential deadlocks\n        addToolResult({\n          tool: 'getLocation',\n          toolCallId: toolCall.toolCallId,\n          output: cities[Math.floor(Math.random() * cities.length)],\n        });\n      }\n    },\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map(part => {\n            switch (part.type) {\n              // render text parts as simple text:\n              case 'text':\n                return part.text;\n\n\n              // for tool parts, use the typed tool part names:\n              case 'tool-askForConfirmation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Loading confirmation request...</div>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        {part.input.message}\n                        <div>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'Yes, confirmed.',\n                              })\n                            }\n                          >\n                            Yes\n                          </button>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'No, denied',\n                              })\n                            }\n                          >\n                            No\n                          </button>\n                        </div>\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Location access allowed: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={callId}>Error: {part.errorText}</div>;\n                }\n                break;\n              }\n\n\n              case 'tool-getLocation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Preparing location request...</div>\n                    );\n                  case 'input-available':\n                    return <div key={callId}>Getting location...</div>;\n                  case 'output-available':\n                    return <div key={callId}>Location: {part.output}</div>;\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting location: {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n\n\n              case 'tool-getWeatherInformation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  // example of pre-rendering streaming tool inputs:\n                  case 'input-streaming':\n                    return (\n                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        Getting weather information for {part.input.city}...\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Weather in {part.input.city}: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting weather for {part.input.city}:{' '}\n                        {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n            }\n          })}\n          <br />\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </>\n  );\n}\nError handling\n\nSometimes an error may occur during client-side tool execution. Use the addToolResult method with a state of output-error and errorText value instead of output record the error.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getWeatherInformation') {\n        try {\n          const weather = await getWeatherInformation(toolCall.input);\n\n\n          // No await - avoids potential deadlocks\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            output: weather,\n          });\n        } catch (err) {\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            state: 'output-error',\n            errorText: 'Unable to get the weather information',\n          });\n        }\n      }\n    },\n  });\n}\nDynamic Tools\n\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic dynamic-tool type instead of specific tool types:\n\napp/page.tsx\n{\n  message.parts.map((part, index) => {\n    switch (part.type) {\n      // Static tools with specific (`tool-${toolName}`) types\n      case 'tool-getWeatherInformation':\n        return <WeatherDisplay part={part} />;\n\n\n      // Dynamic tools use generic `dynamic-tool` type\n      case 'dynamic-tool':\n        return (\n          <div key={index}>\n            <h4>Tool: {part.toolName}</h4>\n            {part.state === 'input-streaming' && (\n              <pre>{JSON.stringify(part.input, null, 2)}</pre>\n            )}\n            {part.state === 'output-available' && (\n              <pre>{JSON.stringify(part.output, null, 2)}</pre>\n            )}\n            {part.state === 'output-error' && (\n              <div>Error: {part.errorText}</div>\n            )}\n          </div>\n        );\n    }\n  });\n}\n\nDynamic tools are useful when integrating with:\n\nMCP (Model Context Protocol) tools without schemas\nUser-defined functions loaded at runtime\nExternal tool providers\nTool call streaming\n\nTool call streaming is enabled by default in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    // toolCallStreaming is enabled by default in v5\n    // ...\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream. They are available through the useChat hook. The typed tool parts of assistant messages will also contain partial tool calls. You can use the state property of the tool part to render the correct UI.\n\napp/page.tsx\nexport default function Chat() {\n  // ...\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.parts.map(part => {\n            switch (part.type) {\n              case 'tool-askForConfirmation':\n              case 'tool-getLocation':\n              case 'tool-getWeatherInformation':\n                switch (part.state) {\n                  case 'input-streaming':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'input-available':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'output-available':\n                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\n                  case 'output-error':\n                    return <div>Error: {part.errorText}</div>;\n                }\n            }\n          })}\n        </div>\n      ))}\n    </>\n  );\n}\nStep start parts\n\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages. If you want to display boundaries between tool calls, you can use the step-start parts as follows:\n\napp/page.tsx\n// ...\n// where you render the message parts:\nmessage.parts.map((part, index) => {\n  switch (part.type) {\n    case 'step-start':\n      // show step boundaries as horizontal lines:\n      return index > 0 ? (\n        <div key={index} className=\"text-gray-500\">\n          <hr className=\"my-2 border-gray-300\" />\n        </div>\n      ) : null;\n    case 'text':\n    // ...\n    case 'tool-askForConfirmation':\n    case 'tool-getLocation':\n    case 'tool-getWeatherInformation':\n    // ...\n  }\n});\n// ...\nServer-side Multi-Step Calls\n\nYou can also use multi-step calls on the server-side with streamText. This works when all invoked tools have an execute function on the server side.\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        // tool has execute function:\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n    },\n    stopWhen: stepCountIs(5),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nErrors\n\nLanguage models can make errors when calling tools. By default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\n\nTo surface the errors, you can use the onError function when calling toUIMessageResponse.\n\nexport function errorHandler(error: unknown) {\n  if (error == null) {\n    return 'unknown error';\n  }\n\n\n  if (typeof error === 'string') {\n    return error;\n  }\n\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n\n  return JSON.stringify(error);\n}\nconst result = streamText({\n  // ...\n});\n\n\nreturn result.toUIMessageStreamResponse({\n  onError: errorHandler,\n});\n\nIn case you are using createUIMessageResponse, you can use the onError function when calling toUIMessageResponse:\n\nconst response = createUIMessageResponse({\n  // ...\n  async execute(dataStream) {\n    // ...\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\nPrevious\nChatbot Resume Streams\nNext\nGenerative User Interfaces"
  },
  {
    "title": "Advanced: Stopping Streams",
    "url": "https://ai-sdk.dev/docs/advanced/stopping-streams",
    "html": "Advanced\nStopping Streams\nCopy markdown\nStopping Streams\n\nCancelling ongoing streams is often needed. For example, users might want to stop a stream when they realize that the response is not what they want.\n\nThe different parts of the AI SDK support cancelling streams in different ways.\n\nAI SDK Core\n\nThe AI SDK functions have an abortSignal argument that you can use to cancel a stream. You would use this if you want to cancel a stream from the server side to the LLM API, e.g. by forwarding the abortSignal from the request.\n\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    prompt,\n    // forward the abort signal:\n    abortSignal: req.signal,\n    onAbort: ({ steps }) => {\n      // Handle cleanup when stream is aborted\n      console.log('Stream aborted after', steps.length, 'steps');\n      // Persist partial results to database\n    },\n  });\n\n\n  return result.toTextStreamResponse();\n}\nAI SDK UI\n\nThe hooks, e.g. useChat or useCompletion, provide a stop helper function that can be used to cancel a stream. This will cancel the stream from the client side to the server.\n\nStream abort functionality is not compatible with stream resumption. If you're using resume: true in useChat, the abort functionality will break the resumption mechanism. Choose either abort or resume functionality, but not both.\n\n'use client';\n\n\nimport { useCompletion } from '@ai-sdk/react';\n\n\nexport default function Chat() {\n  const { input, completion, stop, status, handleSubmit, handleInputChange } =\n    useCompletion();\n\n\n  return (\n    <div>\n      {(status === 'submitted' || status === 'streaming') && (\n        <button type=\"button\" onClick={() => stop()}>\n          Stop\n        </button>\n      )}\n      {completion}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n      </form>\n    </div>\n  );\n}\nHandling stream abort cleanup\n\nWhen streams are aborted, you may need to perform cleanup operations such as persisting partial results or cleaning up resources. The onAbort callback provides a way to handle these scenarios on the server side.\n\nUnlike onFinish, which is called when a stream completes normally, onAbort is specifically called when a stream is aborted via AbortSignal. This distinction allows you to handle normal completion and aborted streams differently.\n\nFor UI message streams (toUIMessageStreamResponse), the onFinish callback also receives an isAborted parameter that indicates whether the stream was aborted. This allows you to handle both completion and abort scenarios in a single callback.\n\nimport { streamText } from 'ai';\n\n\nconst result = streamText({\n  model: openai('gpt-4.1'),\n  prompt: 'Write a long story...',\n  abortSignal: controller.signal,\n  onAbort: ({ steps }) => {\n    // Called when stream is aborted - persist partial results\n    await savePartialResults(steps);\n    await logAbortEvent(steps.length);\n  },\n  onFinish: ({ steps, totalUsage }) => {\n    // Called when stream completes normally\n    await saveFinalResults(steps, totalUsage);\n  },\n});\n\nThe onAbort callback receives:\n\nsteps: Array of all completed steps before the abort occurred\n\nThis is particularly useful for:\n\nPersisting partial conversation history to database\nSaving partial progress for later continuation\nCleaning up server-side resources or connections\nLogging abort events for analytics\n\nYou can also handle abort events directly in the stream using the abort stream part:\n\nfor await (const part of result.fullStream) {\n  switch (part.type) {\n    case 'text-delta':\n      // Handle text delta content\n      break;\n    case 'abort':\n      // Handle abort event directly in stream\n      console.log('Stream was aborted');\n      break;\n    // ... other cases\n  }\n}\nUI Message Streams\n\nWhen using toUIMessageStreamResponse, you need to handle stream abortion slightly differently. The onFinish callback receives an isAborted parameter, and you should pass the consumeStream function to ensure proper abort handling:\n\nimport { openai } from '@ai-sdk/openai';\nimport {\n  consumeStream,\n  convertToModelMessages,\n  streamText,\n  UIMessage,\n} from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    abortSignal: req.signal,\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    onFinish: async ({ isAborted }) => {\n      if (isAborted) {\n        console.log('Stream was aborted');\n        // Handle abort-specific cleanup\n      } else {\n        console.log('Stream completed normally');\n        // Handle normal completion\n      }\n    },\n    consumeSseStream: consumeStream,\n  });\n}\n\nThe consumeStream function is necessary for proper abort handling in UI message streams. It ensures that the stream is properly consumed even when aborted, preventing potential memory leaks or hanging connections.\n\nAI SDK RSC\n\nThe AI SDK RSC does not currently support stopping streams.\n\nPrevious\nPrompt Engineering\nNext\nBackpressure"
  },
  {
    "title": "Advanced: Backpressure",
    "url": "https://ai-sdk.dev/docs/advanced/backpressure",
    "html": "Advanced\nBackpressure\nCopy markdown\nStream Back-pressure and Cancellation\n\nThis page focuses on understanding back-pressure and cancellation when working with streams. You do not need to know this information to use the AI SDK, but for those interested, it offers a deeper dive on why and how the SDK optimally streams responses.\n\nIn the following sections, we'll explore back-pressure and cancellation in the context of a simple example program. We'll discuss the issues that can arise from an eager approach and demonstrate how a lazy approach can resolve them.\n\nBack-pressure and Cancellation with Streams\n\nLet's begin by setting up a simple example program:\n\n// A generator that will yield positive integers\nasync function* integers() {\n  let i = 1;\n  while (true) {\n    console.log(`yielding ${i}`);\n    yield i++;\n\n\n    await sleep(100);\n  }\n}\nfunction sleep(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\n\n// Wraps a generator into a ReadableStream\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async start(controller) {\n      for await (const v of iterator) {\n        controller.enqueue(v);\n      }\n      controller.close();\n    },\n  });\n}\n\n\n// Collect data from stream\nasync function run() {\n  // Set up a stream of integers\n  const stream = createStream(integers());\n\n\n  // Read values from our stream\n  const reader = stream.getReader();\n  for (let i = 0; i < 10_000; i++) {\n    // we know our stream is infinite, so there's no need to check `done`.\n    const { value } = await reader.read();\n    console.log(`read ${value}`);\n\n\n    await sleep(1_000);\n  }\n}\nrun();\n\nIn this example, we create an async-generator that yields positive integers, a ReadableStream that wraps our integer generator, and a reader which will read values out of our stream. Notice, too, that our integer generator logs out \"yielding ${i}\", and our reader logs out \"read ${value}\". Both take an arbitrary amount of time to process data, represented with a 100ms sleep in our generator, and a 1sec sleep in our reader.\n\nBack-pressure\n\nIf you were to run this program, you'd notice something funny. We'll see roughly 10 \"yield\" logs for every \"read\" log. This might seem obvious, the generator can push values 10x faster than the reader can pull them out. But it represents a problem, our stream has to maintain an ever expanding queue of items that have been pushed in but not pulled out.\n\nThe problem stems from the way we wrap our generator into a stream. Notice the use of for await (…) inside our start handler. This is an eager for-loop, and it is constantly running to get the next value from our generator to be enqueued in our stream. This means our stream does not respect back-pressure, the signal from the consumer to the producer that more values aren't needed yet. We've essentially spawned a thread that will perpetually push more data into the stream, one that runs as fast as possible to push new data immediately. Worse, there's no way to signal to this thread to stop running when we don't need additional data.\n\nTo fix this, ReadableStream allows a pull handler. pull is called every time the consumer attempts to read more data from our stream (if there's no data already queued internally). But it's not enough to just move the for await(…) into pull, we also need to convert from an eager enqueuing to a lazy one. By making these 2 changes, we'll be able to react to the consumer. If they need more data, we can easily produce it, and if they don't, then we don't need to spend any time doing unnecessary work.\n\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async pull(controller) {\n      const { value, done } = await iterator.next();\n\n\n      if (done) {\n        controller.close();\n      } else {\n        controller.enqueue(value);\n      }\n    },\n  });\n}\n\nOur createStream is a little more verbose now, but the new code is important. First, we need to manually call our iterator.next() method. This returns a Promise for an object with the type signature { done: boolean, value: T }. If done is true, then we know that our iterator won't yield any more values and we must close the stream (this allows the consumer to know that the stream is also finished producing values). Else, we need to enqueue our newly produced value.\n\nWhen we run this program, we see that our \"yield\" and \"read\" logs are now paired. We're no longer yielding 10x integers for every read! And, our stream now only needs to maintain 1 item in its internal buffer. We've essentially given control to the consumer, so that it's responsible for producing new values as it needs it. Neato!\n\nCancellation\n\nLet's go back to our initial eager example, with 1 small edit. Now instead of reading 10,000 integers, we're only going to read 3:\n\n// A generator that will yield positive integers\nasync function* integers() {\n  let i = 1;\n  while (true) {\n    console.log(`yielding ${i}`);\n    yield i++;\n\n\n    await sleep(100);\n  }\n}\nfunction sleep(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\n\n// Wraps a generator into a ReadableStream\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async start(controller) {\n      for await (const v of iterator) {\n        controller.enqueue(v);\n      }\n      controller.close();\n    },\n  });\n}\n// Collect data from stream\nasync function run() {\n  // Set up a stream that of integers\n  const stream = createStream(integers());\n\n\n  // Read values from our stream\n  const reader = stream.getReader();\n  // We're only reading 3 items this time:\n  for (let i = 0; i < 3; i++) {\n    // we know our stream is infinite, so there's no need to check `done`.\n    const { value } = await reader.read();\n    console.log(`read ${value}`);\n\n\n    await sleep(1000);\n  }\n}\nrun();\n\nWe're back to yielding 10x the number of values read. But notice now, after we've read 3 values, we're continuing to yield new values. We know that our reader will never read another value, but our stream doesn't! The eager for await (…) will continue forever, loudly enqueuing new values into our stream's buffer and increasing our memory usage until it consumes all available program memory.\n\nThe fix to this is exactly the same: use pull and manual iteration. By producing values lazily, we tie the lifetime of our integer generator to the lifetime of the reader. Once the reads stop, the yields will stop too:\n\n// Wraps a generator into a ReadableStream\nfunction createStream(iterator) {\n  return new ReadableStream({\n    async pull(controller) {\n      const { value, done } = await iterator.next();\n\n\n      if (done) {\n        controller.close();\n      } else {\n        controller.enqueue(value);\n      }\n    },\n  });\n}\n\nSince the solution is the same as implementing back-pressure, it shows that they're just 2 facets of the same problem: Pushing values into a stream should be done lazily, and doing it eagerly results in expected problems.\n\nTying Stream Laziness to AI Responses\n\nNow let's imagine you're integrating AIBot service into your product. Users will be able to prompt \"count from 1 to infinity\", the browser will fetch your AI API endpoint, and your servers connect to AIBot to get a response. But \"infinity\" is, well, infinite. The response will never end!\n\nAfter a few seconds, the user gets bored and navigates away. Or maybe you're doing local development and a hot-module reload refreshes your page. The browser will have ended its connection to the API endpoint, but will your server end its connection with AIBot?\n\nIf you used the eager for await (...) approach, then the connection is still running and your server is asking for more and more data from AIBot. Our server spawned a \"thread\" and there's no signal when we can end the eager pulls. Eventually, the server is going to run out of memory (remember, there's no active fetch connection to read the buffering responses and free them).\n\nWith the lazy approach, this is taken care of for you. Because the stream will only request new data from AIBot when the consumer requests it, navigating away from the page naturally frees all resources. The fetch connection aborts and the server can clean up the response. The ReadableStream tied to that response can now be garbage collected. When that happens, the connection it holds to AIBot can then be freed.\n\nPrevious\nStopping Streams\nNext\nCaching"
  },
  {
    "title": "Advanced: Caching",
    "url": "https://ai-sdk.dev/docs/advanced/caching",
    "html": "Advanced\nCaching\nCopy markdown\nCaching Responses\n\nDepending on the type of application you're building, you may want to cache the responses you receive from your AI provider, at least temporarily.\n\nUsing Language Model Middleware (Recommended)\n\nThe recommended approach to caching responses is using language model middleware and the simulateReadableStream function.\n\nLanguage model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model. Let's see how you can use language model middleware to cache responses.\n\nai/middleware.ts\nimport { Redis } from '@upstash/redis';\nimport {\n  type LanguageModelV2,\n  type LanguageModelV2Middleware,\n  type LanguageModelV2StreamPart,\n  simulateReadableStream,\n} from 'ai';\n\n\nconst redis = new Redis({\n  url: process.env.KV_URL,\n  token: process.env.KV_TOKEN,\n});\n\n\nexport const cacheMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n\n    const cached = (await redis.get(cacheKey)) as Awaited<\n      ReturnType<LanguageModelV2['doGenerate']>\n    > | null;\n\n\n    if (cached !== null) {\n      return {\n        ...cached,\n        response: {\n          ...cached.response,\n          timestamp: cached?.response?.timestamp\n            ? new Date(cached?.response?.timestamp)\n            : undefined,\n        },\n      };\n    }\n\n\n    const result = await doGenerate();\n\n\n    redis.set(cacheKey, result);\n\n\n    return result;\n  },\n  wrapStream: async ({ doStream, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n\n    // Check if the result is in the cache\n    const cached = await redis.get(cacheKey);\n\n\n    // If cached, return a simulated ReadableStream that yields the cached result\n    if (cached !== null) {\n      // Format the timestamps in the cached response\n      const formattedChunks = (cached as LanguageModelV2StreamPart[]).map(p => {\n        if (p.type === 'response-metadata' && p.timestamp) {\n          return { ...p, timestamp: new Date(p.timestamp) };\n        } else return p;\n      });\n      return {\n        stream: simulateReadableStream({\n          initialDelayInMs: 0,\n          chunkDelayInMs: 10,\n          chunks: formattedChunks,\n        }),\n      };\n    }\n\n\n    // If not cached, proceed with streaming\n    const { stream, ...rest } = await doStream();\n\n\n    const fullResponse: LanguageModelV2StreamPart[] = [];\n\n\n    const transformStream = new TransformStream<\n      LanguageModelV2StreamPart,\n      LanguageModelV2StreamPart\n    >({\n      transform(chunk, controller) {\n        fullResponse.push(chunk);\n        controller.enqueue(chunk);\n      },\n      flush() {\n        // Store the full response in the cache after streaming is complete\n        redis.set(cacheKey, fullResponse);\n      },\n    });\n\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n\nThis example uses @upstash/redis to store and retrieve the assistant's responses but you can use any KV storage provider you would like.\n\nLanguageModelMiddleware has two methods: wrapGenerate and wrapStream. wrapGenerate is called when using generateText and generateObject, while wrapStream is called when using streamText and streamObject.\n\nFor wrapGenerate, you can cache the response directly. Instead, for wrapStream, you cache an array of the stream parts, which can then be used with simulateReadableStream function to create a simulated ReadableStream that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the initialDelayInMs and chunkDelayInMs parameters of simulateReadableStream.\n\nYou can see a full example of caching with Redis in a Next.js application in our Caching Middleware Recipe.\n\nUsing Lifecycle Callbacks\n\nAlternatively, each AI SDK Core function has special lifecycle callbacks you can use. The one of interest is likely onFinish, which is called when the generation is complete. This is where you can cache the full response.\n\nHere's an example of how you can implement caching using Vercel KV and Next.js to cache the OpenAI response for 1 hour:\n\nThis example uses Upstash Redis\n and Next.js to cache the response for 1 hour.\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { formatDataStreamPart, streamText, UIMessage } from 'ai';\nimport { Redis } from '@upstash/redis';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nconst redis = new Redis({\n  url: process.env.KV_URL,\n  token: process.env.KV_TOKEN,\n});\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  // come up with a key based on the request:\n  const key = JSON.stringify(messages);\n\n\n  // Check if we have a cached response\n  const cached = await redis.get(key);\n  if (cached != null) {\n    return new Response(formatDataStreamPart('text', cached), {\n      status: 200,\n      headers: { 'Content-Type': 'text/plain' },\n    });\n  }\n\n\n  // Call the language model:\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    async onFinish({ text }) {\n      // Cache the response text:\n      await redis.set(key, text);\n      await redis.expire(key, 60 * 60);\n    },\n  });\n\n\n  // Respond with the stream\n  return result.toUIMessageStreamResponse();\n}\nPrevious\nBackpressure\nNext\nMultiple Streamables"
  },
  {
    "title": "Advanced: Multiple Streamables",
    "url": "https://ai-sdk.dev/docs/advanced/multiple-streamables",
    "html": "Advanced\nMultiple Streamables\nCopy markdown\nMultiple Streams\nMultiple Streamable UIs\n\nThe AI SDK RSC APIs allow you to compose and return any number of streamable UIs, along with other data, in a single request. This can be useful when you want to decouple the UI into smaller components and stream them separately.\n\n'use server';\n\n\nimport { createStreamableUI } from '@ai-sdk/rsc';\n\n\nexport async function getWeather() {\n  const weatherUI = createStreamableUI();\n  const forecastUI = createStreamableUI();\n\n\n  weatherUI.update(<div>Loading weather...</div>);\n  forecastUI.update(<div>Loading forecast...</div>);\n\n\n  getWeatherData().then(weatherData => {\n    weatherUI.done(<div>{weatherData}</div>);\n  });\n\n\n  getForecastData().then(forecastData => {\n    forecastUI.done(<div>{forecastData}</div>);\n  });\n\n\n  // Return both streamable UIs and other data fields.\n  return {\n    requestedAt: Date.now(),\n    weather: weatherUI.value,\n    forecast: forecastUI.value,\n  };\n}\n\nThe client side code is similar to the previous example, but the tool call will return the new data structure with the weather and forecast UIs. Depending on the speed of getting weather and forecast data, these two components might be updated independently.\n\nNested Streamable UIs\n\nYou can stream UI components within other UI components. This allows you to create complex UIs that are built up from smaller, reusable components. In the example below, we pass a historyChart streamable as a prop to a StockCard component. The StockCard can render the historyChart streamable, and it will automatically update as the server responds with new data.\n\nasync function getStockHistoryChart({ symbol: string }) {\n  'use server';\n\n\n  const ui = createStreamableUI(<Spinner />);\n\n\n  // We need to wrap this in an async IIFE to avoid blocking.\n  (async () => {\n    const price = await getStockPrice({ symbol });\n\n\n    // Show a spinner as the history chart for now.\n    const historyChart = createStreamableUI(<Spinner />);\n    ui.done(<StockCard historyChart={historyChart.value} price={price} />);\n\n\n    // Getting the history data and then update that part of the UI.\n    const historyData = await fetch('https://my-stock-data-api.com');\n    historyChart.done(<HistoryChart data={historyData} />);\n  })();\n\n\n  return ui;\n}\nPrevious\nCaching\nNext\nRate Limiting"
  },
  {
    "title": "Advanced: Rate Limiting",
    "url": "https://ai-sdk.dev/docs/advanced/rate-limiting",
    "html": "Advanced\nRate Limiting\nCopy markdown\nRate Limiting\n\nRate limiting helps you protect your APIs from abuse. It involves setting a maximum threshold on the number of requests a client can make within a specified timeframe. This simple technique acts as a gatekeeper, preventing excessive usage that can degrade service performance and incur unnecessary costs.\n\nRate Limiting with Vercel KV and Upstash Ratelimit\n\nIn this example, you will protect an API endpoint using Vercel KV\n and Upstash Ratelimit\n.\n\napp/api/generate/route.ts\nimport kv from '@vercel/kv';\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\nimport { Ratelimit } from '@upstash/ratelimit';\nimport { NextRequest } from 'next/server';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\n// Create Rate limit\nconst ratelimit = new Ratelimit({\n  redis: kv,\n  limiter: Ratelimit.fixedWindow(5, '30s'),\n});\n\n\nexport async function POST(req: NextRequest) {\n  // call ratelimit with request ip\n  const ip = req.ip ?? 'ip';\n  const { success, remaining } = await ratelimit.limit(ip);\n\n\n  // block the request if unsuccessfull\n  if (!success) {\n    return new Response('Ratelimited!', { status: 429 });\n  }\n\n\n  const { messages } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-3.5-turbo'),\n    messages,\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nSimplify API Protection\n\nWith Vercel KV and Upstash Ratelimit, it is possible to protect your APIs from such attacks with ease. To learn more about how Ratelimit works and how it can be configured to your needs, see Ratelimit Documentation\n.\n\nPrevious\nMultiple Streamables\nNext\nRendering UI with Language Models"
  },
  {
    "title": "Advanced: Rendering UI with Language Models",
    "url": "https://ai-sdk.dev/docs/advanced/rendering-ui-with-language-models",
    "html": "Advanced\nRendering UI with Language Models\nCopy markdown\nRendering User Interfaces with Language Models\n\nLanguage models generate text, so at first it may seem like you would only need to render text in your application.\n\napp/actions.tsx\nconst text = generateText({\n  model: openai('gpt-3.5-turbo'),\n  system: 'You are a friendly assistant',\n  prompt: 'What is the weather in SF?',\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({\n        city: z.string().describe('The city to get the weather for'),\n        unit: z\n          .enum(['C', 'F'])\n          .describe('The unit to display the temperature in'),\n      }),\n      execute: async ({ city, unit }) => {\n        const weather = getWeather({ city, unit });\n        return `It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;\n      },\n    },\n  },\n});\n\nAbove, the language model is passed a tool called getWeather that returns the weather information as text. However, instead of returning text, if you return a JSON object that represents the weather information, you can use it to render a React component instead.\n\napp/action.ts\nconst text = generateText({\n  model: openai('gpt-3.5-turbo'),\n  system: 'You are a friendly assistant',\n  prompt: 'What is the weather in SF?',\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({\n        city: z.string().describe('The city to get the weather for'),\n        unit: z\n          .enum(['C', 'F'])\n          .describe('The unit to display the temperature in'),\n      }),\n      execute: async ({ city, unit }) => {\n        const weather = getWeather({ city, unit });\n        const { temperature, unit, description, forecast } = weather;\n\n\n        return {\n          temperature,\n          unit,\n          description,\n          forecast,\n        };\n      },\n    },\n  },\n});\n\nNow you can use the object returned by the getWeather function to conditionally render a React component <WeatherCard/> that displays the weather information by passing the object as props.\n\napp/page.tsx\nreturn (\n  <div>\n    {messages.map(message => {\n      if (message.role === 'function') {\n        const { name, content } = message\n        const { temperature, unit, description, forecast } = content;\n\n\n        return (\n          <WeatherCard\n            weather={{\n              temperature: 47,\n              unit: 'F',\n              description: 'sunny'\n              forecast,\n            }}\n          />\n        )\n      }\n    })}\n  </div>\n)\n\nHere's a little preview of what that might look like.\n\nWhat is the weather in SF?\ngetWeather(\"San Francisco\")\nThursday, March 7\n47°\nSunny\n7am\n48°\n8am\n50°\n9am\n52°\n10am\n54°\n11am\n56°\n12pm\n58°\n1pm\n60°\nThanks!\n\nWeather\n\nAn example of an assistant that renders the weather information in a streamed component.\n\nRendering interfaces as part of language model generations elevates the user experience of your application, allowing people to interact with language models beyond text.\n\nThey also make it easier for you to interpret sequential tool calls that take place in multiple steps and help identify and debug where the model reasoned incorrectly.\n\nRendering Multiple User Interfaces\n\nTo recap, an application has to go through the following steps to render user interfaces as part of model generations:\n\nThe user prompts the language model.\nThe language model generates a response that includes a tool call.\nThe tool call returns a JSON object that represents the user interface.\nThe response is sent to the client.\nThe client receives the response and checks if the latest message was a tool call.\nIf it was a tool call, the client renders the user interface based on the JSON object returned by the tool call.\n\nMost applications have multiple tools that are called by the language model, and each tool can return a different user interface.\n\nFor example, a tool that searches for courses can return a list of courses, while a tool that searches for people can return a list of people. As this list grows, the complexity of your application will grow as well and it can become increasingly difficult to manage these user interfaces.\n\napp/page.tsx\n{\n  message.role === 'tool' ? (\n    message.name === 'api-search-course' ? (\n      <Courses courses={message.content} />\n    ) : message.name === 'api-search-profile' ? (\n      <People people={message.content} />\n    ) : message.name === 'api-meetings' ? (\n      <Meetings meetings={message.content} />\n    ) : message.name === 'api-search-building' ? (\n      <Buildings buildings={message.content} />\n    ) : message.name === 'api-events' ? (\n      <Events events={message.content} />\n    ) : message.name === 'api-meals' ? (\n      <Meals meals={message.content} />\n    ) : null\n  ) : (\n    <div>{message.content}</div>\n  );\n}\nRendering User Interfaces on the Server\n\nThe AI SDK RSC (@ai-sdk/rsc) takes advantage of RSCs to solve the problem of managing all your React components on the client side, allowing you to render React components on the server and stream them to the client.\n\nRather than conditionally rendering user interfaces on the client based on the data returned by the language model, you can directly stream them from the server during a model generation.\n\napp/action.ts\nimport { createStreamableUI } from '@ai-sdk/rsc'\n\n\nconst uiStream = createStreamableUI();\n\n\nconst text = generateText({\n  model: openai('gpt-3.5-turbo'),\n  system: 'you are a friendly assistant'\n  prompt: 'what is the weather in SF?'\n  tools: {\n    getWeather: {\n      description: 'Get the weather for a location',\n      parameters: z.object({\n        city: z.string().describe('The city to get the weather for'),\n        unit: z\n          .enum(['C', 'F'])\n          .describe('The unit to display the temperature in')\n      }),\n      execute: async ({ city, unit }) => {\n        const weather = getWeather({ city, unit })\n        const { temperature, unit, description, forecast } = weather\n\n\n        uiStream.done(\n          <WeatherCard\n            weather={{\n              temperature: 47,\n              unit: 'F',\n              description: 'sunny'\n              forecast,\n            }}\n          />\n        )\n      }\n    }\n  }\n})\n\n\nreturn {\n  display: uiStream.value\n}\n\nThe createStreamableUI function belongs to the @ai-sdk/rsc module and creates a stream that can send React components to the client.\n\nOn the server, you render the <WeatherCard/> component with the props passed to it, and then stream it to the client. On the client side, you only need to render the UI that is streamed from the server.\n\napp/page.tsx\nreturn (\n  <div>\n    {messages.map(message => (\n      <div>{message.display}</div>\n    ))}\n  </div>\n);\n\nNow the steps involved are simplified:\n\nThe user prompts the language model.\nThe language model generates a response that includes a tool call.\nThe tool call renders a React component along with relevant props that represent the user interface.\nThe response is streamed to the client and rendered directly.\n\nNote: You can also render text on the server and stream it to the client using React Server Components. This way, all operations from language model generation to UI rendering can be done on the server, while the client only needs to render the UI that is streamed from the server.\n\nCheck out this example for a full illustration of how to stream component updates with React Server Components in Next.js App Router.\n\nPrevious\nRate Limiting\nNext\nLanguage Models as Routers"
  },
  {
    "title": "Advanced: Language Models as Routers",
    "url": "https://ai-sdk.dev/docs/advanced/model-as-router",
    "html": "Advanced\nLanguage Models as Routers\nCopy markdown\nGenerative User Interfaces\n\nSince language models can render user interfaces as part of their generations, the resulting model generations are referred to as generative user interfaces.\n\nIn this section we will learn more about generative user interfaces and their impact on the way AI applications are built.\n\nDeterministic Routes and Probabilistic Routing\n\nGenerative user interfaces are not deterministic in nature because they depend on the model's generation output. Since these generations are probabilistic in nature, it is possible for every user query to result in a different user interface.\n\nUsers expect their experience using your application to be predictable, so non-deterministic user interfaces can sound like a bad idea at first. However, language models can be set up to limit their generations to a particular set of outputs using their ability to call functions.\n\nWhen language models are provided with a set of function definitions and instructed to execute any of them based on user query, they do either one of the following things:\n\nExecute a function that is most relevant to the user query.\nNot execute any function if the user query is out of bounds of the set of functions available to them.\napp/actions.ts\nconst sendMessage = (prompt: string) =>\n  generateText({\n    model: 'gpt-3.5-turbo',\n    system: 'you are a friendly weather assistant!',\n    prompt,\n    tools: {\n      getWeather: {\n        description: 'Get the weather in a location',\n        parameters: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: async ({ location }: { location: string }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      },\n    },\n  });\n\n\nsendMessage('What is the weather in San Francisco?'); // getWeather is called\nsendMessage('What is the weather in New York?'); // getWeather is called\nsendMessage('What events are happening in London?'); // No function is called\n\nThis way, it is possible to ensure that the generations result in deterministic outputs, while the choice a model makes still remains to be probabilistic.\n\nThis emergent ability exhibited by a language model to choose whether a function needs to be executed or not based on a user query is believed to be models emulating \"reasoning\".\n\nAs a result, the combination of language models being able to reason which function to execute as well as render user interfaces at the same time gives you the ability to build applications where language models can be used as a router.\n\nLanguage Models as Routers\n\nHistorically, developers had to write routing logic that connected different parts of an application to be navigable by a user and complete a specific task.\n\nIn web applications today, most of the routing logic takes place in the form of routes:\n\n/login would navigate you to a page with a login form.\n/user/john would navigate you to a page with profile details about John.\n/api/events?limit=5 would display the five most recent events from an events database.\n\nWhile routes help you build web applications that connect different parts of an application into a seamless user experience, it can also be a burden to manage them as the complexity of applications grow.\n\nNext.js has helped reduce complexity in developing with routes by introducing:\n\nFile-based routing system\nDynamic routing\nAPI routes\nMiddleware\nApp router, and so on...\n\nWith language models becoming better at reasoning, we believe that there is a future where developers only write core application specific components while models take care of routing them based on the user's state in an application.\n\nWith generative user interfaces, the language model decides which user interface to render based on the user's state in the application, giving users the flexibility to interact with your application in a conversational manner instead of navigating through a series of predefined routes.\n\nRouting by parameters\n\nFor routes like:\n\n/profile/[username]\n/search?q=[query]\n/media/[id]\n\nthat have segments dependent on dynamic data, the language model can generate the correct parameters and render the user interface.\n\nFor example, when you're in a search application, you can ask the language model to search for artworks from different artists. The language model will call the search function with the artist's name as a parameter and render the search results.\n\nArt made by Van Gogh?\nsearchImages(\"Van Gogh\")\nHere are a few of his notable works\nStarry Night\nSunflowers\nOlive Trees\nWow, these look great! How about Monet?\nsearchImages(\"Monet\")\nSure! Here are a few of his paintings\nFrau im Gartenfrau\nCliff Walk\nWaves\n\nMedia Search\n\nLet your users see more than words can say by rendering components directly within your search experience.\n\nRouting by sequence\n\nFor actions that require a sequence of steps to be completed by navigating through different routes, the language model can generate the correct sequence of routes to complete in order to fulfill the user's request.\n\nFor example, when you're in a calendar application, you can ask the language model to schedule a happy hour evening with your friends. The language model will then understand your request and will perform the right sequence of tool calls to:\n\nLookup your calendar\nLookup your friends' calendars\nDetermine the best time for everyone\nSearch for nearby happy hour spots\nCreate an event and send out invites to your friends\nI'd like to get drinks with Max tomorrow evening after studio!\nsearchContacts(\"Max\")\nMax\n@mleiter\nShu\n@shuding\ngetEvents(\"2023-10-18\", [\"jrmy\", \"mleiter\"])\n4PM\n5PM\n6PM\n7PM\nStudio\n4-6 PM\nsearchNearby(\"Bar\")\nWild Colonial\n200m\nThe Eddy\n1.3km\ncreateEvent(\"2023-10-18\", [\"jrmy\", \"mleiter\"])\n4PM\n5PM\n6PM\n7PM\nStudio\n4-6 PM\nDrinks At Wild Colonial\n6-7 PM\nExciting! Max is free around that time and Wild Colonial is right around the corner, would you like me to mark it on your calendar?\nSure, sounds good!\n\nPlanning an Event\n\nThe model calls functions and generates interfaces based on user intent, acting like a router.\n\nJust by defining functions to lookup contacts, pull events from a calendar, and search for nearby locations, the model is able to sequentially navigate the routes for you.\n\nTo learn more, check out these examples using the streamUI function to stream generative user interfaces to the client based on the response from the language model.\n\nPrevious\nRendering UI with Language Models\nNext\nMultistep Interfaces"
  },
  {
    "title": "Advanced: Multistep Interfaces",
    "url": "https://ai-sdk.dev/docs/advanced/multistep-interfaces",
    "html": "Advanced\nMultistep Interfaces\nCopy markdown\nMultistep Interfaces\n\nMultistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.\n\nIn order to understand multistep interfaces, it is important to understand two concepts:\n\nTool composition\nApplication context\n\nTool composition is the process of combining multiple tools to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps.\n\nApplication context refers to the state of the application at any given point in time. This includes the user's input, the output of the language model, and any other relevant information.\n\nWhen designing multistep interfaces, you need to consider how the tools in your application can be composed together to form a coherent user experience as well as how the application context changes as the user progresses through the interface.\n\nApplication Context\n\nThe application context can be thought of as the conversation history between the user and the language model. The richer the context, the more information the model has to generate relevant responses.\n\nIn the context of multistep interfaces, the application context becomes even more important. This is because the user's input in one step may affect the output of the model in the next step.\n\nFor example, consider a meal logging application that helps users track their daily food intake. The language model is provided with the following tools:\n\nlog_meal takes in parameters like the name of the food, the quantity, and the time of consumption to log a meal.\ndelete_meal takes in the name of the meal to be deleted.\n\nWhen the user logs a meal, the model generates a response confirming the meal has been logged.\n\nUser: Log a chicken shawarma for lunch.\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\nModel: Chicken shawarma has been logged for lunch.\n\nNow when the user decides to delete the meal, the model should be able to reference the previous step to identify the meal to be deleted.\n\nUser: Log a chicken shawarma for lunch.\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\nModel: Chicken shawarma has been logged for lunch.\n...\n...\nUser: I skipped lunch today, can you update my log?\nTool: delete_meal(\"chicken shawarma\")\nModel: Chicken shawarma has been deleted from your log.\n\nIn this example, managing the application context is important for the model to generate the correct response. The model needs to have information about the previous actions in order for it to use generate the parameters for the delete_meal tool.\n\nTool Composition\n\nTool composition is the process of combining multiple tools to create a new tool. This involves defining the inputs and outputs of each tool, as well as how they interact with each other.\n\nThe design of how these tools can be composed together to form a multistep interface is crucial to both the user experience of your application and the model's ability to generate the correct output.\n\nFor example, consider a flight booking assistant that can help users book flights. The assistant can be designed to have the following tools:\n\nsearchFlights: Searches for flights based on the user's query.\nlookupFlight: Looks up details of a specific flight based on the flight number.\nbookFlight: Books a flight based on the user's selection.\n\nThe searchFlights tool is called when the user wants to lookup flights for a specific route. This would typically mean the tool should be able to take in parameters like the origin and destination of the flight.\n\nThe lookupFlight tool is called when the user wants to get more details about a specific flight. This would typically mean the tool should be able to take in parameters like the flight number and return information about seat availability.\n\nThe bookFlight tool is called when the user decides to book a flight. In order to identify the flight to book, the tool should be able to take in parameters like the flight number, trip date, and passenger details.\n\nSo the conversation between the user and the model could look like this:\n\nUser: I want to book a flight from New York to London.\nTool: searchFlights(\"New York\", \"London\")\nModel: Here are the available flights from New York to London.\nUser: I want to book flight number BA123 on 12th December for myself and my wife.\nTool: lookupFlight(\"BA123\") -> \"4 seats available\"\nModel: Sure, there are seats available! Can you provide the names of the passengers?\nUser: John Doe and Jane Doe.\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\nModel: Your flight has been booked!\n\nIn the last tool call, the bookFlight tool does not include passenger details as it is not available in the application context. As a result, it requests the user to provide the passenger details before proceeding with the booking.\n\nLooking up passenger information could've been another tool that the model could've called before calling the bookFlight tool assuming that the user is logged into the application. This way, the model does not have to ask the user for the passenger details and can proceed with the booking.\n\nUser: I want to book a flight from New York to London.\nTool: searchFlights(\"New York\", \"London\")\nModel: Here are the available flights from New York to London.\nUser: I want to book flight number BA123 on 12th December for myself an my wife.\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\nModel: Your flight has been booked!\n\nThe lookupContacts tool is called before the bookFlight tool to ensure that the passenger details are available in the application context when booking the flight. This way, the model can reduce the number of steps required from the user and use its ability to call tools that populate its context and use that information to complete the booking process.\n\nNow, let's introduce another tool called lookupBooking that can be used to show booking details by taking in the name of the passenger as parameter. This tool can be composed with the existing tools to provide a more complete user experience.\n\nUser: What's the status of my wife's upcoming flight?\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\nTool: lookupBooking(\"Jane Doe\") -> \"BA123 confirmed\"\nTool: lookupFlight(\"BA123\") -> \"Flight BA123 is scheduled to depart on 12th December.\"\nModel: Your wife's flight BA123 is confirmed and scheduled to depart on 12th December.\n\nIn this example, the lookupBooking tool is used to provide the user with the status of their wife's upcoming flight. By composing this tool with the existing tools, the model is able to generate a response that includes the booking status and the departure date of the flight without requiring the user to provide additional information.\n\nAs a result, the more tools you design that can be composed together, the more complex and powerful your application can become.\n\nPrevious\nLanguage Models as Routers\nNext\nSequential Generations"
  },
  {
    "title": "Advanced: Sequential Generations",
    "url": "https://ai-sdk.dev/docs/advanced/sequential-generations",
    "html": "Advanced\nSequential Generations\nCopy markdown\nSequential Generations\n\nWhen working with the AI SDK, you may want to create sequences of generations (often referred to as \"chains\" or \"pipes\"), where the output of one becomes the input for the next. This can be useful for creating more complex AI-powered workflows or for breaking down larger tasks into smaller, more manageable steps.\n\nExample\n\nIn a sequential chain, the output of one generation is directly used as input for the next generation. This allows you to create a series of dependent generations, where each step builds upon the previous one.\n\nHere's an example of how you can implement sequential actions:\n\nimport { openai } from '@ai-sdk/openai';\nimport { generateText } from 'ai';\n\n\nasync function sequentialActions() {\n  // Generate blog post ideas\n  const ideasGeneration = await generateText({\n    model: openai('gpt-4o'),\n    prompt: 'Generate 10 ideas for a blog post about making spaghetti.',\n  });\n\n\n  console.log('Generated Ideas:\\n', ideasGeneration);\n\n\n  // Pick the best idea\n  const bestIdeaGeneration = await generateText({\n    model: openai('gpt-4o'),\n    prompt: `Here are some blog post ideas about making spaghetti:\n${ideasGeneration}\n\n\nPick the best idea from the list above and explain why it's the best.`,\n  });\n\n\n  console.log('\\nBest Idea:\\n', bestIdeaGeneration);\n\n\n  // Generate an outline\n  const outlineGeneration = await generateText({\n    model: openai('gpt-4o'),\n    prompt: `We've chosen the following blog post idea about making spaghetti:\n${bestIdeaGeneration}\n\n\nCreate a detailed outline for a blog post based on this idea.`,\n  });\n\n\n  console.log('\\nBlog Post Outline:\\n', outlineGeneration);\n}\n\n\nsequentialActions().catch(console.error);\n\nIn this example, we first generate ideas for a blog post, then pick the best idea, and finally create an outline based on that idea. Each step uses the output from the previous step as input for the next generation.\n\nPrevious\nMultistep Interfaces\nNext\nVercel Deployment Guide"
  },
  {
    "title": "Advanced: Vercel Deployment Guide",
    "url": "https://ai-sdk.dev/docs/advanced/vercel-deployment-guide",
    "html": "Advanced\nVercel Deployment Guide\nCopy markdown\nVercel Deployment Guide\n\nIn this guide, you will deploy an AI application to Vercel\n using Next.js\n (App Router).\n\nVercel is a platform for developers that provides the tools, workflows, and infrastructure you need to build and deploy your web apps faster, without the need for additional configuration.\n\nVercel allows for automatic deployments on every branch push and merges onto the production branch of your GitHub, GitLab, and Bitbucket projects. It is a great option for deploying your AI application.\n\nBefore You Begin\n\nTo follow along with this guide, you will need:\n\na Vercel account\nan account with a Git provider (this tutorial will use Github\n)\nan OpenAI API key\n\nThis guide will teach you how to deploy the application you built in the Next.js (App Router) quickstart tutorial to Vercel. If you haven’t completed the quickstart guide, you can start with this repo\n.\n\nCommit Changes\n\nVercel offers a powerful git-centered workflow that automatically deploys your application to production every time you push to your repository’s main branch.\n\nBefore committing your local changes, make sure that you have a .gitignore. Within your .gitignore, ensure that you are excluding your environment variables (.env) and your node modules (node_modules).\n\nIf you have any local changes, you can commit them by running the following commands:\n\ngit add .\ngit commit -m \"init\"\nCreate Git Repo\n\nYou can create a GitHub repository from within your terminal, or on github.com\n. For this tutorial, you will use the GitHub CLI (more info here\n).\n\nTo create your GitHub repository:\n\nNavigate to github.com\nIn the top right corner, click the \"plus\" icon and select \"New repository\"\nPick a name for your repository (this can be anything)\nClick \"Create repository\"\n\nOnce you have created your repository, GitHub will redirect you to your new repository.\n\nScroll down the page and copy the commands under the title \"...or push an existing repository from the command line\"\nGo back to the terminal, paste and then run the commands\n\nNote: if you run into the error \"error: remote origin already exists.\", this is because your local repository is still linked to the repository you cloned. To \"unlink\", you can run the following command:\n\nrm -rf .git\ngit init\ngit add .\ngit commit -m \"init\"\n\nRerun the code snippet from the previous step.\n\nImport Project in Vercel\n\nOn the New Project\n page, under the Import Git Repository section, select the Git provider that you would like to import your project from. Follow the prompts to sign in to your GitHub account.\n\nOnce you have signed in, you should see your newly created repository from the previous step in the \"Import Git Repository\" section. Click the \"Import\" button next to that project.\n\nAdd Environment Variables\n\nYour application stores uses environment secrets to store your OpenAI API key using a .env.local file locally in development. To add this API key to your production deployment, expand the \"Environment Variables\" section and paste in your .env.local file. Vercel will automatically parse your variables and enter them in the appropriate key:value format.\n\nDeploy\n\nPress the Deploy button. Vercel will create the Project and deploy it based on the chosen configurations.\n\nEnjoy the confetti!\n\nTo view your deployment, select the Project in the dashboard and then select the Domain. This page is now visible to anyone who has the URL.\n\nConsiderations\n\nWhen deploying an AI application, there are infrastructure-related considerations to be aware of.\n\nFunction Duration\n\nIn most cases, you will call the large language model (LLM) on the server. By default, Vercel serverless functions have a maximum duration of 10 seconds on the Hobby Tier. Depending on your prompt, it can take an LLM more than this limit to complete a response. If the response is not resolved within this limit, the server will throw an error.\n\nYou can specify the maximum duration of your Vercel function using route segment config\n. To update your maximum duration, add the following route segment config to the top of your route handler or the page which is calling your server action.\n\nexport const maxDuration = 30;\n\nYou can increase the max duration to 60 seconds on the Hobby Tier. For other tiers, see the documentation\n for limits.\n\nSecurity Considerations\n\nGiven the high cost of calling an LLM, it's important to have measures in place that can protect your application from abuse.\n\nRate Limit\n\nRate limiting is a method used to regulate network traffic by defining a maximum number of requests that a client can send to a server within a given time frame.\n\nFollow this guide\n to add rate limiting to your application.\n\nFirewall\n\nA firewall helps protect your applications and websites from DDoS attacks and unauthorized access.\n\nVercel Firewall\n is a set of tools and infrastructure, created specifically with security in mind. It automatically mitigates DDoS attacks and Enterprise teams can get further customization for their site, including dedicated support and custom rules for IP blocking.\n\nTroubleshooting\nStreaming not working when proxied\nExperiencing Timeouts\nPrevious\nSequential Generations\nNext\nReference"
  }
]
